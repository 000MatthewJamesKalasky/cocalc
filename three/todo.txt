Programming Todo
----------------

NOW: 
  [ ] switch to use worker.py in backend for execute action:  
  [ ] scalability test: using worker.py, but running on a different VM
  [ ] scalability test: test using backend.py(s), also running on different VM's
  [ ] this line in haproxy conf looks wrong/misleading since I settled on a file, right? 
             #daemon   -- commented out so I can just log to stdout 
  [ ] clean up specification of ports, especially for haproxy -- should be part of Process creation, not a file -- template the conf!

SOON: 
   [ ] user-selectable option to use memcached to cache single cell evaluations between backends
   [ ] don't allow mesg until connected
   [ ] worker.py dies when run in daemon mode -- for now just run as subproc, but need to fix this when rewrite worker.py
   [ ] separate worker client from worker
   [ ] simplify worker for my particular application and properly daemonize worker using same options as for backend
   [ ] admin: support start/stop/status worker
   [ ] implement basic message routing between workers and connected clients (?)
   [ ] two different haproxies at once (so either can be used if the other dies)
   [ ] think about how to separate my data into multiple database for scalability
   [ ] sqlalchemy postgresql central DB server
        - do test: what is overhead of storing a BLOB of a git bundle?
   [ ] implement basic "single cell server"
   [ ] implement interact
   [ ] logging -- it might be good if there were a common format for
       the date part of every log message, so the time when log row
       was *record* could be set, instead of the time of insertion in
       DB.  This could ble very hard though, and would ungroup similar
       entries. 

SOMEDAY:

   [ ] admin: support start/stop/status log server
[ ] How to do multiple haproxies?:
    - do what stackoverflow does: heartbeat + haproxy: http://blog.stackoverflow.com/2010/01/stack-overflow-network-configuration/; http://www.linux-ha.org/wiki/Main_Page
    - this discussion says to use "DNS failover":  
        * http://www.webhostingtalk.com/archive/index.php/t-1117385.html
        * http://www.dnsmadeeasy.com/
    - this page talks about round robin DNS, which seems sensible to me:
      http://blog.engelke.com/2011/06/07/web-resilience-with-round-robin-dns/


[ ] reduce number of mime types in nginx.conf, since I will barely serve anything??
[ ] sudo worker.py -- executing commands doesn't work at all on Linux
    (but does on OS X), so probably a resource issue?
[ ] move all certfile generation stuff to a single master control / launcher module
[ ] startup time -- after fork even after importing sage -- is *very* slow.  Fix.
[ ] make it so client knows port of server?
[ ] worker.py -- anti DOS measures (from users on own machine?)
[ ] configuration framework
[ ] rewrite reset_all_accounts/reset_account to use that worker.py is root.
[ ] log server going down is *fatal* to worker
PLAN: Implement each component without UI polish.
  * Client -- Javascript library that runs in any modern web browser
    [ ] Write very simple ugly version that is fully functional.
  * Load Balancer -- HAProxy
    [ ] Learn how to deploy it and write config script.
        Example config script on some SockJS site.
  * Database -- PostgreSQL + SQLalchemy + Memcached + SSL
    [ ] Assemble SQLalchemy schema by combining what is current
        frontend and backend schema, plus actually store github bundle.
  * Worker -- forking SSL socket server + Sage + JSON
    [ ] Rewrite pulling code from backend.py in order to make this
        into a single integrated component with a straightforward API.
  * Backend -- HTTPS SockJS server; "create workspace" into DB queries; connect to worker
    [ ] Rewrite what I have to use SockJS (remove socket.io)
  * Static HTTP server -- simple nginx (no ssl)
    [ ] Configuration so my static/ directory served using nginx.
    [ ] Ability to serve static/ content created via statically publishing workspaces
  * Log server -- SSL socket server + database writer + Python logging
    [ ] update to use PostgreSQL database
    [ ] make it timeout connections that it forks off
    [ ] can I rewrite to use tornado so it's a single process (no forking)?  that would scale more

DONE:
[x] proper basic worker.py logging
[x] fix Ctrl-D EOF bug in worker.py -- process gets left running full steam on EOF
[x] worker: forked process gets left as a zombie when it quits; fix this.
[x] implement auth token for worker
[x] user switching: How will this work?  
* It runs as either root or normal user
* Imports the sage library
* Client connects to socket
* Fork
* If running as root, change to user:
        import os, pwd
        if os.system('whoami') == 'root'
            user = 'sagews'
            os.setegid(pwd.getpwnam(user)[3])
            os.seteuid(pwd.getpwnam(user)[2])
  * then, fix any environment and PATH issues, e.g., DOT_SAGE
* If running as normal user, just gets new session as usual, running
  as same user as the server
[x] worker.py -- list of users for switching (command line option)
[x] todos near /tmp/xyz
[x] fix: server terminates when any client disconnects if we don't ignore SIGCHLD.

[x] clients must be killed after n seconds (another limit):
      - use a separate thread in the worker root process
      - it will check every n seconds to see if anything needs to be killed:
           use the Connection object
      - if so, sends SIGQUIT, SIGKILL, etc. 

[x] when main server terminates, it needs to kill the other thread that is watching
    for connections to kill.

[x] clean up certfile in worker a bit

[x] limiting memory usage doesn't work at all:
  -- this is a well-known OS X "bug"/feature?
   sage: import resource
sage: resource.setrlimit(resource.RLIMIT_DATA, (10,10))
sage: v = range(10^9)
/Users/wstein/sage/build/sage-5.0/spkg/bin/sage: line 311: 51632 Killed: 9               sage-ipython "$@" -i

This works perfectly on Linux:

  megs=1000; resource.setrlimit(resource.RLIMIT_AS, (megs * 1048576L, -1L))
[x] create basic launcher / master control script
-- ensure data directory exist and has right permissions
-- read configuration file
-- start relevant services on various machines:
   - haproxy
   - nginx
   - postgreSQL server
   - memcached
   - backend(s)
   - worker(s)


[x] build system working on linux
[x] serve static files with nginx
[x] haproxy to nginx
[x] psql database

[x] simple tornado+sockjs+haproxy demo
[x] simple load balancing + haproxy + sockjs demo
[x] move the one static file to nginx instead of tornado
   [x] optparse backend.py
   [x] new file: sagews.conf 
   [x] move conf files to a subdirectory conf/
   [x] rename "launch.py" to "admin.py". 

   [x] base class functionality for each component:
          * start(which='all')
          * status(which='all')
          * stop(which='all')
   [x] refactor existing code to work as above 

   [x] read about multiple ip addresses for one name (=round robin dns)
   [x] read through admin.py cleaning up; finish implementing postgresql options command
   [x] test starting/stopping haproxy, nginx, and database, and creating database
   [x] properly daemonize backend
   [x] admin: support start/stop/status backend
   [x] LOGGING THINKING: I've spent a while now thinking about logging, and have
       changed my mind about what I want to do.  
         * get rid of log.py; it is potentially too low performance and is
           tied to Python.
         * do what is suggested on stackoverflow: "Write locally to
           disk, then batch insert to the database periodically
           (e.g. at log rollover time). Do that in a separate,
           low-priority process. More efficient and more
           robust... (Make sure that your database log table contains
           a column for "which machine the log event came from" by the
           way - very handy!)": http://stackoverflow.com/questions/290304/is-writing-server-log-files-to-a-database-a-good-idea
         * Why *not* to log directly to a database: http://stackoverflow.com/questions/209497/using-a-sql-server-for-application-logging-pros-cons?lq=1

   [x] write logwatch script
   [x] postgres logging
   [x] create logwatch.py, which watches a file for changes, and when
       it changes, inserts it into a log database postgreSQL server
       securely
   [x] when admin starts anything, it also starts one logwatch.py;
       when it stops anything, it stops corresponding logwatch.py

   [-not-] idea about doing nginx right:
See http://www.ruby-forum.com/topic/134115   
      in particular,
       mv error.log error.log.0
       kill -USR1 `cat master.nginx.pid`
       wait 1 second and do anything with log
     But instead do "nginx -s reload"
   [x] nginx logging 
   [x] memcached logging
   [x] backend logging
   [x] issue with postgresql pid -- nothing; I was confused
   [x] worker pidfile, logging, Process object, etc. 
   [x] haproxy logging (hardest) -- have to patch/modify log.c to:
         print instead of send to network socket -- printf("%s", log_ptr); in "__send_log"

   [x] admin -- systematic logfile support: 
         - must specify logfile when creating Process
         - function to read what is new in logfile since last read
         - function to empty logfile
         - I will be able to build database on this, in uniform way
           that doesn't require python logger support.  So it was a
           day wasted on log.py...
  [x] simple single computation server using what I have
  [x] switch to testing with multiple backends instead of 1 in config1.py
  [x] test under linux
  [x] automatic reconnect
  [x] postgres logging is problematic due to: FATAL:  sorry, too many clients already
  [x] issues with anything but websocket -- needed to load balance by IP address
  [x] implement ssl (with a self-signed cert for now):
          - just encrypting between haproxy and backend doesn't work, and is insane
          - encrypting between backend and worker is probably insane too.
          - use stunnel!?  YES, it is awesome!
  [x] with stunnel haproxy sees everybody as coming from the same localhost ip, so load balancing  doesn't work at all:
        Evidently one must patch stunnel -- http://www.completefusion.com/ssl-load-balancing-with-haproxy-and-stunnel-on-debian/
  [x] make top-level mobile friendly index.html and also for cell
  [x] make top level index.html, etc. for desktop look jquery-ish
  [x] worker.py: 
        [x] running as root
        [x] making up a user
        [x] limitations
  [x] figure out how to use a proper ssl certificate
