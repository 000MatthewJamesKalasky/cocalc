TODO
-----

NOW: 
[ ] consider switching from postgresql to cassandra -- ugh... but it
    has the right properties, and I'll be fighting postgresql if I
    don't do this from the start.  Right now, the database is the only
    non-truly distributed multi-data center component...

 This would mean getting rid of:
   * momoko
   * psycopg
   * memcached

SOON:

  [ ] more complicated VPN:

  I think I need to put all the sage servers on a different vpn than  everything else:

VPN's:

   salvus: everything except the sage servers
   sage:   all the sage servers + all tornados

The tornados will be on *both* the salvus and sage network, so they can talk
to the sage machines and the database and haproxy.  But the sage servers
can *only* talk to the tornado machines and that is it.  And no databases
will run on the same machines as the tornado machines.    Even if somebody
roots a sage server, they do not get access to the salvus vpn, so it gains
them little.   

This way I will not have to worry a lot about security of the database
(at least of the connections between nodes).  That said, anything in
the database that is really sensitive may as well be encrypted.  How
might that work?  What is sensitive?

    highest level: email addresses of users, along with password hash (if not openid)
    lower level:  their content (local cache or when not stored on google drive)

* Password hash is fine, assuming we use a state of the art hashing algorithm!
* email address must be protected.

Why/when do we need email addresses?   

Let userid = SHA1(email_address).   

Then the tornado web servers do not need to know the email address do
they?  They just need the SHA1 hash of it, which can be computed by
the client, as long as that hash function is implemented in
javascript.   For password recovery, the user would submit their
email address, and I would compute the SHA1, verify it is correct,
and then send email to that address.   Why else would I need to
store their email?  If I want to send them status info about their
accounts, the system, etc... but that can be done on a completely
different server.

SO... conclusion: the production network and big distributed database will
*not* have any email addresses in it -- only the SHA1 of them. 

There will be another super-secure database that contains email
address and account info, and that server will be used for sending any
relevant status emails, etc.  It can be passphrase protected in an
encrypted file, etc.  When new people sign up, their email addresses
could be sent public-key encrypted to that server, then added to the
database.  The actual database should be pretty small; it'll just be a
key:value store from userid to email_address (and possibly phone
number for some sort of 2-factor thing).

   [ ] *passphrase* + shared ssh keys:

On my laptop and some computers somewhere on the net, I will setup the
salvus account to have a passphrase protected .ssh private key, and
make all the nodes trust that key.  This will be just like how I have
things now, except now it's my wstein account on my laptop, and it's
not passphrase protected.  I will also make it so root login is
allowed on all nodes.  Then when I want to launch services or do
anything to any nodes (or their hosts -- e.g., reset VM's), I can type
the passphrase once and *everything* thereafter is password free and
automatic.

   [ ] write crafting config2.py, which will deploy live on the web.

   [ ] top goal: have the page be live with a single compute cell

   [ ] create query that will be used by backend/monitor/etc. to determine 
       memcache pool:
          [ ] make sure startup goes like this:
             (1) start database
             (2) start memcached
             ... 
          [ ] create the query to get memcached pool
          [ ] use in tornado, monitor, etc. 

   [ ] haproxy should also proxy ssl tcp connections to the tornado servers, so they 
       can provide a simpler faster non-http API.  
     
   [ ] "monitor": database ...
     [ ] add support to admin.py to clean out old entries from the
         database, i.e., something like capped collection via a delete call: calls db.cap_table somehow...

   [ ] fix this, which is caused by postgresql[0].createdb('monitor') in config1.py:
 8787 | postgresql-0.log | 2012-08-08 11:18:05.854365 | STATEMENT:  CREATE DATABASE monitor;
 8786 | postgresql-0.log | 2012-08-08 11:18:05.854365 | ERROR:  database "monitor" already exists

   [ ] update apple dev network account and make a first PhoneGap app for iphone
   [ ] make a first PhoneGap app for iphone

   [ ] database: function to check validity of entries; if anything not current, then updated.

   [ ] database: table of connected browers and which tornado server they are connected to

   [ ] admin: make it so this doesn't happen every time: 
          INFO:root:running 'sudo cp /var/folders/fc/tdg_b00d2rv0_0c940cxttjc0000gp/T/tmp3XEkwZ data/haproxy-0.conf'
  
   [ ] tornado: implement sending message to any connected browser, possibly connected to a different tornado

   [ ] sage_server: sage_server.py dies when run in daemon mode -- for now just run as subproc, but need to fix this

   [ ] sage_server/tornado: range(10^4 ) *reliably* fails to get the final I/O terminate message through from tornado to browser.
   [ ] sage_server: specify tornado sage_servers by putting them in a config file, which the
       tornado monitors for changes (via file descriptor)
   [ ] sage_server: when sage_server.py killed, it doesn't properly clean up the processes that it forked off.   
   [ ] sage_server: max limit on number of simultaneous sage_server processes allowed by sage_server server
   [ ] sage_server: sudo sage_server.py -- executing commands doesn't work at all on Linux
       (but does on OS X), so probably a resource issue?

   [ ] tornado: dropbox oauth: dropbox too 
   [ ] haproxy: this line in haproxy conf looks wrong/misleading since I settled on a file, right? 
             #daemon   -- commented out so I can just log to stdout 
   [ ] haproxy: two different haproxies at once (so either can be used if the other dies)
   [ ] logging: format for date part of every log message, so in
       database the time when log row was *record* could be set,
       instead of the time of insertion in DB.  
   [ ] browser/sage_server/tornado: interacts
   [ ] browser/sage_server/tornado: 2d graphics
   [ ] tornado/database: shortened url database
   [ ] I didn't build openVPN with LZO, but I should so all network traffic is compressed, saving money.
   [ ] come up with deployment plan
   [ ] setup openVPN
   [ ] admin: make default admin import less verbose
     [ ] decide on what computers to buy...

   [ ] address this in tornado_server:
          TODO: message should not use stderr, but instead maybe a new/extended protobuf2 type

infrastructure:
   [ ] remove SPOF's from openVPN (?)
   [ ] fix or ensure raid1 is working on combinat machine

   [ ] decide on what computers to buy from Dell to deploy in a few weeks
        - buy RAM for bsd.math ?

   [ ] install ubuntu 12.04 on redhawk


VAGUE:


   [ ] sqlalchemy postgresql central DB server
        - do test: what is overhead of storing a BLOB of a git bundle?
   [ ] scalability test: using sage_server.py, but running on a different VM
   [ ] scalability test: test using tornado.py(s), also running on different VM's
   [ ] think about how to separate my data into multiple database for scalability
   [ ] multiple haproxies for high availability:
    - do what stackoverflow does: heartbeat + haproxy: http://blog.stackoverflow.com/2010/01/stack-overflow-network-configuration/; http://www.linux-ha.org/wiki/Main_Page
    - this discussion says to use "DNS failover":  
        * http://www.webhostingtalk.com/archive/index.php/t-1117385.html
        * http://www.dnsmadeeasy.com/
    - this page talks about round robin DNS, which seems sensible to me:
      http://blog.engelke.com/2011/06/07/web-resilience-with-round-robin-dns/
   [ ] reduce number of mime types in nginx.conf, since I will barely serve anything??
   [ ] move all certfile generation stuff to a single master control / launcher module
   [ ] startup time -- after fork even after importing sage -- is *very* slow.  Fix.
   [ ] make it so client knows port of server?
   [ ] sage_server.py -- anti DOS measures (from users on own machine?)
   [ ] configuration framework
   [ ] rewrite reset_all_accounts/reset_account to use that sage_server.py is root.
   [ ] log server going down is *fatal* to sage_server

DONE:
   [x] tornado: decide on way for any tornado to send a message to any connected
       user, possibly connected to a different tornado:
       Proposal 2 (accepted):
         - tornados will have token from database
         - add to ioloop listening on SSL+TCP socket (encrypted); only accept connection when given token
         - send ProtoBuf messages
         - connection terminated if no messages sent over it for t seconds
         - *maybe* connections proxied through haproxy, so that tornado is not actually on an external network.
           I don't know if using haproxy is needed or a good idea; it is a small detail in the implementation.
       Proposal 1 (rejected):
         - tornados will have access to a token they read from database, which they know but
           not publicly available (since tornados have to be able to get personal user data, 
           the database must support this).
         - Add another url handler:  /message
            POST message with variables:
                 token: the token
                 message: the message, as a ProtoBuf -- the message format should have user id of recipient as part of message
         - haproxy will also map 
                /tornado[n]/... to the n-th tornado.
         - each tornado has an id number (the n above)
         - tornado communication will go via stunnel, so is secure
           outside of LAN, where it matters
         - messages might suggest pulling down static web content (e.g, describing an image), 
           and url will be /tornado[n]/static/[id].[ext]
         Thoughts: the above seems untenable because of the overhead in estabilishing a complete 
         HTTPS connection for every single message. 
   [x] tornado: find a way to move the code for "Persistent connections to sage_servers" into another file (like
       is done with "Authentication with Facebook, Google, and DropBox (TODO)")
   [x] yes, use a vpn???
         I'm thinking about how to structure services, define their location,
         make it so every service can connect to every other one, etc. 
         One possibility is that I setup a VPN connecting all the
         physical sites.   Hmmm.  
         It looks like openVPN is a good choice. 
            http://openvpn.net/index.php/open-source/overview.html
         It takes only a few seconds to build from source.  It's small.  The documentation
         seems excellent.  And using this will make it so I can easily use exactly one
         config script to manage all the sites at once, that any node can talk to any 
         other node, while they are all still behind a layer of security (not directly
         on the web), etc. 
   [x] add openvpn to build.py -- easy to build on linux; not sure how yet on OS X!
   [x] tornado: postgresql async client for tornado -- needed to
       implement tornado socket communication (just a little), and
       will be generally very important.   This is the canonical
       solution, but might be substantial work to implement with sqlalchemy
           https://gist.github.com/861193
       Another possibility that looks better:
           [x] do not use SQLalchemy at all (sad, but I have only written
               a few lines in logwatch.py so far in my newest version, so this
               shouldn't be hard).
           [x] Directly use psycopg2 (which has builtin async support) and 
               momoko which officially supports using PostgreSQL + Tornado.
       This would explicitly tie us to PostgreSQL.
    
    [x] tornado communication system working with ports hardcoded and no encryption
    [x] tornado communication system with encryption
    [x] html: make "Thyme" name clear.

   [x] haproxy: put in a port 80 redirect to port 443
       XX sadly, haproxy alone *can't* do this -- http://www.mentby.com/Group/haproxy/http-https-redirects.html
       XX but haproxy + nginx should be able to, but that is complicated.
       ---> we will in the long run have a canonical site name, so let's just make that work with a template.
   [x] admin: config1 --status should default to status=all.
   [x] admin: redo the very limited use of templates to use python standard library templates.
   [x] haproxy, etc.: clean up specification of ports, especially for haproxy -- should be part of Process creation, not a file -- template the conf
   [x] admin: rename "backned" to "tornado_server"/Tornado, sage_server to sage_server/Sage
   [x] rename backend_sage to tornado_sage; rename backend_mesg to tornado_mesg
   [x] rebrand to "salvus"
   [x] gracefully degrade when memcached dies: this is evidently 100% automatic by the client! WOW
   [x] tornado: should degrade gracefully when sage server vanishes...
   [x] obtain an SSL certificate for https://salvusmath.com
         https://wiki.cac.washington.edu/display/infra/UW+Certificate+Services
       * Don't use UW's because their FAQ says: "Will all UW web servers
         eventually get certs from the UW Services CA?  No, not at
         all. Even when the UW Services CA expands its scope there
         will be many cases where it is still appropriate for a web
         server to use a certificate from a commercial CA. For
         example, if a web server has many users from outside the UW
         it will probably want to use a commercial CA certificate."
       * Good instructions: http://clearcove.ca/2010/11/how-to-secure-a-rails-app-on-heroku-with-ssl-firesheep/
       * Googling "godaddy ssl" really does provide a $12.99 deal for a year certificate!

   [x] delete thyme gmail account (see msecure)
   [x] create salvusmath gmail account
   [x] change github repo to be called salvus (instead of sagews)
   [x] massive renames and moves

   [x] get everything to work after the big rename: just rebuild.
   [x] clean checkout and build on linux

   [x] Address this comment I found: "Site note: our 2gb VPS's memcahed is set up to accept over 40k requests. The default setting in memcached is 1024 which is WAY too low."
           http://blog.dpn.name/asyncronous-memcache-driver-for-tornado

   [x] async tornado memcached:  
           Solution: https://github.com/dpnova/tornado-memcache
         Add to build system.

   [x] check-in upstream source tar.bz2's to github repo.
   [x] switch to using tornado-memcache client in tornado_server.py
   [x] Salvus favicon
   [x] salv.us certificate
   [x] need to use namespace for my use of memcached in tornado_server.py
   [x] decided not to add in-process ram cache to tornado_server that I check first, since this could easily lead to using up all available RAM if not implemented sensibly, and doing this sensibly is really complicated.
           
[...x] monitor database:
     [x] rename logwatch.py to monitor.py
     [x] change docs to indicate what monitor.py does
     [x] fix this doc, which must be wrong since we don't use sqlalchemy anymore:
           "SQLalchemy description of database server, e.g., postgresql://user@hostname:port/dbname"
     [x] instead of putting the log in "log" database, put it in log table in monitor database
     [x] add cap_table function in db.py, which deletes all but most recent entries from a table
     [x] code to define new monitoring table schemas:
services:
  - id - unique sequential integer
  - type - string: one of 'nginx', 'haproxy', 'sage', 'tornado', 'stunnel', 'openvpn', 'memcached', 'postgresql'
  - site - string: where this service is physically located
  - address - string (ip address or hostname)
  - port - integer  
  - running - boolean
  - user
  - pid
  - monitor_pid

status:
  - id -- unique id of service entry above
  - timestamp -- timestamp
  - load - integer
  - percent_mem -- float
  - percent_cpu -- float
  - cputime -- float
  - walltime -- float (etime in ps output)
  - virtual_size -- integer
  - resident_size -- integer

     [x] monitor: change to use -- options since - options are too short

   [x] "Failed to commit log messages to database (invalid byte sequence for encoding "UTF8": 0x8d)"
       Seen when doing 
            sudo ./monitor.py --debug --logfile data/logs/haproxy-0.log --database "dbname=monitor user=wstein" --pidfile data/pids/haproxy-0-log.pid --interval 6 --target_pidfile data/pids/haproxy-0.pid --target_name haproxy --target_address localhost --target_port 8000

     [x] add address command line option to monitor.py, which gives
         the address and port that the service listens on: 
           "--address 10.7.1.5 --port 6000 --site padelford".  This will get entered into the
         database when process starts.  When this happens, we will also 
         get the unique id that corresponds to this service, which will be used
         by the monitor for all future db interaction.  
     [x] make a command line option for when info is put in database
     [x] service startup -- put data in database
     [x] when monitored process terminates, update the corresponding entry (using id)
         to set running to false
     [x] status -- put periodic information in database about state of watched processes
     [x] memcached + monitor: store something in memcached at same time as put in database, so 
         query about latest status can be done instantly using memcached if in cache.
     [x] add support to easily query the monitor database and use memcached when appropriate
          [x] - list of running processes and last status
          [x] - should use memcache too, so must use clear API defined in monitor.py
          [x] - status of lifetime of process
     [x] create synchronous query that will be used by backend to determine which
         sage_server to use, and use that -- might choose based on
         lowest load, etc.
     [x] race condition: sage_server seems to delete its pid file even when it doesn't quit, which causes havoc...
         

     [x] sage_server: "./config1 --restart=sage_server" doesn't work; this is because the
         port isn't being freed quickly enough.  we use SO_REUSEADDR to fix this.

     [x] make tornado determining sage_server query be async
   [x] get it to start under linux 

   [x] setup openVPN

   [x] The domain name "salv.io" is also available ($100/year from
       name.com), but more complicated to get.  Would it be better
       than salv.us?  No, since it is hallucinogenic sage!

   [x] setup a private DNS server -- instead use godaddy to do this. 

   [x] change DNS clients to run in daemon mode, maybe connecting on
       startup (at least for the VMs)?

   [x] change openvpn to use compression

   [x]  change my salvus installs on all machines in vpn to use git, so I can easily pull/push -- I guess I could just have them directly pull/push from my laptop or combinat account safely over the vpn.

[x] automate adding new host to vpn

