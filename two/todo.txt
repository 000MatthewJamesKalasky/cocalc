
2012-06-26: 

  I've taken some major turns in my design work, so the next steps are
  unclear.  Thus I need to think this through.   I want to approach this
  via creating cleanly designed components that are independent units, and
  then assembling them together at the end.   For deployment, use several 
  clean virtual machine, and ssh access to a root account in order to
  configure the virtual machines dynamically.  

The components:

  * Client -- Javascript library that runs in any modern web browser
    [ ] Write very simple ugly version that is fully functional.

  * Load Balancer -- HAProxy
    [ ] Learn how to deploy it and write config script.
        Example config script on some SockJS site.

  * Database -- PostgreSQL + SQLalchemy + Memcached + SSL
    [ ] Assemble SQLalchemy schema by combining what is current
        frontend and backend schema, plus actually store github bundle.

  * Worker -- forking SSL socket server + Sage + JSON
    [ ] Rewrite pulling code from backend.py in order to make this
        into a single integrated component with a straightforward API.

  * Backend -- HTTPS SockJS server; "create workspace" into DB queries; connect to worker
    [ ] Rewrite what I have to use SockJS (remove socket.io)

  * Static HTTP server -- simple nginx (no ssl)
    [ ] Configuration so my static/ directory served using nginx.
    [ ] Ability to serve static/ content created via statically publishing workspaces

  * Log server -- SSL socket server + database writer + Python logging
    [ ] update to use PostgreSQL database


------------------------------------------------

Not sure task:


  [ ] Display a list of workspaces (bare minimum possible):
        - just use an readonly textarea
  [ ] Select workspace to visit it
        - type a number next to a button and press button
  [ ] Create new workspace
        - 

Clear tasks

  [ ] make frontend ssh command that starts backend use subprocess
  [ ] read through code I wrote for worker <--> backend communication; 
      make sure db transactions are limited in scope; use rollbacks.


Unclear tasks

  [ ] choosing which worker to assign to a workspace
  [ ] make it so worker forked process shuts down sensibly maybe when gets SIGXCPU signal?
  [ ] see note in reset_account function in worker.py
  [ ] worker registering 
  [ ] actual session from browser with computation working, using worker
  [ ] maybe change how backend gets configured by frontend (make more secure/robust)





 
-------------

(2012-06-17) minimal goals:

Session 1:
  [x] configure worker VirtualBox Machine: 
          - put scripts into sagews repo
  [x] configure frontend Virtualbox Machine
  [x] add code to frontend database to store details about workers (login@hostname)

Session 2:
  [x] page to edit frontend database worker info
  [x] add code to backend database to store details about workers they will use 



TODO List:

[x] implement worker.py demo using a normal socket on a port and benchmark

[ ] design workers so that they are very secure, using ephemeral
    accounts, limited-size git bundles, etc.:



[ ] implement design.
    




  [ ] fix permissions before spawning worker, and redo worker more effectively
   
      1. Run worker.py with:

            worker.py --backend_port=8080 --workspace_id=3

      2. Worker will register backend at the given port, and the
         backend will respond with a JSON object containing
         substantial additional data, e.g., path, username, resource
         limits, etc.

      3. worker.py then sets permission, ssh's to other account, 
         starts worker.py there, and exits. 
 
      This avoids using up file descriptors and is cleaner. 

            worker.py --stop

      Worker could autostop within n seconds of backend vanishing or
      of absolute timeout..., which would be much nicer than having to
      ssh into many accounts and kill things. 


[ ] general thing: write code that goes through and locks down
    permissions as much as possible...  runs on backend startup, etc.


=============================

ideas

today:

 [x] command line option to backend.py to take workers:
        --workers=
 [x] backend: placeholder decorator -- @frontend, which means whatever
     is making the request must be an authenticated frontend server.
        @frontend
 [x] backend: URL handler to create new workspace (and associated git repo)
        use POST with --
            * var: id -- integer
            * files: bundle -- git bundle (if not given make empty workspace)
 [x] backend git functions -- (not HTTP; will get used automatically(?) and by socket.io stuff)
         * save: make git snapshot of workspace with given id
         * revert: git changeset that make it like an old version (but do *not* delete history!)
 [x] ssh/code to launch worker.py

 [ ] look into Bootstrap -- http://twitter.github.com/bootstrap/javascript.html

 [ ] clean up after shutdown
       -- git checkout workspaces/id
 
 [ ] ** spawning of backend is very dangerous since if the management
     console or database are compromised at all (even by accident), then
     arbitrary code could be run on the frontend server!  Not good.
     So... instead of configuring the backend via options, let's pass in
     only the id and frontend uri, which we carefully sanitize, and
     send the other configuration?
     Also, never use os.system/os.popen, but instead subprocess.Popen,
     which is safer. **

--------------------------------------------------------------------------------------

 [ ] write Frontend

      [ ] write frontend.py server (flask + SQLalchemy version)

          [x] design an extensible data model that is simple enough to implement but
              powerful enough to be interesting and later extend to full
          [x] implement sqlalchemy database, copying from my old frontend.py code
          [ ] design a client/server setup that works with the data model
          [ ] implement frontend server part of model a bit
          [ ] implement desktop client (not polished) a bit
          [ ] implement mobile client (not polished) a bit

      [ ] write static/sagews/desktop/frontend

      [ ] write static/sagews/mobile/frontend


 [x] switch client server role for worker.py!

 [ ] (not sure I want to do this) properly spoof stdout/stderr.  This
     is done right in class SageSpoofOut(SageObject): that is in
     http://trac.sagemath.org/sage_trac/attachment/ticket/12415/12415_inprogress.patch
     and I've copied to spoof.txt in this directory temporarily.

 [ ] write backend.py

      [x] unix domain sockets client/server using tornado non-blocking sockets
      [ ] implement very basic backend desktop client (not polished)
      [ ] implement very basic backend mobile client (not polished)

 [ ] write worker.py 
      [x] register self with backend web server on startup, via a post request


DONE:  
  
 [x] Test: get process.py to doctest (possibly explicitly from command line)

 [x] Test: create file test.py that:
      (1) when run with no args, runs all test suites
      (2) when run with args, tests only those modules

 [x] store process-port.pid file somewhere else.

Session 1:
  [x] 8:42am-9:42am configure frontend again, due to vbox crash = corruption!
  [x] 9:41am-?: configure backend Virtualbox Machine (a clone of
        frontend but with different ssh key setup)

Session 2:
  [x] setup sagews_worker_2 and _3 accounts on laptop
  [x] put conf.json file in sagews-worker
  [x] test ssh through ssh to start stuff.
  [x] database of worker accounts
  [x] remove worker code I won't be using
  [x] learn/install memcached
  [x] implementing backend getting worker config via ssh (and scp'ing files?) and storing in database
  [x] initializing worker machine
  [X] *BUG* this hangs forever:
sage backend.py
Then:
misc.post('http://localhost:8080/worker/init', {'username':'sagews', 'hostname':'worker', 'path':'.'})
and leaves behind a sleeping process:
blastoff:~ wstein$ ps ax |grep 15045
15045 s000  S+     0:00.25 python backend_blocking.py --init_worker --username=sagews --hostname=worker --path="."
  [x] make ip addresses of virtualbox machines static


  [x] worker starting via ssh (and scp'ing files, etc.)
      - looking at resource limits package in python
(but needs testing)
  [x] read http://www.python.org/dev/peps/pep-3143/
  [x] support ssl in worker.py for communication
      


  [x] need to start integrating centralized logging everywhere in my work ASAP; will make everything easier:
       Good Python logging tutorial: http://plumberjack.blogspot.com/2009/09/python-logging-101.html
Which processes will log:

    * backends: 
        - log worker init
        - log workspace start/stop/save events
        - log inputs
        - log replication of workspace bundles

    * frontends:
        - log account creation
        - log user connecting
        - log all kinds of actions that users take, e.g., renaming workspaces
        - log admin actions

Only the above machines will be allowed to log.  In particular, the
workers will not log because: (1) they are untrusted so could corrupt
logs, and (2) this way they *only* have to be able to make a network
connection to one *single* ip on the same network -- the backend.  The
log process will have a whitelist of allowed machines, and will deny 
connections (or refuse to log) anything from any other machines.

Both the backend and frontend processes are running Tornado servers,
so the logging can easily be via Tornado's non-blocking SSL sockets.
I need to use SSL sockets, since the log messages contain potentially
sensitive information and it would be stupid to send them in the
clear.

The logs gets stored in a database.

The logs need to get monitored in real time, possibly building
statistical models, and messages get sent when things need to be dealt
with.

Steps to implement this:

   [ ] implement a new python handler built on tornadio's ssl non-blocking sockets
       [x] define a new class
       [x] implement emit. 
       [x] see what else I might need from the examples in the loggers module in python
       [x] make messages record the ip address of the sender automatically
       [x] must be robust against logging server dieing.

   [ ] implement a logging server/daemon using ideas from worker.py and ssl
       [x] argparse
       [x] ssl cert stuff
       [x] forking socket server
       [x] only accept connection from explicit whitelist of ip in a config file --whitelist=list
       [x] implement sqlalchemy database schema 
       [x] writing to database with timestamp column, so easy to drop old rows
-->       [ ] analytics hooks:  
            - right after writing message to database, call a function that has in-memory history
            - under various conditions things will happen (e.g. email sent)
       [ ] feature -- automatic replication to another log server
           database (via a non-blocking subprocess and ssl socket --
           don't use ssh since that would be too insecure)

   [ ] interface to log database:
       [ ] command line functions for standard queries
       [ ] web interface to same command line functions




-----------------------
  [x] make it so my new logger gets used. 
  [x] make it so when frontend starts backend, it passes useful --log message.
  [x] way to push config info from frontend to backend about workers
  [x] make worker.py support ssl by default when using network socket
  [x] make backend.py support ssl by default when using network socket
