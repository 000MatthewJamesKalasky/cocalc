- [x] make "destroy_project" safe.; make replicate first repair snapshot list in db.

- [x] when doing the recv, if we get this error: "cannot receive incremental stream"
      run a rollback and try one more time.  This is pretty safe, in that it only looses
      non-snapshotted data, which is pretty minimal.

- [x] what to do about missing snasphots on targets?  maybe zfs can add those in?  These were because I neglected to use "-I", etc.

- [x] add last replication error to status.

- [x] storage: make it so projects don't get (re-)opened on disabled-for-maintenance hosts

        alter table storage_topology add disabled boolean;
        update storage_topology set disabled=true where data_center='0' and host='10.1.1.4';
        update storage_topology set disabled=true where data_center='0' and host='10.1.2.4';

- [x] stop hubs on web1, web2, so they don't cause trouble.

- [x] move everything off of cloud1 and cloud2
     [x] disable dns --
         https://dns.godaddy.com/ZoneFile.aspx?zone=SAGEMATHCLOUD.COM&zoneType=0&sa=isc%3dbb1455jd90&refer=dcc&marketID=en-US
     [x] disable haproxy pointing out hubs on cloud1, cloud2 (edited hosts conf file and restart haproxy)

- [ ] clean up the flier -- https://cloud.sagemath.com/projects/3e44a434-c2f0-43ff-87bf-affdb1efcdbe/files/flyer-meeting/flyer01.tex

- [ ] on first run: first /home/salvus/salvus/salvus/data/local/sbin/tincd --kill   then normal tinc

- [ ] close all projects on a given machine

- [ ] automated zpool scrub

- [ ] automated nodetool repair

- [ ] automated replication repair

- [ ] upgrade to sage-6.0
- [ ] double number of GCE compute nodes
- [ ] run web nodes on GCE
- [ ] run database on GCE
- [ ] route53 dns

  - [ ] in all cases: need to lock the websocket (I don't think I implemented this)

  - [ ] things to monitor:
          - root fs disk usage on compute machines (due to storage temp files)
          - available space on each project zpool

  - [ ] very long sagemathcloud.log

  - [ ] locking the message websocket: https://mail.google.com/mail/u/0/?shva=1#inbox/14379ddcde5abb13

  - [ ] killall often works when pkill doesn't (and vice versa) really need to do both.

  - [ ] (1:30?) implement ui for selecting from locations to move project; no automatic move, ever
   - [ ] implement a message "get locations", which returns all ip's where project can be opened, along with the latest snapshot time on each.
   - [ ] change move dialog to first call that, then show one button for each choice.
   - [ ] implement a message: "move project *to*" with target ip address

   - [ ] implement a message "get load", which returns the load on a given list of machines.


 - [ ] fix this printing issue -- https://mail.google.com/mail/u/0/?shva=1#inbox/14367e63a3fa1052
 - [ ] (0:45?) add a `./build.py --build_zfs` option to make updating ZFS easier.

 - [ ] course homework workflow...

 - [ ] see "# TODO: must fix this -- it could overwrite a user bash or ssh stuff.  BAD." in create_project_user.py

 - [ ] check -- is there anything in help or faq about /scratch (?) -- it is out of date.
 - [ ] some hosts, e.g. cloud1 are not on UTC. Should they be?
 - [ ] switch to ubuntu 12.04 at GCE


# Next base image update:

  - [ ] GCE: verify that have fixed storage ssh key on salvus base image

# NEXT

 - [ ] command line "open ." doesn't do what I expect.
 - [ ] add a few people to internal SMC mailing list, e.g., David Ro ] command line "open ." doesn't do what I expect.
 - [ ] add a few people to internal SMC mailing list, e.g., David Roe. - [ ] location autodetect by hub doesn't always work
 - [ ] get SMC in SMC working again
 - [ ] javascript hook to record the diff stream
 - [ ] preserve own choice of ssh private key for ssh server
 - [ ] snapshot time doesn't work in Firefox.
 - [ ] project server restart maybe isn't working?
 - [ ] remove the home images in servers from compute vm's and reboot each one
 - [ ] copy all the computex-home.img's to one computer to delete later.
 - [ ] upgrade to sage 6.0
 - [ ] snap: delete snap cassandra user everywhere
 - [ ] bug: "%load foo.js"
 - [ ] route 53 dns instead of what I have now;
 - [ ] get google webserver updated (108.59.84.126) and added to dns.
 - [ ] make script for raising quota:
        s=require('storage'); s.init()
        s.quota(project_id:'b33ef4d2-fbf3-467a-a5ec-2ebc5033afec', size:'20G', cb:console.log)
 - [ ] put the /mnt/snap scripts from 10.1.1.3 somewhere good and use 'em, e.g., to repair db, scrub zfs.
 - [ ] implement automatic regular scrub of zfs.
 - [ ] port forwarding:
         ssh -L cloud1.math.washington.edu:4567:10.1.2.4:4567 ce2d267d00df42deab4464509a5f3e74@10.1.2.4


# DONE


  - [x] rebuild ZFS on base machine; kernel upgrade;
  - [x] (0:16?) anna unicode bug: https://mail.google.com/mail/u/0/?shva=1#starred/1437897c46edae24
  - [x] admin monitor:
         - make it report any single errors checking on DNS
         - make it report *and email me* if number of zfs processes exceeds 100 (?)
         - make it email if available disk space in zpool is less than 50GB
         - make it email if load exceeds some percentage.
  - [x] turn off core dump on gce nodes!  see http://thesystemadministrator.net/cpanel/how-to-disable-core-dumps-on-linux-servers
              echo '* soft core 0' >> /etc/security/limits.conf

 - [x] add function to storage:

        x={}; s.get_snapshots(project_id:'ce2d267d-00df-42de-ab44-64509a5f3e74',cb:(e,s) -> console.log(([a, s[a][0]] for a in require('misc').keys(s))))

 - [x] editor: go editor mode.

- [x] ZFS is deadlocked on 10.1.6.4...
      More longterm, I desperately need to figure out how to not deadlock ZFS.
      This could be done by maybe doing exactly one ZFS operation per project at a time.  (?)  I should at least read about all deadlock bugs.  Also setup a monitor so I know fast that this is happening.
 - [x] start using cloud13 again
- [x] (0:25) lock so projects can't be moved automatically
     - [x] make it so storage doesn't set location=undefined on close, instead only killing all procs; push out new code
     - [x] change move UI to bring up a "temp. disabled" message, and email me if files vanished

- [x] (0:30?) (0:12+) move MRC project back (in progress)


 - [x] (1:00?) (0:43) make a git repo for my class with basic content
 - [x] (0:15?) (0:43) warm-up: "The Sagemath Cloud" --> SageMathCloud
 - [x] (0:24) get projects to work locally on my laptop and move class to one.
 - [x] (1:00?) (0:54) investigate changing the uid schema gracefully for the zpool's
        this can be done but it is a tedious rsync process
        using replication doesn't work, since can't chmod without breaking the snapshot (despite same name).  DUH.
              465  zfs create projects/tmp
              466  rsync -axvH /projects/10f2b056-285b-48b2-b468-8df293b943a9/.zfs/snapshot/2014-01-07T19:39:12/ /projects/tmp/
              467  chown -R 10f2b056285b48b2b4688df293b943a9. /projects/tmp/
              468  zfs snapshot projects/tmp@2014-01-07T19:39:12
              469  rsync -axvH /projects/10f2b056-285b-48b2-b468-8df293b943a9/.zfs/snapshot/test/ /projects/tmp/
              470  chown -R 10f2b056285b48b2b4688df293b943a9. /projects/tmp/
              471  zfs snapshot projects/tmp@test
              472  rsync -axvH /projects/10f2b056-285b-48b2-b468-8df293b943a9/ /projects/tmp/
              473  chown -R 10f2b056285b48b2b4688df293b943a9. /projects/tmp/
  - [x] (0:45?) (0:41) implement chown for a zfs filesystem
  - [x] (1:00?) make location the hostname *and* optionally a ZFS filesystem name (default = "projects");
  Worry -- it will just cause trouble...
  Better to just do this dev stuff with multiple vm's or whatever.
  How would it work?
     - in database, add pool; the default is 'projects'.
     - {"host":"localhost","pool":"projects2","username":"cd9c78cfff8143dc8c35a1269e8489f4","port":22,"path":"."}
     - Adapt *all* the code in storage.coffee to take an optional pool argument.
     - change in database the "locations" mapping so that the key can be 'hostname:poolname', e.g.,
          {'localhost:projects2': '["2014-01-07T21:04:09","2014-01-07T21:02:37","2014-01-07T21:00:49",...}
     - I just don't like this.  It will make bad use of space.  And is bug prone.
 I tried to mirror out to OSX, but the zfs stream version is different, so it doesn't work.  Wasted 30 minutes on that.


 - [x] make a new base vm and restart compute vm's using it.
 - [x] update gce base vm and re-create all compute vm's using it.  Could be issues
 - [x] re-install /root ssh
 - [x] in parallel
    - [x] do two migrates
    - [x] add option to migrate so it migrates project *and* sets location to null!
    - [x] more migrates

'f56643b6-e96d-4a73-83b2-cea7bd5298eb': { 'src-10.1.2.4-dest-10.1.6.4': 'destroyed target project -- cannot receive incremental stream: most recent snapshot of projects/f56643b6-e96d-4a73-83b2-
cea7bd5298eb does not\nmatch incremental source\n' },
     'dac4431b-1ed2-4c15-ae41-8a7dde4a8622

DO IT

 - [x] migrate
 - [x] in parallel

     - [ ] do another migrate, then set all to null:

         r=require('storage'); r.init()
         y={};r.migrate_all(limit:25000,cb:(a,b)->x.a=a;y.b=b)
         r.migrate_unset_all_locs(cb:console.log)

     - [ ] reboot all web vm's:

         [cloud.restart('vm',hostname='web%s'%i) for i in range(1,8) + range(10,22)]

 - [x] start all hub servers and nginx servers
 - [x] send an email; encourage testing and feedback about issues.
 - [x] start using cloud13 again
 - [x] (1:00?) a database field to disable project timeouts
 - [x] (1:30?) hub: implement automatic timeout when inactive, removing existing code; honor database field to ignore timeouts
 - [x] (0:30?) snap: disable in startup scripts...
 - [x] (0:45?) look at, try, new print script


- [x] (0:45?) bug: file chat broken by switch; it's an "open before file create" issue; touching file makes it work. Has to do with code for dealing with read-only files that I added.



- [x] come up with ideas for Sage Days 56:

 - make some use of GCE for sage dev (?)
 - organize bug days
 - triage bugs
 - online homework system
 - ideas for better use of snapshots:
     - tags like git; could be used in url, e.g.,  <https://cloud.sagemath.com/projects/af95e67e-809d-49b1-a323-5c7e441b06e5/tags/foobar/>
        - the tag would just be mapped to existing snapshot names; just a simple map in projects table.
        - user wouldn't be allowed to delete snapshot without big fat warning.
     - could have user-wide tags, which point to a specific version of a file/directory in a project:
         <https://cloud.sagemath.com/users/user_id/tags/foobar>
       (or when we have usernames, `user_id --> username`).  If foobar not given, would list all tags for that user.


 - user guidebook
 - marketing campaign ideas
     - blog post ideas
 - business ideas
     - early signup price (like google play): basically only way to get permanent discount
 - brainstorm ideas that are very fast because of using ZFS, e.g., clone/forking/etc.
 - Sage infrastructure discussions.
 - optimize (with Andrew) the file listing script



BUGS:
 - [ ] "open .git" from terminal fails
 - [ ] open often shows wrong tab


DONE:


- [ ] (2:00?) setup working SMC dev environment on vm... unless I run into a show-stopper issue with missing deps
       - [x] update database schema (used "git diff <commit_id> db_schema.cql")
       - [x] create projects zpool locally:
                - [x] shutdown vm
                - [x] add a new 16GB disk image
                - [x] setup compressed and dedup'd as zpool "projects":
                       zpool create -f projects /dev/sdb
                       zfs set dedup=on projects
                       zfs set compression=lz4 projects
                - [x] create storage user with sudo zfs access
                - [x] new script in /usr/local/bin/ (create_project_user.py)

       - [x] get a new project to work there
               - need to add to storage_topology table in db.
                   update storage_topology set vnodes=256 where data_center='0' and host='localhost';
       - [x] "git pull" a copy of my dev code to a new project

- [x] January SMC development log.
---------------------------
I'm on a flight to Hawaii, and I'm not going to get to do any real
SMC development with testing... because I used ZFS on a file on OS X,
and it DIED killing all my vm's when I stupidly (?) hard rebooted my
machine.
  Or NOT.  I couldn't import since they are already imported! Duh.!
  - update my smc plans, ideas, etc. a LOT
  - plan out course grading functionality



 - [x] GCE and libc issues
 - [x] debug: closing stale projects...
    Trace
        at exports.defaults (/home/salvus/salvus/salvus/node_modules/misc.js:65:15)
        at Object.exports.close_stale_projects (/home/salvus/salvus/salvus/node_modules/storage.js:616:12)
        at close_stale_projects (/home/salvus/salvus/salvus/node_modules/hub3.js:7009:20)
        at wrapper [as _onTimeout] (timers.js:252:14)
        at Timer.listOnTimeout [as ontimeout] (timers.js:110:15)
    /home/salvus/salvus/salvus/node_modules/misc.js:66
    w "misc.defaults -- TypeError: function takes inputs as an object " + (error()                                                                       ^
    misc.defaults -- TypeError: function takes inputs as an object (obj1=undefined, obj2={"ttl":86400,"dry_run":true,"limit":20,"cb":"__!!!!!!this is a required property!!!!!!__"})
 - [x] snap: delete the images on machines with low disk
 - [x] printing: add a line "\usepackage[utf8]{inputenc}" (after \usepackage{amsmath} ) the correct characters appears in the PDF generated by the print button: https://mail.google.com/mail/u/0/?shva=1#search/sagews2pdf/143539cb372003f5


# Things that can go wrong:

   [ ]  modified but not snapshotted:

         s.send(project_id:'4a5f0542-5873-4eed-a85c-a18c706e8bcd',source:{"version":"2014-01-11T17:47:43","host":"10.3.2.4"}, dest:{"version":"2013-12-18T00:21:55","host":"10.1.15.4"},cb:console.log)

        "cannot receive incremental stream: destination projects/4a5f0542-5873-4eed-a85c-a18c706e8bcd has been modified since most recent snapshot"

   [ ] but wait:

       debug: replicate (3702601d-9fbc-4e4e-b7ab-c10a79e34d3b): stored last replication error in database: {"src-10.3.4.4-dest-10.3.1.4":"cannot receive incremental stream: destination projects/3702601d-9fbc-4e4e-b7ab-c10a79e34d3b has been modified\nsince most recent snapshot\n"}

       and yet the diff is empty:

       root@compute1dc2:/projects/3702601d-9fbc-4e4e-b7ab-c10a79e34d3b# zfs diff projects/3702601d-9fbc-4e4e-b7ab-c10a79e34d3b@2014-01-11T21:15:36 projects/3702601d-9fbc-4e4e-b7ab-c10a79e34d3b


   [ ] edited but not mounted

        cannot mount \'/projects/3702601d-9fbc-4e4e-b7ab-c10a79e34d3b\': directory is not empty\n

        This was caused by me manually moving to -DELETE but not changing mountpoint:

           root@compute16a:/home/salvus# mount |grep /projects/3702601d-9fbc-4e4e-b7ab-c10a79e34d3b
projects/3702601d-9fbc-4e4e-b7ab-c10a79e34d3b-DELETE on /projects/3702601d-9fbc-4e4e-b7ab-c10a79e34d3b type zfs (rw,noatime,xattr)

FACTS:
    - snapshot stream *can* get out of sync, and still keep working, i.e., all that is needed to replicate is that most recent snapshot is there.  If older ones get deleted or whatever, no problem.


--

- [x] delete all replication files in /home/storage that are >= 6 hours old:
         find /home/storage/.storage* -type f -mmin +360 -delete

