
- [x] (0:14) update readme

- [x] (0:45?) (0:34) latex editor: don't render errors/warnings after a certain number of errors.  If there are thousands then browser hangs.

- [x] (0:30?) (0:16) latex editor: scrollbar for list of errors is not visible

- [x] (1:00?) (1:23) quick fix snapshot times in Firefox and Safari (since will be changing this); most time spent hacking on IE, which was wasted time.

- [x] (1:00?) upgrade to codemirror 3.21: https://mail.google.com/mail/u/0/?shva=1#inbox/1439c770436eb409

- [ ] (0:45?) add last mod timeago to directories (not just for files)

- [ ] (0:30?) fix new project dialog -- modal-body div looks all weird.

- [ ] (1:00?) storage: write something that restarts all open projects on a given host (similar to close)

- [ ] (1:30?) upgrade sockjs, which has seen new development: https://github.com/sockjs/sockjs-client/commits/master

- [ ] (2:00?) create a new proper backup VM; in the meantime, run the backup script to a projects/ zpool *somewhere*.  Easiest would be a qcow2 image on a full-disk encrypted lvm.  Obviously, for now could do full backup to a GCE image with no processor but lots of space and some new random 14-character password (?). Can also do a big encrypted sparse image bundle on OS X again...

- [ ] servedby.net -- I might as well write to them and let them know precisely what I'm doing and ask if they are interested in "matching" what Google does.


# for course grading workflow

- [ ] once backup is done, start making backups of recently modified projects on a regular basis.

- [ ] groups = owners; collection of projects and easy way to add/remove/create; that's all it.

        CREATE TABLE groups (
             group_id         uuid  PRIMARY KEY,
             title            varchar,
             description      varchar,
             owners           set<uuid>,
             collaborators    set<uuid>,
             projects         set<uuid>
       );

Also add this to the projects table:

             groups         set<uuid>;    // groups this project is in

Add this to the accounts table:

             groups         set<uuid>,    // groups this user owns

HMMM -- this is replicating how projects already work, so is stupid.
Maybe instead generalize the notion of project?  ???

Some ideas...

    - a project could appear differently depending on the user.  BAD

    - different types of projects:

        - compute project: a replicated Linux account
        - course project:
             - one compute project for each collaborator (inc. owner), shared with project owners
             - a shared compute project
             - it's own UI view that provides lots of information about the linked projects, configuration, etc.



- [ ] bug: fuse is now broken for users. UGH.

- [ ] send = target a (path in a) project *or* group; optional time cutoff

- [ ] get  = from path in a project or group
- [ ] snapshot browsing is broken on ie due to their toLocaleTimeString being WEIRD -- it puts hidden unicode characters all over.
- [ ] (0:30?) hit return for submit on password reset dialog
- [ ] (0:30?) enter password twice in password reset dialog
- [ ] course workflow: group of projects: 1 private, 1 shared by all, 1 between each; push/pull of files
- [ ] need to lock the (web)socket  when sending message -- (did I implement this?): https://mail.google.com/mail/u/0/?shva=1#inbox/14379ddcde5abb13
- [ ] codemirror unindent bug
- [ ] triple check that all ssh'ing ignores host keys -- we have a vpn so don't need them.
- [ ] read about cgroups
- [ ] rotate out very long .sagemathcloud.log's
- [ ] on first run: first /home/salvus/salvus/salvus/data/local/sbin/tincd --kill   then normal tinc
- [ ] automate zpool scrub's
- [ ] automate nodetool repair's
- [ ] upgrade to sage-6.0
- [ ] switch to ubuntu 12.04 on GCE
- [ ] double number of GCE compute nodes
- [ ] run web nodes on GCE
- [ ] run database on GCE
- [ ] route53 dns
- [ ] add to monitor: available space on each project zpool
- [ ] add to monitor: root fs disk usage on compute machines (due to storage temp files)
- [ ] implement ui for selecting from locations to move project to
- [ ] implement a message "get locations", which returns all ip's where project can be opened, along with the latest snapshot time on each.
- [ ] change move dialog to first call that, then show one button for each choice.
- [ ] implement a message: "move project *to*" with target ip address
- [ ] implement a message "get load", which returns the load on a given list of machines.
- [ ] fix this printing issue -- https://mail.google.com/mail/u/0/?shva=1#inbox/14367e63a3fa1052
- [ ] add a `./build.py --build_zfs` option to make updating ZFS easier.
- [ ] see "# TODO: must fix this -- it could overwrite a user bash or ssh stuff.  BAD." in create_project_user.py
- [ ] check -- is there anything in help or faq about /scratch (?) -- it is out of date.
- [ ] some hosts, e.g. cloud1 are not on UTC. Should they be?
- [ ] command line "open ." doesn't do what I expect.
- [ ] location autodetect by hub doesn't always work
- [ ] make SMC in SMC work again
- [ ] project server restart maybe isn't working?
- [ ] remove the home images in servers from compute vm's and reboot each one
- [ ] copy all the computex-home.img's to one computer to delete later.
- [ ] snap: delete snap cassandra user in database credentials
- [ ] snap: delete it all
- [ ] bug: "%load foo.js" fails
- [ ] route 53 dns
- [ ] port forwarding: "ssh -L cloud1.math.washington.edu:4567:10.1.2.4:4567 ce2d267d00df42deab4464509a5f3e74@10.1.2.4"





------




# DONE

- [x] make script for raising quota:
    s=require('storage'); s.init()
    s.quota(project_id:'b33ef4d2-fbf3-467a-a5ec-2ebc5033afec', size:'20G', cb:console.log)

  - [x] rebuild ZFS on base machine; kernel upgrade;
  - [x] (0:16?) anna unicode bug: https://mail.google.com/mail/u/0/?shva=1#starred/1437897c46edae24
  - [x] admin monitor:
         - make it report any single errors checking on DNS
         - make it report *and email me* if number of zfs processes exceeds 100 (?)
         - make it email if available disk space in zpool is less than 50GB
         - make it email if load exceeds some percentage.
  - [x] turn off core dump on gce nodes!  see http://thesystemadministrator.net/cpanel/how-to-disable-core-dumps-on-linux-servers
              echo '* soft core 0' >> /etc/security/limits.conf

 - [x] add function to storage:

        x={}; s.get_snapshots(project_id:'ce2d267d-00df-42de-ab44-64509a5f3e74',cb:(e,s) -> console.log(([a, s[a][0]] for a in require('misc').keys(s))))

 - [x] editor: go editor mode.

- [x] ZFS is deadlocked on 10.1.6.4...
      More longterm, I desperately need to figure out how to not deadlock ZFS.
      This could be done by maybe doing exactly one ZFS operation per project at a time.  (?)  I should at least read about all deadlock bugs.  Also setup a monitor so I know fast that this is happening.
 - [x] start using cloud13 again
- [x] (0:25) lock so projects can't be moved automatically
     - [x] make it so storage doesn't set location=undefined on close, instead only killing all procs; push out new code
     - [x] change move UI to bring up a "temp. disabled" message, and email me if files vanished

- [x] (0:30?) (0:12+) move MRC project back (in progress)


 - [x] (1:00?) (0:43) make a git repo for my class with basic content
 - [x] (0:15?) (0:43) warm-up: "The Sagemath Cloud" --> SageMathCloud
 - [x] (0:24) get projects to work locally on my laptop and move class to one.
 - [x] (1:00?) (0:54) investigate changing the uid schema gracefully for the zpool's
        this can be done but it is a tedious rsync process
        using replication doesn't work, since can't chmod without breaking the snapshot (despite same name).  DUH.
              465  zfs create projects/tmp
              466  rsync -axvH /projects/10f2b056-285b-48b2-b468-8df293b943a9/.zfs/snapshot/2014-01-07T19:39:12/ /projects/tmp/
              467  chown -R 10f2b056285b48b2b4688df293b943a9. /projects/tmp/
              468  zfs snapshot projects/tmp@2014-01-07T19:39:12
              469  rsync -axvH /projects/10f2b056-285b-48b2-b468-8df293b943a9/.zfs/snapshot/test/ /projects/tmp/
              470  chown -R 10f2b056285b48b2b4688df293b943a9. /projects/tmp/
              471  zfs snapshot projects/tmp@test
              472  rsync -axvH /projects/10f2b056-285b-48b2-b468-8df293b943a9/ /projects/tmp/
              473  chown -R 10f2b056285b48b2b4688df293b943a9. /projects/tmp/
  - [x] (0:45?) (0:41) implement chown for a zfs filesystem
  - [x] (1:00?) make location the hostname *and* optionally a ZFS filesystem name (default = "projects");
  Worry -- it will just cause trouble...
  Better to just do this dev stuff with multiple vm's or whatever.
  How would it work?
     - in database, add pool; the default is 'projects'.
     - {"host":"localhost","pool":"projects2","username":"cd9c78cfff8143dc8c35a1269e8489f4","port":22,"path":"."}
     - Adapt *all* the code in storage.coffee to take an optional pool argument.
     - change in database the "locations" mapping so that the key can be 'hostname:poolname', e.g.,
          {'localhost:projects2': '["2014-01-07T21:04:09","2014-01-07T21:02:37","2014-01-07T21:00:49",...}
     - I just don't like this.  It will make bad use of space.  And is bug prone.
 I tried to mirror out to OSX, but the zfs stream version is different, so it doesn't work.  Wasted 30 minutes on that.


 - [x] make a new base vm and restart compute vm's using it.
 - [x] update gce base vm and re-create all compute vm's using it.  Could be issues
 - [x] re-install /root ssh
 - [x] in parallel
    - [x] do two migrates
    - [x] add option to migrate so it migrates project *and* sets location to null!
    - [x] more migrates

'f56643b6-e96d-4a73-83b2-cea7bd5298eb': { 'src-10.1.2.4-dest-10.1.6.4': 'destroyed target project -- cannot receive incremental stream: most recent snapshot of projects/f56643b6-e96d-4a73-83b2-
cea7bd5298eb does not\nmatch incremental source\n' },
     'dac4431b-1ed2-4c15-ae41-8a7dde4a8622

DO IT

 - [x] migrate
 - [x] in parallel

     - [ ] do another migrate, then set all to null:

         r=require('storage'); r.init()
         y={};r.migrate_all(limit:25000,cb:(a,b)->x.a=a;y.b=b)
         r.migrate_unset_all_locs(cb:console.log)

     - [ ] reboot all web vm's:

         [cloud.restart('vm',hostname='web%s'%i) for i in range(1,8) + range(10,22)]

 - [x] start all hub servers and nginx servers
 - [x] send an email; encourage testing and feedback about issues.
 - [x] start using cloud13 again
 - [x] (1:00?) a database field to disable project timeouts
 - [x] (1:30?) hub: implement automatic timeout when inactive, removing existing code; honor database field to ignore timeouts
 - [x] (0:30?) snap: disable in startup scripts...
 - [x] (0:45?) look at, try, new print script


- [x] (0:45?) bug: file chat broken by switch; it's an "open before file create" issue; touching file makes it work. Has to do with code for dealing with read-only files that I added.



- [x] come up with ideas for Sage Days 56:

 - make some use of GCE for sage dev (?)
 - organize bug days
 - triage bugs
 - online homework system
 - ideas for better use of snapshots:
     - tags like git; could be used in url, e.g.,  <https://cloud.sagemath.com/projects/af95e67e-809d-49b1-a323-5c7e441b06e5/tags/foobar/>
        - the tag would just be mapped to existing snapshot names; just a simple map in projects table.
        - user wouldn't be allowed to delete snapshot without big fat warning.
     - could have user-wide tags, which point to a specific version of a file/directory in a project:
         <https://cloud.sagemath.com/users/user_id/tags/foobar>
       (or when we have usernames, `user_id --> username`).  If foobar not given, would list all tags for that user.


 - user guidebook
 - marketing campaign ideas
     - blog post ideas
 - business ideas
     - early signup price (like google play): basically only way to get permanent discount
 - brainstorm ideas that are very fast because of using ZFS, e.g., clone/forking/etc.
 - Sage infrastructure discussions.
 - optimize (with Andrew) the file listing script



BUGS:
 - [ ] "open .git" from terminal fails
 - [ ] open often shows wrong tab


DONE:


- [ ] (2:00?) setup working SMC dev environment on vm... unless I run into a show-stopper issue with missing deps
       - [x] update database schema (used "git diff <commit_id> db_schema.cql")
       - [x] create projects zpool locally:
                - [x] shutdown vm
                - [x] add a new 16GB disk image
                - [x] setup compressed and dedup'd as zpool "projects":
                       zpool create -f projects /dev/sdb
                       zfs set dedup=on projects
                       zfs set compression=lz4 projects
                - [x] create storage user with sudo zfs access
                - [x] new script in /usr/local/bin/ (create_project_user.py)

       - [x] get a new project to work there
               - need to add to storage_topology table in db.
                   update storage_topology set vnodes=256 where data_center='0' and host='localhost';
       - [x] "git pull" a copy of my dev code to a new project

- [x] January SMC development log.
---------------------------
I'm on a flight to Hawaii, and I'm not going to get to do any real
SMC development with testing... because I used ZFS on a file on OS X,
and it DIED killing all my vm's when I stupidly (?) hard rebooted my
machine.
  Or NOT.  I couldn't import since they are already imported! Duh.!
  - update my smc plans, ideas, etc. a LOT
  - plan out course grading functionality



 - [x] GCE and libc issues
 - [x] debug: closing stale projects...
    Trace
        at exports.defaults (/home/salvus/salvus/salvus/node_modules/misc.js:65:15)
        at Object.exports.close_stale_projects (/home/salvus/salvus/salvus/node_modules/storage.js:616:12)
        at close_stale_projects (/home/salvus/salvus/salvus/node_modules/hub3.js:7009:20)
        at wrapper [as _onTimeout] (timers.js:252:14)
        at Timer.listOnTimeout [as ontimeout] (timers.js:110:15)
    /home/salvus/salvus/salvus/node_modules/misc.js:66
    w "misc.defaults -- TypeError: function takes inputs as an object " + (error()                                                                       ^
    misc.defaults -- TypeError: function takes inputs as an object (obj1=undefined, obj2={"ttl":86400,"dry_run":true,"limit":20,"cb":"__!!!!!!this is a required property!!!!!!__"})
 - [x] snap: delete the images on machines with low disk
 - [x] printing: add a line "\usepackage[utf8]{inputenc}" (after \usepackage{amsmath} ) the correct characters appears in the PDF generated by the print button: https://mail.google.com/mail/u/0/?shva=1#search/sagews2pdf/143539cb372003f5


# Things that can go wrong:

   [ ]  modified but not snapshotted:

         s.send(project_id:'4a5f0542-5873-4eed-a85c-a18c706e8bcd',source:{"version":"2014-01-11T17:47:43","host":"10.3.2.4"}, dest:{"version":"2013-12-18T00:21:55","host":"10.1.15.4"},cb:console.log)

        "cannot receive incremental stream: destination projects/4a5f0542-5873-4eed-a85c-a18c706e8bcd has been modified since most recent snapshot"

   [ ] but wait:

       debug: replicate (3702601d-9fbc-4e4e-b7ab-c10a79e34d3b): stored last replication error in database: {"src-10.3.4.4-dest-10.3.1.4":"cannot receive incremental stream: destination projects/3702601d-9fbc-4e4e-b7ab-c10a79e34d3b has been modified\nsince most recent snapshot\n"}

       and yet the diff is empty:

       root@compute1dc2:/projects/3702601d-9fbc-4e4e-b7ab-c10a79e34d3b# zfs diff projects/3702601d-9fbc-4e4e-b7ab-c10a79e34d3b@2014-01-11T21:15:36 projects/3702601d-9fbc-4e4e-b7ab-c10a79e34d3b


   [ ] edited but not mounted

        cannot mount \'/projects/3702601d-9fbc-4e4e-b7ab-c10a79e34d3b\': directory is not empty\n

        This was caused by me manually moving to -DELETE but not changing mountpoint:

           root@compute16a:/home/salvus# mount |grep /projects/3702601d-9fbc-4e4e-b7ab-c10a79e34d3b
projects/3702601d-9fbc-4e4e-b7ab-c10a79e34d3b-DELETE on /projects/3702601d-9fbc-4e4e-b7ab-c10a79e34d3b type zfs (rw,noatime,xattr)

FACTS:
    - snapshot stream *can* get out of sync, and still keep working, i.e., all that is needed to replicate is that most recent snapshot is there.  If older ones get deleted or whatever, no problem.


--

- [x] delete all replication files in /home/storage that are >= 6 hours old:
         find /home/storage/.storage* -type f -mmin +360 -delete

- [x] fix keyboard repeat on my laptop: defaults write -g ApplePressAndHoldEnabled -bool false
      See https://discussions.apple.com/thread/3190706

- [x] make "destroy_project" safe.; make replicate first repair snapshot list in db.

- [x] when doing the recv, if we get this error: "cannot receive incremental stream"
      run a rollback and try one more time.  This is pretty safe, in that it only looses
      non-snapshotted data, which is pretty minimal.

- [x] what to do about missing snasphots on targets?  maybe zfs can add those in?  These were because I neglected to use "-I", etc.

- [x] add last replication error to status.

- [x] storage: make it so projects don't get (re-)opened on disabled-for-maintenance hosts

        alter table storage_topology add disabled boolean;
        update storage_topology set disabled=true where data_center='0' and host='10.1.1.4';
        update storage_topology set disabled=true where data_center='0' and host='10.1.2.4';

- [x] stop hubs on web1, web2, so they don't cause trouble.

- [x] move everything off of cloud1 and cloud2
     [x] disable dns --
         https://dns.godaddy.com/ZoneFile.aspx?zone=SAGEMATHCLOUD.COM&zoneType=0&sa=isc%3dbb1455jd90&refer=dcc&marketID=en-US
     [x] disable haproxy pointing out hubs on cloud1, cloud2 (edited hosts conf file and restart haproxy)

- [x] sometimes hubs just go "crazy" trying to replicate, which makes the hub itself get very slow and not respond.  High cpu.  This has to be a bug in storage.


- [ ] this seems silly:
salvus@compute4dc2:~$ ps ax |grep zfs |grep send |wc -l
617
salvus@compute4dc2:~$ ps ax |grep zfs |grep recv |wc -l
313
salvus@compute4dc2:~$ sudo su
root@compute4dc2:/home/salvus# cd /home/storage
root@compute4dc2:/home/storage# du -sch .
13G     .
13G     total
root@compute4dc2:/home/storage# ls .storage* |wc -l
11956

- [x] clean up the flier -- https://cloud.sagemath.com/projects/3e44a434-c2f0-43ff-87bf-affdb1efcdbe/files/flyer-meeting/flyer01.tex

- [x] y={};s.projects_needing_replication(cb:(e,n)->y.n=n); x={};s.replication_errors(cb:(e,n)->x.n=n)

- [x] close all projects on a given machine

- [x] killall often works when pkill doesn't (and vice versa) really need to do both.

- [x] automate replication repair

- [x] delete an account:

        cqlsh:salvus> select account_id from accounts where email_address='shanikaseales@gmail.com';
        cqlsh:salvus> update accounts set email_address=null,first_name=null,last_name=null where account_id=708c6228-df8f-47c5-9d28-31b015504199;
        cqlsh:salvus> delete from email_address_to_account_id where email_address='shanikaseales@gmail.com';

# SUPER top priority

 - [x] setup and start running an encrypted off-site backup of all projects in new format
     PLAN:
         - [x] upgrade
         - [x] build and install zfs
         - [x] use image files: start with a single sparse 1TB file (?)
              mkdir /zfs
              cd /zfs
              truncate -s 1000G 1.img
              zpool create -m /projects projects zfs/1.img
              zfs set dedup=on projects
              zfs set compression=lz4 projects
         - [x] compress and dedup as much as possible... as usual
         - [x] make it so my backup machine's key is trusted by all "storage" users (both on live machines and in base images)
         - [x] setup database access
         - [x] write function in storage that goes through and makes/updates backup of each project, etc.


- [x] add clawpack to standard Sage packages: "export LDFLAGS=-shared; pip install clawpack"

# Next base image update:

- [x] GCE: verify that have fixed storage ssh key on salvus base image
