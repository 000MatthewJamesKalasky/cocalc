# Next Update


# TODO

- [ ] new encrypted offsite backup.
      (1) finished the zpool of all projects
      (2) now re-building/encrypting it:
          time kvm-img convert  -f qcow2 -O qcow2 -o encryption=on,cluster_size=2M  backup1-projects.img  ENCRYPTED-backup1-projects.img

- [ ] forkbomb r: 4b3894ff-073b-4c54-becd-6bb31f4a0c9a on compute17a

- [ ] zfs replication: several errors that aren't getting sorted out.  RESOLVE THEM.

- [ ] add haskell editor syntax highlighting support.

- [ ] (2:00?) project move -- redo dialog to provide info about targets.

    - [x] (0:42) client message to get snapshot/host status
   --> - [ ] ui to display move targets
    - [ ] send move request with specific target; get back result and display message

- [ ] watch quotas of these:

94a3a0b8-0257-43ef-9986-89d7fc789a72
6ff32b4a-1747-4760-9f83-aaf694e17006
9ac5a8ca-7dbf-4b27-adea-4709b0fb7105
e51a48b0-8b47-4d20-ac2d-e9aa7a4462ba

- [ ] storage issue: we need a way to deal with this:

        storage@compute7a:~$ sudo zfs rename projects/03caf13c-23ce-452f-a0c7-29af4e160c24 projects/DELETED-2014-02-01T17:43:19-03caf13c-23ce-452f-a0c7-29af4e160c24
        cannot rename 'projects/03caf13c-23ce-452f-a0c7-29af4e160c24': dataset is busy

    Doing a proper reboot of 10.1.7.4:

        # in cqlsh
        update storage_topology set disabled=true where data_center='0' and host='10.1.7.4';

        # in storage:
        s=require('storage'); s.init()
        x={};s.close_all_projects(host:'10.1.7.4', limit:20, cb:(e)->x.e=e)

        # in admin ipython console
        import admin; import cassandra; reload(cassandra); reload(admin); cloud = admin.Services('conf/deploy_cloud/')
        cloud.restart('vm',hostname='compute7a')

        # wait for this to show nothing
        ps ax |grep zpool

        # special case: I added more storage
        fdisk /dev/vdd # no partitions!
        sudo zpool add projects -f /dev/vdd

        # after it comes up fully
        update storage_topology set disabled=null where data_center='0' and host='10.1.7.4';


- [ ] HIGH PRIORITY: wrapping did nothing; the problem is that res isn't even defined in
        /home/salvus/salvus/salvus/node_modules/http-proxy/lib/http-proxy/passes/ws-incoming.js
      so it is a bug in the proxy library.  DANG.   Check on upstream.  For today, just edit manually.
      I just *temporarily* commented out the whole emit line, since "emit is not defined".  Gees.

- [ ] upgrade font-awesome: https://github.com/FortAwesome/Font-Awesome/wiki/Upgrading-from-3.2.1-to-4

- [ ] implement some things sharelatex has just done (?) -- https://mail.google.com/mail/u/0/?shva=1#inbox/143decd4d3755360

- [ ] upgrade to new coffeescript -- http://ihackernews.com/comments/7139175

- [ ] (0:30?) issue: the timeout to open a file is shorter than the time it often takes to open a closed static project. This means you get an error even though opening would actually have worked fine if given them time.

- [ ] project -- quota report is wrong

- [ ] storage: write code to scan and find all projects that are within 90% of running out of space.

- [ ] snapshots eating space: keep an eye on -- 94a3a0b8-0257-43ef-9986-89d7fc789a72
      it's basically just a bunch of texing, but has used 5GB in *snapshots*.
      This will be a good example for when I write code for trimming old snapshots.

- [ ] copy: enhance copy functionality to send a file to all linked projects

- [ ] collect: button to grab files from other projects (naming with project_id).

- [ ] return: button to return grabbed files from other projects (naming with project_id).

- [ ] spell checker discussion: https://mail.google.com/mail/u/0/?shva=1#inbox/143da6512102abb4  typo.js ?

- [ ] (1:00?) account settings: fix email address change bug; also add a "always confirm on page close" setting.

- [ ] (1:00?) fix franco's security issue: https://mail.google.com/mail/u/0/?shva=1#search/franco/143ca2f6db753e56

- [ ] (3:00?) implement user preference when moving project
      - [ ] (1:00?) implement a message: project_replica_status, which uses the "status" code in storage, more or less.
      - [ ] (1:00?) update ui for when "move" button pushed to grab the latest status and display it; and when moving, you select a given target
      - [ ] (1:00?) change the move project message from client to server to allow for a preference

- [ ] (1:00?) switch over to actually using the new GCE nodes
      - [ ] (0:15?) restart hubs so they start using new nodes on new projects (would be nice if this could wait until after replication ??)
      - [ ] (0:45?) safely close (setting loc=null) projects currently on gce nodes but not running. (might have to write some code)

- [ ] (1:00?) upgrade to jQuery 2.1 -- http://blog.jquery.com/2014/01/24/jquery-1-11-and-2-1-released/

- [ ] (1:30?) truncate large output in worksheets; not doing this can make worksheets un-usable; need link to pop up raw output in external tab.

- [ ] (1:30?) SMC in SMC -- get it working again

- [ ] (1:30?) storage: write cleanup_project -- that cleans up after a particular project, which means:
         - compute full replica status
         - verify that all official locations are up to date
         - identify old locations and destroy them
         - identify project DELETEs and destroy them too (?) -- not possible since not tracked in db right now.









----

- [ ] route 53 dns

- [ ] full text search of projects is stupid; needs to do lots of subsearches... (?)

- [ ] (0:45?) close_stale_projects has some problems:
     - [x] it should space out each close by a few seconds, since doing all at once makes hub SLOW to respond to users.
     - [ ]  it is deciding what is closed/open based on location, which doesn't get reset on close, so it does WAY too many closes!

- [ ] (1:00?) check that cassandra nodes updated, then run "nodetool cleanup".

- [ ] (1:00?) ensure rebooting and/or restarting the cassandra nodes works; maybe have to put some zpool import functionality in admin/startup script.

- [ ] (5:00?) upgrade to bootstrap 3: http://stackoverflow.com/questions/17974998/updating-bootstrap-to-version-3-what-do-i-have-to-do


- [ ] (1:00?) (1:25+) fix firefox right click to copy/paste bug:
       - only a problem on firefox; nowhere else.
       - not a problem in codemirror itself -- something is messed up by some other library in SMC.
       - what's happening is somehow the div with the text has nothing in it selected, so when user right clicks,
         they don't get a context menu.  If we move the selection div to front with z-index>2 (in codemirror source)
         and make opacity like .4, then it works fine, but of course looks silly with the font color
         changing.
       - not bootstrap
       - not a css problem.
       - not a $(document).click problem.
       - giving up for now.


- [ ] (1:30?) upgrade to sage-6.0

- [ ] (0:45?) maybe use these options like gcutil does, in some scripted cases:
        "-o UserKnownHostsFile=/dev/null -o CheckHostIP=no -o StrictHostKeyChecking=no"

- [ ] (2:00?) automate zpool scrub's
      - started one manually on compute1, 10, and 10.3.1.4 -- see how they do first:  good.
      - write code in storage to do the following:
          - should do "zpool status" on each machine in parallel
          - parse output to seen when last scrub was
          - if output indicates any errors, notify me
          - if it was one week ago or more, do another scrub

- [ ] (1:30?) upgrade sockjs, which has seen new development: https://github.com/sockjs/sockjs-client/commits/master

- [ ] (3:00?) bug: fuse is now broken for users. UGH. -- this is because of BIG uid's....
      Idea to fix this:
          - figure out what the uid cutoff is in both npm and fuse; it's probably the same and fairly universal.
          - figure out the uid's of all existing projects; how many exceed the cutoff?
          - fix the ones that exceed the cutoff, somehow.
          - for the ones that don't leave 'em.
          - store the account uid in the projects database; changing code of create_project_user to take uid as non-optional input.
          - when creating a new project, choose a random uid, check if it is in use, and if not grab it, then check again (to avoid race).

- [ ] (2:00?) storage: edge case replication loop -- in this case 10.1.12.4 gets deleted one time, then 10.1.14.4 overwrites it in stage 2, ad infinitum.
        project_id: 306cce9f-fa4a-481b-93ef-afd69a7fb5b4
        current location: 10.3.3.4
        usage: {"avail":"4.97G","used":"26.9M","usedsnap":"8.03M"}
        last_replication_error: {"error":{"src-10.1.14.4-dest-10.1.12.4":"problem -- destination has snapshots that the source doesn't have -- destroying the target (really
         safely renaming)"},"timestamp":"2014-01-20T15:23:13"}
        snapshots:
                10.1.5.4 (dc 0): 2014-01-13T19:32:08, 2014-01-13T19:32:41, 2014-01-13T19:31:43, 2014-01-13T19:30:34, ...
                10.1.1.4 (dc 0): 2014-01-13T19:32:08, 2014-01-13T19:32:41, 2014-01-13T19:31:43, 2014-01-13T19:30:34, ...
        (old)   10.1.12.4 (dc 1): undefined, undefined, undefined, undefined, ...
        (old)   10.1.14.4 (dc 1): 2014-01-13T19:32:41, 2014-01-13T19:31:43, 2014-01-13T19:30:34, 2014-01-13T19:25:19, ...
                10.3.4.4 (dc 2): 2014-01-13T19:32:08, 2014-01-13T19:32:41, 2014-01-13T19:31:43, 2014-01-13T19:30:34, ...
                10.3.3.4 (dc 2): 2014-01-13T19:32:08, 2014-01-13T19:32:41, 2014-01-13T19:31:43, 2014-01-13T19:30:34, ...

    In this case, fix by doing: s.destroy_project(project_id:'306cce9f-fa4a-481b-93ef-afd69a7fb5b4',host:'10.1.14.4',safe:true)

- [ ] (0:30?) fix new project dialog -- modal-body div looks all weird.

- [ ] (1:00?) storage: write something that restarts all open projects on a given host (similar to close)

- [ ] for offsite backup (once done): i'll just periodically recompress the file vm/images/persistent/backup1-projects.img onto some external usb drive and put it on a shelf.  It will have all projects and snapshots of the database, encrypted.

- [ ] (1:00?) (???) CodeMirror slow when edit window big... bug is back in 3.21.  profiling the code suggests it's all some call into codemirror to get a size or something. Hmm.  Now I'm not seeing it.  Weird.

- [ ] once backup is done, start making backups of recently modified projects on a regular basis.

- [ ] different types of projects:
        - compute project: a replicated Linux account
        - course project:
             - one compute project for each collaborator (inc. owner), shared with project owners
             - a shared compute project
             - it's own UI view that provides lots of information about the linked projects, configuration, etc.

    - [ ] send = target a (path in a) project *or* group; optional time cutoff
    - [ ] get  = from path in a project or group

- [ ] snapshot browsing is broken on ie due to their toLocaleTimeString being WEIRD -- it puts hidden unicode characters all over.
- [ ] (0:30?) hit return for submit on password reset dialog
- [ ] (0:30?) enter password twice in password reset dialog
- [ ] course workflow: group of projects: 1 private, 1 shared by all, 1 between each; push/pull of files
- [ ] need to lock the (web)socket  when sending message -- (did I implement this?): https://mail.google.com/mail/u/0/?shva=1#inbox/14379ddcde5abb13
- [ ] codemirror unindent bug
- [ ] triple check that all ssh'ing ignores host keys -- we have a vpn so don't need them.
- [ ] rotate out very long .sagemathcloud.log's
- [ ] on first run: first /home/salvus/salvus/salvus/data/local/sbin/tincd --kill   then normal tinc
- [ ] double number of GCE compute nodes
- [ ] run web nodes on GCE
- [ ] run database on GCE
- [ ] route53 dns
- [ ] add to monitor: available space on each project zpool
- [ ] add to monitor: root fs disk usage on compute machines (due to storage temp files)
- [ ] implement ui for selecting from locations to move project to
- [ ] implement a message "get locations", which returns all ip's where project can be opened, along with the latest snapshot time on each.
- [ ] change move dialog to first call that, then show one button for each choice.
- [ ] implement a message: "move project *to*" with target ip address
- [ ] implement a message "get load", which returns the load on a given list of machines.
- [ ] fix this printing issue -- https://mail.google.com/mail/u/0/?shva=1#inbox/14367e63a3fa1052
- [ ] add a `./build.py --build_zfs` option to make updating ZFS easier.
- [ ] see "# TODO: must fix this -- it could overwrite a user bash or ssh stuff.  BAD." in create_project_user.py
- [ ] check -- is there anything in help or faq about /scratch (?) -- it is out of date.
- [ ] some hosts, e.g. cloud1 are not on UTC. Should they be?
- [ ] command line "open ." doesn't do what I expect.
- [ ] location autodetect by hub doesn't always work
- [ ] make SMC in SMC work again
- [ ] project server restart maybe isn't working?
- [ ] remove the home images in servers from compute vm's and reboot each one
- [ ] copy all the computex-home.img's to one computer to delete later.
- [ ] snap: delete snap cassandra user in database credentials
- [ ] snap: delete it all
- [ ] bug: "%load foo.js" fails
- [ ] route 53 dns
- [ ] port forwarding: "ssh -L cloud1.math.washington.edu:4567:10.1.2.4:4567 ce2d267d00df42deab4464509a5f3e74@10.1.2.4"





------




# DONE


- [x] investigate codemirror feeling slow: it turns out the problem was messing with the class of the save button.  Surprising and weird... but true.

- [x] (1:00?) account creation using old Internet Explorer doesn't work.
      so, when opening the site, put a big red warning if they are using an old version of: IE, Firefox, Chrome, Opera, Safari

- [x] (2:00?) (2:05+) try switching to codemirror 4 beta: -- have branch on laptop
      - [x] sage worksheets don't work in chrome, etc.
      - [x] doesn't work at all with firefox  (but does work with Chrome and Safari)
      - [x] doesn't work at all with IE 11



- [x] make script for raising quota:
    s=require('storage'); s.init()
    s.quota(project_id:'b33ef4d2-fbf3-467a-a5ec-2ebc5033afec', size:'20G', cb:console.log)

  - [x] rebuild ZFS on base machine; kernel upgrade;
  - [x] (0:16?) anna unicode bug: https://mail.google.com/mail/u/0/?shva=1#starred/1437897c46edae24
  - [x] admin monitor:
         - make it report any single errors checking on DNS
         - make it report *and email me* if number of zfs processes exceeds 100 (?)
         - make it email if available disk space in zpool is less than 50GB
         - make it email if load exceeds some percentage.
  - [x] turn off core dump on gce nodes!  see http://thesystemadministrator.net/cpanel/how-to-disable-core-dumps-on-linux-servers
              echo '* soft core 0' >> /etc/security/limits.conf

 - [x] add function to storage:

        x={}; s.get_snapshots(project_id:'ce2d267d-00df-42de-ab44-64509a5f3e74',cb:(e,s) -> console.log(([a, s[a][0]] for a in require('misc').keys(s))))

 - [x] editor: go editor mode.

- [x] ZFS is deadlocked on 10.1.6.4...
      More longterm, I desperately need to figure out how to not deadlock ZFS.
      This could be done by maybe doing exactly one ZFS operation per project at a time.  (?)  I should at least read about all deadlock bugs.  Also setup a monitor so I know fast that this is happening.
 - [x] start using cloud13 again
- [x] (0:25) lock so projects can't be moved automatically
     - [x] make it so storage doesn't set location=undefined on close, instead only killing all procs; push out new code
     - [x] change move UI to bring up a "temp. disabled" message, and email me if files vanished

- [x] (0:30?) (0:12+) move MRC project back (in progress)


 - [x] (1:00?) (0:43) make a git repo for my class with basic content
 - [x] (0:15?) (0:43) warm-up: "The Sagemath Cloud" --> SageMathCloud
 - [x] (0:24) get projects to work locally on my laptop and move class to one.
 - [x] (1:00?) (0:54) investigate changing the uid schema gracefully for the zpool's
        this can be done but it is a tedious rsync process
        using replication doesn't work, since can't chmod without breaking the snapshot (despite same name).  DUH.
              465  zfs create projects/tmp
              466  rsync -axvH /projects/10f2b056-285b-48b2-b468-8df293b943a9/.zfs/snapshot/2014-01-07T19:39:12/ /projects/tmp/
              467  chown -R 10f2b056285b48b2b4688df293b943a9. /projects/tmp/
              468  zfs snapshot projects/tmp@2014-01-07T19:39:12
              469  rsync -axvH /projects/10f2b056-285b-48b2-b468-8df293b943a9/.zfs/snapshot/test/ /projects/tmp/
              470  chown -R 10f2b056285b48b2b4688df293b943a9. /projects/tmp/
              471  zfs snapshot projects/tmp@test
              472  rsync -axvH /projects/10f2b056-285b-48b2-b468-8df293b943a9/ /projects/tmp/
              473  chown -R 10f2b056285b48b2b4688df293b943a9. /projects/tmp/
  - [x] (0:45?) (0:41) implement chown for a zfs filesystem
  - [x] (1:00?) make location the hostname *and* optionally a ZFS filesystem name (default = "projects");
  Worry -- it will just cause trouble...
  Better to just do this dev stuff with multiple vm's or whatever.
  How would it work?
     - in database, add pool; the default is 'projects'.
     - {"host":"localhost","pool":"projects2","username":"cd9c78cfff8143dc8c35a1269e8489f4","port":22,"path":"."}
     - Adapt *all* the code in storage.coffee to take an optional pool argument.
     - change in database the "locations" mapping so that the key can be 'hostname:poolname', e.g.,
          {'localhost:projects2': '["2014-01-07T21:04:09","2014-01-07T21:02:37","2014-01-07T21:00:49",...}
     - I just don't like this.  It will make bad use of space.  And is bug prone.
 I tried to mirror out to OSX, but the zfs stream version is different, so it doesn't work.  Wasted 30 minutes on that.


 - [x] make a new base vm and restart compute vm's using it.
 - [x] update gce base vm and re-create all compute vm's using it.  Could be issues
 - [x] re-install /root ssh
 - [x] in parallel
    - [x] do two migrates
    - [x] add option to migrate so it migrates project *and* sets location to null!
    - [x] more migrates

'f56643b6-e96d-4a73-83b2-cea7bd5298eb': { 'src-10.1.2.4-dest-10.1.6.4': 'destroyed target project -- cannot receive incremental stream: most recent snapshot of projects/f56643b6-e96d-4a73-83b2-
cea7bd5298eb does not\nmatch incremental source\n' },
     'dac4431b-1ed2-4c15-ae41-8a7dde4a8622

DO IT

 - [x] migrate
 - [x] in parallel

     - [ ] do another migrate, then set all to null:

         r=require('storage'); r.init()
         y={};r.migrate_all(limit:25000,cb:(a,b)->x.a=a;y.b=b)
         r.migrate_unset_all_locs(cb:console.log)

     - [ ] reboot all web vm's:

         [cloud.restart('vm',hostname='web%s'%i) for i in range(1,8) + range(10,22)]

 - [x] start all hub servers and nginx servers
 - [x] send an email; encourage testing and feedback about issues.
 - [x] start using cloud13 again
 - [x] (1:00?) a database field to disable project timeouts
 - [x] (1:30?) hub: implement automatic timeout when inactive, removing existing code; honor database field to ignore timeouts
 - [x] (0:30?) snap: disable in startup scripts...
 - [x] (0:45?) look at, try, new print script


- [x] (0:45?) bug: file chat broken by switch; it's an "open before file create" issue; touching file makes it work. Has to do with code for dealing with read-only files that I added.



- [x] come up with ideas for Sage Days 56:

 - make some use of GCE for sage dev (?)
 - organize bug days
 - triage bugs
 - online homework system
 - ideas for better use of snapshots:
     - tags like git; could be used in url, e.g.,  <https://cloud.sagemath.com/projects/af95e67e-809d-49b1-a323-5c7e441b06e5/tags/foobar/>
        - the tag would just be mapped to existing snapshot names; just a simple map in projects table.
        - user wouldn't be allowed to delete snapshot without big fat warning.
     - could have user-wide tags, which point to a specific version of a file/directory in a project:
         <https://cloud.sagemath.com/users/user_id/tags/foobar>
       (or when we have usernames, `user_id --> username`).  If foobar not given, would list all tags for that user.


 - user guidebook
 - marketing campaign ideas
     - blog post ideas
 - business ideas
     - early signup price (like google play): basically only way to get permanent discount
 - brainstorm ideas that are very fast because of using ZFS, e.g., clone/forking/etc.
 - Sage infrastructure discussions.
 - optimize (with Andrew) the file listing script



BUGS:
 - [ ] "open .git" from terminal fails
 - [ ] open often shows wrong tab


DONE:


- [ ] (2:00?) setup working SMC dev environment on vm... unless I run into a show-stopper issue with missing deps
       - [x] update database schema (used "git diff <commit_id> db_schema.cql")
       - [x] create projects zpool locally:
                - [x] shutdown vm
                - [x] add a new 16GB disk image
                - [x] setup compressed and dedup'd as zpool "projects":
                       zpool create -f projects /dev/sdb
                       zfs set dedup=on projects
                       zfs set compression=lz4 projects
                - [x] create storage user with sudo zfs access
                - [x] new script in /usr/local/bin/ (create_project_user.py)

       - [x] get a new project to work there
               - need to add to storage_topology table in db.
                   update storage_topology set vnodes=256 where data_center='0' and host='localhost';
       - [x] "git pull" a copy of my dev code to a new project

- [x] January SMC development log.
---------------------------
I'm on a flight to Hawaii, and I'm not going to get to do any real
SMC development with testing... because I used ZFS on a file on OS X,
and it DIED killing all my vm's when I stupidly (?) hard rebooted my
machine.
  Or NOT.  I couldn't import since they are already imported! Duh.!
  - update my smc plans, ideas, etc. a LOT
  - plan out course grading functionality



 - [x] GCE and libc issues
 - [x] debug: closing stale projects...
    Trace
        at exports.defaults (/home/salvus/salvus/salvus/node_modules/misc.js:65:15)
        at Object.exports.close_stale_projects (/home/salvus/salvus/salvus/node_modules/storage.js:616:12)
        at close_stale_projects (/home/salvus/salvus/salvus/node_modules/hub3.js:7009:20)
        at wrapper [as _onTimeout] (timers.js:252:14)
        at Timer.listOnTimeout [as ontimeout] (timers.js:110:15)
    /home/salvus/salvus/salvus/node_modules/misc.js:66
    w "misc.defaults -- TypeError: function takes inputs as an object " + (error()                                                                       ^
    misc.defaults -- TypeError: function takes inputs as an object (obj1=undefined, obj2={"ttl":86400,"dry_run":true,"limit":20,"cb":"__!!!!!!this is a required property!!!!!!__"})
 - [x] snap: delete the images on machines with low disk
 - [x] printing: add a line "\usepackage[utf8]{inputenc}" (after \usepackage{amsmath} ) the correct characters appears in the PDF generated by the print button: https://mail.google.com/mail/u/0/?shva=1#search/sagews2pdf/143539cb372003f5


# Things that can go wrong:

   [ ]  modified but not snapshotted:

         s.send(project_id:'4a5f0542-5873-4eed-a85c-a18c706e8bcd',source:{"version":"2014-01-11T17:47:43","host":"10.3.2.4"}, dest:{"version":"2013-12-18T00:21:55","host":"10.1.15.4"},cb:console.log)

        "cannot receive incremental stream: destination projects/4a5f0542-5873-4eed-a85c-a18c706e8bcd has been modified since most recent snapshot"

   [ ] but wait:

       debug: replicate (3702601d-9fbc-4e4e-b7ab-c10a79e34d3b): stored last replication error in database: {"src-10.3.4.4-dest-10.3.1.4":"cannot receive incremental stream: destination projects/3702601d-9fbc-4e4e-b7ab-c10a79e34d3b has been modified\nsince most recent snapshot\n"}

       and yet the diff is empty:

       root@compute1dc2:/projects/3702601d-9fbc-4e4e-b7ab-c10a79e34d3b# zfs diff projects/3702601d-9fbc-4e4e-b7ab-c10a79e34d3b@2014-01-11T21:15:36 projects/3702601d-9fbc-4e4e-b7ab-c10a79e34d3b


   [ ] edited but not mounted

        cannot mount \'/projects/3702601d-9fbc-4e4e-b7ab-c10a79e34d3b\': directory is not empty\n

        This was caused by me manually moving to -DELETE but not changing mountpoint:

           root@compute16a:/home/salvus# mount |grep /projects/3702601d-9fbc-4e4e-b7ab-c10a79e34d3b
projects/3702601d-9fbc-4e4e-b7ab-c10a79e34d3b-DELETE on /projects/3702601d-9fbc-4e4e-b7ab-c10a79e34d3b type zfs (rw,noatime,xattr)

FACTS:
    - snapshot stream *can* get out of sync, and still keep working, i.e., all that is needed to replicate is that most recent snapshot is there.  If older ones get deleted or whatever, no problem.


--

- [x] delete all replication files in /home/storage that are >= 6 hours old:
         find /home/storage/.storage* -type f -mmin +360 -delete

- [x] fix keyboard repeat on my laptop: defaults write -g ApplePressAndHoldEnabled -bool false
      See https://discussions.apple.com/thread/3190706

- [x] make "destroy_project" safe.; make replicate first repair snapshot list in db.

- [x] when doing the recv, if we get this error: "cannot receive incremental stream"
      run a rollback and try one more time.  This is pretty safe, in that it only looses
      non-snapshotted data, which is pretty minimal.

- [x] what to do about missing snasphots on targets?  maybe zfs can add those in?  These were because I neglected to use "-I", etc.

- [x] add last replication error to status.

- [x] storage: make it so projects don't get (re-)opened on disabled-for-maintenance hosts

        alter table storage_topology add disabled boolean;
        update storage_topology set disabled=true where data_center='0' and host='10.1.1.4';
        update storage_topology set disabled=true where data_center='0' and host='10.1.2.4';

- [x] stop hubs on web1, web2, so they don't cause trouble.

- [x] move everything off of cloud1 and cloud2
     [x] disable dns --
         https://dns.godaddy.com/ZoneFile.aspx?zone=SAGEMATHCLOUD.COM&zoneType=0&sa=isc%3dbb1455jd90&refer=dcc&marketID=en-US
     [x] disable haproxy pointing out hubs on cloud1, cloud2 (edited hosts conf file and restart haproxy)

- [x] sometimes hubs just go "crazy" trying to replicate, which makes the hub itself get very slow and not respond.  High cpu.  This has to be a bug in storage.


- [ ] this seems silly:
salvus@compute4dc2:~$ ps ax |grep zfs |grep send |wc -l
617
salvus@compute4dc2:~$ ps ax |grep zfs |grep recv |wc -l
313
salvus@compute4dc2:~$ sudo su
root@compute4dc2:/home/salvus# cd /home/storage
root@compute4dc2:/home/storage# du -sch .
13G     .
13G     total
root@compute4dc2:/home/storage# ls .storage* |wc -l
11956

- [x] clean up the flier -- https://cloud.sagemath.com/projects/3e44a434-c2f0-43ff-87bf-affdb1efcdbe/files/flyer-meeting/flyer01.tex

- [x] y={};s.projects_needing_replication(cb:(e,n)->y.n=n); x={};s.replication_errors(cb:(e,n)->x.n=n)

- [x] close all projects on a given machine

- [x] killall often works when pkill doesn't (and vice versa) really need to do both.

- [x] automate replication repair

- [x] delete an account:

        cqlsh:salvus> select account_id from accounts where email_address='shanikaseales@gmail.com';
        cqlsh:salvus> update accounts set email_address=null,first_name=null,last_name=null where account_id=708c6228-df8f-47c5-9d28-31b015504199;
        cqlsh:salvus> delete from email_address_to_account_id where email_address='shanikaseales@gmail.com';

# SUPER top priority

 - [x] setup and start running an encrypted off-site backup of all projects in new format
     PLAN:
         - [x] upgrade
         - [x] build and install zfs
         - [x] use image files: start with a single sparse 1TB file (?)
              mkdir /zfs
              cd /zfs
              truncate -s 1000G 1.img
              zpool create -m /projects projects zfs/1.img
              zfs set dedup=on projects
              zfs set compression=lz4 projects
         - [x] compress and dedup as much as possible... as usual
         - [x] make it so my backup machine's key is trusted by all "storage" users (both on live machines and in base images)
         - [x] setup database access
         - [x] write function in storage that goes through and makes/updates backup of each project, etc.


- [x] add clawpack to standard Sage packages: "export LDFLAGS=-shared; pip install clawpack"

- [x] GCE: verify that have fixed storage ssh key on salvus base image

- [x] (0:14) update readme

- [x] (0:45?) (0:34) latex editor: don't render errors/warnings after a certain number of errors.  If there are thousands then browser hangs.

- [x] (0:30?) (0:16) latex editor: scrollbar for list of errors is not visible

- [x] (1:00?) (1:23) quick fix snapshot times in Firefox and Safari (since will be changing this); most time spent hacking on IE, which was wasted time.

- [x] (1:00?) upgrade to codemirror 3.21: https://mail.google.com/mail/u/0/?shva=1#inbox/1439c770436eb409

- [x] (2:00?) create a new proper backup VM; in the meantime, run the backup script to a projects/ zpool *somewhere*.  Easiest would be a qcow2 image on a full-disk encrypted lvm.  Obviously, for now could do full backup to a GCE image with no processor but lots of space and some new random 14-character password (?). Can also do a big encrypted sparse image bundle on OS X again...


- [x] (0:30?) (0:16+) increase ram and cores in backup1 temporarily.

- [x] (0:20?) automate nodetool repair's again

- [x] backup all cassandra to my backup1 zfs fs.

        sudo zfs create projects/cassandra
            #!/usr/bin/env python
            import os, time
            hosts = ['10.1.%s.2'%i for i in range(1,8) + range(10,22)]; print hosts
            def run(s):
                print s
                os.system(s)
            run("sudo zfs snapshot projects/cassandra@%s"%time.strftime('%Y-%m-%dT%H:%M:%S'))
            for host in hosts:
                run("mkdir -p nodes/%s; rsync -axH %s:/mnt/cassandra/ nodes/%s/ &"%(host, host, host))

- [x] (1:00?) (2:15) Do some longterm SMC planning and brainstorming...


- [x] put this in the crontabs of base machine (on GCE ones; crontab is messed up on compute machines?):
         0 5 * * * find /home/storage/.storage* -type f -mmin +360 -delete


- [x] (0:30?) (0:14) make cassandra1 and cassandra2 use zfs:
    salvus@cassandra1:~$ sudo zpool create -f cassandra /dev/vdb
    salvus@cassandra1:~$ sudo zfs set dedup=on cassandra
    salvus@cassandra1:~$ sudo zfs set compression=lz4 cassandra
    salvus@cassandra1:~$ sudo zfs set mountpoint=/mnt/cassandra/ cassandra
    salvus@cassandra1:~$ sudo chown salvus. /mnt/cassandra

- [x] (0:30?) (0:19) add cassandra1 and cassandra2 nodes to cluster: http://www.datastax.com/documentation/cassandra/1.2/webhelp/cassandra/operations/ops_add_node_to_cluster_t.html

- [x] (0:45?) search hub logs for "Unhandled 'error' event" and fix things.


- [x] (2:30?) (0:50) snapshot browser broken!!  https://cloud.sagemath.com/projects/54949eee-57da-4bd7-bb43-c2602b429f9a/files/.snapshot/2014-01-21/

- [x] (0:30?) re-enable cassandra firewall, but safely testing that we don't mess up access by hubs.  Hubs will show this when things go wrong: "received only 0 respon" and start dropping connections like crazy.


- [x] backups -- make faster (?):
          # uses half of available ram
          cat /proc/spl/kstat/zfs/arcstats |grep max
          # increase arc to 16GB - https://github.com/zfsonlinux/zfs/issues/1779
          echo 17179869184 > /sys/module/zfs/parameters/zfs_arc_max
          echo 3 > /proc/sys/vm/drop_caches

- [x] (2:00?) switch to ubuntu 12.04 on GCE -- spent 1 hour investigating and came to the conclusion this will be very, very hard.  NOT worth it.




- [x] (1:30?) double GCE compute capacity

- [x] (1:30?) storage: write code to add new nodes, and run it on the 4 new GCE nodes:
        - [x] (0:12) recreate vm's with fast speed/cpu.
        - [x] change the live table:
                 update storage_topology set vnodes=256 where data_center='2' and host='10.3.5.4';
                 update storage_topology set vnodes=256 where data_center='2' and host='10.3.6.4';
                 update storage_topology set vnodes=256 where data_center='2' and host='10.3.7.4';
                 update storage_topology set vnodes=256 where data_center='2' and host='10.3.8.4';
        - [x] run replicate all, which will ensure that copies exist on the new nodes.

- [x] project hashtag organization: which will be used for course workflow
     - [x] client code for viewing project list parses out all the hash tags
     - [x] (1:30?) button bar at the top with a toggle for each hashtag `[#math480][#star]....;`
            - if All selected shows everything
            - clicking on "All" unselects everything and makes all selected
            - clicking on any other button unselects all and selects that.
            - if anything but all selected, shows union of those
            - hashtags -- currently allows inclusions, which is stupid.

- [x] update schema for linked projects
         alter table projects add   linked_projects      set<uuid>;

- [x] implement very, very basic course project support:
    - [x] linked projects:
        - move project reset block to the right
        - box below collaborators called "Linked Projects"
        - works like collab search, but instead does a project search through *your* projects only.


- [x] add cellserver to vpn and cassandra

        CREATE KEYSPACE cellserver WITH replication = {
          'class': 'NetworkTopologyStrategy',
          'DC0': '3',
          'DC1': '3'
        };
        CREATE USER cellserver WITH PASSWORD '<random 16 characters>';
        GRANT ALL ON KEYSPACE cellserver TO cellserver;

- [x] (0:45?) (0:19) for each project with location a gce machine, set location to null.

- [x] storage: the close_all and close_stale functionality could cause trouble if it is decided that something should be closed/killed due to inactivity, but the actual close call isn't until later; we need to recheck whether really stale again right before doing the action.   This would be especially a major problem given that things keep getting re-killed due to not unsetting the location, and the large number of hubs doing the killing.

- [x] new version email:

Hi,

I've updated https://cloud.sagemath.com with the following changes.  Refresh your browser.

   - Editor speed: Reverted editor back to CodeMirror 3.20.   I use the editor in SMC all day, every day, and frankly Codemirror 3.21 and Codemirror 4.0 (beta) are really *miserable* to use, compared to 3.20, as some of you reported.   They just feel really slow, even with the other speedups I made.  Also, a student showed me that CM 4.0 doesn't open on iOS, which is very serious for ipad users.   So things now work again on iOS, and moreover the editor should be back to feeling very snappy.   (You can confirm you're using the older CodeMirror by typing "CodeMirror.version" in the javascript console of your browser.)

   - Serious MathJax bugfix: The xypic MathJax extension is no longer imported (by default). This is a package for drawing commutative diagrams, and it was causing MathJax to crash sometimes.  I noticed this when demoing an interact when teaching today (http://youtu.be/7Y4m1nHN_aA), which involved a lot of quick-changing MathJax rendering.  Sometimes it would just hang and say "Mathjax Rendering... 0%" and get stuck there (until the page is refreshed).  The console log showed a traceback involving that extension, and disabling the extension fixed the problem.   I don't know if anybody actually knew about the extension and was trying to use it to draw commutative diagrams.

   - Got rid of the work-in-progress "linked projects" setting; I thought of a much better design now for course management, etc. (stay tuned!)

   - I finished doubling of the compute and storage resources for SMC at Google Compute Engine.  This got bogged down earlier today causing some unfortunate problems; the root cause was that I had put the wrong ssh keys on the new VM's.


Please continue to report every little bug, issue, etc., that comes up!  It makes a huge difference.

There were 232 accounts on SMC created already *today* (which isn't over yet), which is the most new
accounts created in any single day since SMC launched in April.   Please keep getting classes, friends,
etc. to start using SageMathCloud.  We have plenty of compute resources; and if we don't, it's easy to spin
up more at Google (they've gifted us a lot for this year), and it is easy to add more RAM (etc)
to the ones I already have at UW.


- [x] folder rename is broken -- https://mail.google.com/mail/u/0/?shva=1#inbox/143d75c5b69aa3fe and https://github.com/sagemath/cloud/issues/84


- [x] the "loose stuff on reconnect" is caused by "scrollIntoView(pos)" not being wrapped in an exception; this in setValueNoJump.


- [x] disable console-kit-daemon
        sudo mv /usr/share/dbus-1/system-services/org.freedesktop.ConsoleKit.service  /usr/share/dbus-1/system-services/org.freedesktop.ConsoleKit.service.orig
        sudo killall console-kit-daemon
- [x] remove landscape-common
        apt-get remove landscape-common
- [x] fuse permissions: "chmod a+rw /dev/fuse"
- [x] wrap this traceback causing disconnects from hub in try/except

        debug: target: setup proxy; time=0.009999990463256836 seconds
        debug: websocket upgrade -- ws://10.1.20.4:49090
        debug: websocket upgrade: not using cache

        /home/salvus/salvus/salvus/node_modules/http-proxy/lib/http-proxy/passes/ws-incoming.js:113
                server.emit('error', err, req, res);
                                               ^
        ReferenceError: res is not defined
            at ClientRequest.<anonymous> (/home/salvus/salvus/salvus/node_modules/http-proxy/lib/http-proxy/passes/ws-incoming.js:113:40)
            at ClientRequest.EventEmitter.emit (events.js:95:17)
            at Socket.socketErrorListener (http.js:1547:9)
            at Socket.EventEmitter.emit (events.js:95:17)
            at net.js:441:14
            at process._tickCallback (node.js:415:13)
        debug: connected to database
- [x] css of file listing is really bad now...
- [x] (1:00?) investigate url issue -- https://mail.google.com/mail/u/0/?shva=1#search/cloud/143dc5e26e99f874


- [x] DNS/hardware: I removed cloud5.math.washington.edu 128.208.160.207 manually from DNS, since it is down.  https://dns.godaddy.com/ZoneFile.aspx?zone=SAGEMATHCLOUD.COM&zoneType=0&sa=isc%3dbb1455jd90&refer=dcc&marketID=en-US


- [x] bitcoin miner: c4675887-8d93-4abf-928c-068dff376316   on compute3a;  xptminer

         update accounts set email_address='ausie81+389@gmail.com' where account_id=c4675887-8d93-4abf-928c-068dff376316;
         update email_address_to_account_id set account_id=c4675887-8d93-4abf-928c-068dff376316  where email_address='ausie81+389@gmail.com';
         delete from email_address_to_account_id where email_address='ausie81@gmail.com';

         # PROBLEM -- they still have a cookie!  and it isn't trivial to delete it... since it is keyed on a hash of the password or something... eek.

- [x] create a table of banned users, where banned is defined by various criterion:
          - email address match; starting with ausie81@gmail.com
          - account_id
          update banned_email_addresses set dummy=true where email_address='ausie81@gmail.com'

