{"desc":"(2:00?) #today #gce\n\n- [x] increase gce nodes from 5 to 8\n- [ ] add new cassandra nodes:\n   - [x] dc5\n   - [x] dc5: (re-)start all hubs and haproxy in dc5:\n   \n            [a.restart('hub', host='smc%sdc5'%i, wait=False) for i in range(1,9)] \n            [a.restart('haproxy', host='smc%sdc5'%i, wait=False) for i in range(1,9)] \n   \n   - [x] dc6\n   - [x] dc6: (re-)start all hubs in dc6\n   \n        [a.restart('hub', host='smc%sdc6'%i, wait=False) for i in range(1,9)] \n        [a.restart('haproxy', host='smc%sdc6'%i, wait=False) for i in range(1,9)] \n        \n   - [x] dc7\n   - [x] dc7: (re-)start all hubs in dc7\n   \n        [a.restart('hub', host='smc%sdc7'%i, wait=False) for i in range(1,9)] \n        [a.restart('haproxy', host='smc%sdc7'%i, wait=False) for i in range(1,9)] \n   \n- [x] spin up nginx, haproxy, stunnel\n\n\n...\n\n- [ ] should do nodetool clean in each dc","position":-1,"last_edited":1424274866304,"task_id":"0932dd45-2c9a-4d41-aa84-e85725b9df8f","done":1424274865896}
{"desc":"(2:00?)  #gce\nswitch to gce load balancing","position":0,"last_edited":1424206365653,"task_id":"d63d12de-c90e-4c7c-a2cf-3c26870e1746"}
{"desc":"#invalid #gce\nrewrite hub/bup_server, etc., to be able to use Google Data Store (and Google Cloud Store for blobs) as an alternative to Cassandra.\n","position":1,"last_edited":1425503202592,"task_id":"8847b690-dd6f-4a5c-936b-d0b2f9d04fc0","done":1425503202176}
{"desc":"#today\n","position":2,"last_edited":1424143447313,"task_id":"b89d32f7-754a-4de3-b743-9275f0b94dcb","deleted":true}
{"desc":"make compute vm's at UW use much more RAM and all cores.","position":-0.5,"last_edited":1424993183695,"task_id":"62f0c1ee-d4ef-4c85-9913-96b3e226c5f6","done":1424993183279}
{"desc":"alternative: instead of using nginx, use this: https://cloud.google.com/storage/docs/website-configuration\n\nhttps://cloud.google.com/storage/docs/website-configuration#tip-dynamic","position":-0.75,"last_edited":1424155806413,"task_id":"ad05d59a-90ae-4984-af7f-1f36d1fd12c7"}
{"desc":"#1 #bug #editor #sagews\ncorruption.\n\nhttps://mail.google.com/mail/u/1/?pli=1&zx=v6crnjw8ww15#inbox/14b9852c4c20ed4b\n\n    I can see that in the history around revision 294.  The list of changes is stored in the history file, which I've copied and will look at closely.\n\n    What web browser, operating system, and are there any javascript console errors?\n\n    https://cloud.sagemath.com/projects/4a5f0542-5873-4eed-a85c-a18c706e8bcd/files/tmp/Balbusaur.sagews.sage-history\n\n\nSolution/idea:\n\n- replace sync by a *local* sync on the cell level / do same with syncdb, ipython, etc.  That would fix a ton of problems.  A sort of sudo op transform...?","position":-0.875,"last_edited":1424791516590,"task_id":"7bf29e20-ec21-430e-b6b4-a86e9fdb419d","done":1424791516187}
{"desc":"#1 #today\nincrease quota from 10MB to 32MB for public files.","position":-0.8125,"last_edited":1424274824854,"task_id":"97322cae-1191-45b6-8b2b-83673e26455a","due_date":1424234908506,"done":1424274824438}
{"desc":"#1 #today \ndeal with compute firewall issues once for all.","position":-0.78125,"last_edited":1424286780506,"task_id":"c14cf799-27fa-44fe-9888-1c2068e4c448","done":1424286780104}
{"desc":"(1:00?) #0 #today\nrewrite code to make it possible to properly rebuild that email_address index, then run it. \n\n\n","position":-2,"last_edited":1424285604487,"task_id":"df7d7a00-65a2-43a9-8ecf-8e58c8f9c20b","done":1424285604073}
{"desc":"(1:30) #0 #today\nlocal_hub call just hangs sometimes....\n\nI think I fixed this -- we shall see.\n\nNew data: this may be caused when a project **moves**.\n\n    2015-02-18T15:49:24.809Z - debug: hub <-- client (client=QAJYGDE8p7f3UfAeAADa): {\"event\":\"project_exec\",\"project_id\":\"80496c4c-c705-46f5-b04f-34e7c031b21d\",\"path\":\"\",\"command\":\"git-ls\",\"args\":[\"--t...\n    2015-02-18T15:49:24.809Z - debug: project(80496c4c-c705-46f5-b04f-34e7c031b21d): call\n    2015-02-18T15:49:24.809Z - debug: local_hub(80496c4c-c705-46f5-b04f-34e7c031b21d): \"call\"\n    2015-02-18T15:49:49.813Z - debug: hub <-- client (client=QAJYGDE8p7f3UfAeAADa): {\"event\":\"project_exec\",\"project_id\":\"80496c4c-c705-46f5-b04f-34e7c031b21d\",\"path\":\"\",\"command\":\"git-ls\",\"args\":[\"--t...\n    2015-02-18T15:49:49.814Z - debug: project(80496c4c-c705-46f5-b04f-34e7c031b21d): call\n    2015-02-18T15:49:49.815Z - debug: local_hub(80496c4c-c705-46f5-b04f-34e7c031b21d): \"call\"\n    2015-02-18T15:50:12.875Z - debug: hub <-- client (client=QAJYGDE8p7f3UfAeAADa): {\"event\":\"project_exec\",\"project_id\":\"80496c4c-c705-46f5-b04f-34e7c031b21d\",\"path\":\"\",\"command\":\"touch\",\"args\":[\".sag...\n    2015-02-18T15:50:12.876Z - debug: project(80496c4c-c705-46f5-b04f-34e7c031b21d): call\n    2015-02-18T15:50:12.876Z - debug: local_hub(80496c4c-c705-46f5-b04f-34e7c031b21d): \"call\"\n    2015-02-18T15:50:14.813Z - debug: hub <-- client (client=QAJYGDE8p7f3UfAeAADa): {\"event\":\"project_exec\",\"project_id\":\"80496c4c-c705-46f5-b04f-34e7c031b21d\",\"path\":\"\",\"command\":\"git-ls\",\"args\":[\"--t...\n    2015-02-18T15:50:14.814Z - debug: project(80496c4c-c705-46f5-b04f-34e7c031b21d): call\n    2015-02-18T15:50:14.814Z - debug: local_hub(80496c4c-c705-46f5-b04f-34e7c031b21d): \"call\"\n    2015-02-18T15:50:39.821Z - debug: hub <-- client (client=QAJYGDE8p7f3UfAeAADa): {\"event\":\"project_exec\",\"project_id\":\"80496c4c-c705-46f5-b04f-34e7c031b21d\",\"path\":\"\",\"command\":\"git-ls\",\"args\":[\"--t...\n    2015-02-18T15:50:39.822Z - debug: project(80496c4c-c705-46f5-b04f-34e7c031b21d): call\n    2015-02-18T15:50:39.823Z - debug: local_hub(80496c4c-c705-46f5-b04f-34e7c031b21d): \"call\"","position":-3,"last_edited":1424886185364,"task_id":"86b4e172-da35-4741-b3ae-8e53e68c689c","done":1424886184955}
{"desc":"(0:30?) #gce #low-usage\nnodetool clean in each dc","position":-0.9375,"last_edited":1424274860558,"task_id":"35ede044-dc1d-479c-b8d8-1b0afcafe2e9"}
{"desc":"(2:00?) #now #upgrade #today\nupgrade to sage-6.5 and sync out","position":-6.856689453125,"last_edited":1424753284301,"task_id":"7438c090-816a-490a-942e-ecd926e97350","done":1424753283887}
{"desc":"#today\nsetup new devel machines on sagemath, inc. infrastructure","position":-8,"last_edited":1424291393896,"task_id":"de7b6d2c-90b4-4bc2-a3aa-4613703d4259","done":1424291393489}
{"desc":"#today\nupdate my local virtualbox devel environment so I can code on the plane, etc. if I want.","position":-7,"last_edited":1424290639195,"task_id":"fbae0512-f889-4790-aa46-d5f582cf8cd0","done":1424290638777}
{"desc":"#now (1:00?) #vm #upgrade #install #today \n\n- [x] push it out\n- [x] as root in /root: `./update_salvus`\n- [x] `umask 022` then `sage -sh` then `pip install mrjob boto pattern seaborn`\n- [x] `apt-get update; apt-get dist-upgrade; apt-get autoremove`","position":-6.85675048828125,"last_edited":1424752389753,"task_id":"66ae9142-271b-4187-9c00-600ae964b28b","done":1424752389343}
{"desc":"(1:00?) #gce\ntest google datastore:\n\n- [ ] how the latency really is \n- [ ] compare with my cassandra latency\n- [ ] get a feel for using it.","position":-6,"last_edited":1424289453590,"task_id":"8cfcf70b-e99e-4fcc-8bee-4c088f5b9f37"}
{"desc":"(1:00?) #gce\ntest google cloud storage for sharing files. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n - see https://cloud.google.com/storage/docs/website-configuration)","position":-6.75,"last_edited":1424993168197,"task_id":"15abe7db-fea5-4882-b336-4fe2febba40c","done":1424993167778}
{"desc":"(1:00?) #gce\ntest google load balancer...\n","position":-4.75,"last_edited":1424289460612,"task_id":"42816ea3-fc4a-41be-a0b1-b18457d68172"}
{"desc":"(2:00?) #gce\nlearn/test switch to google DNS (instead of route 53)\n\nIt turns out that google dns is way behind route 53 in functionality, as keith said.\n\nSo instead I properly setup route 53, so now when a region fails another will automatically take over, etc.","position":-6.5,"last_edited":1425088297010,"task_id":"9a0c6905-655c-41fa-85cb-b76254bc55bb","done":1425088296598}
{"desc":"(4:00+) (1:30?) #check #0 #gce  #backup #today\n\nCome up with better approach that involves first rsyncing everything from the VM's and then (and only then) (possibly) running bup.  Right now bup uses too much RAM.\n\ncreate new automated back-up databases from cloud.\n\nPlan: \n\n0. [x] create zfs snapshot of filesystem across cluster\n\n   [x] delete all snapshots and backups directories:\n   \n     \t cd /mnt/cassandra/lib/data && time rm -rf */*/backups */*/snapshots\n      \n   [x] create new nodetool snapshot:\n   \n         nodetool snapshot\n\n1. [x] create backup entirely on a backup vm in same zone using cassandra snapshots/backups as usual\n    - [x] decide on a new PD size\n    - [x] create PD, format, mount\n    - [x] modify existing backup script\n\n2. [x] rsync the backup to bsd\n\n3. [x] rsync from bsd to other offline storage later...[ ] ","position":1.8125,"last_edited":1424459669181,"task_id":"34d3948a-9ee6-4a8b-97e1-a9be8b118ff1","done":1424459668769}
{"desc":"#gce\nbrand new gce machine template with sage-6.5 and minimal image size.","position":-2.25,"last_edited":1424707486063,"task_id":"681dafa5-b288-4d5a-9a96-ed10f6c59342"}
{"desc":"(0:30) (0:45?) #now #today\nAdd another thing to home screen to add collaborators to encourage viral growth: \n\n\" Create or Import a File, Worksheet, Terminal or Directory...\"","position":1.75,"last_edited":1424395945486,"task_id":"5e71fc53-f186-4e07-b565-75555b9bcb2b","done":1424395945070}
{"desc":"(2:00?) #gce #cassandra #today\nreplace the cassandra node filesystems -- all of them -- with much smaller ext4 filesystems","position":1.625,"last_edited":1424390763273,"task_id":"fee727a9-9b11-425c-beea-51a274435641","deleted":true}
{"desc":"(1:00?) #security\ninvestigate this security scanner:\n\n   http://googleonlinesecurity.blogspot.com/2015/02/using-google-cloud-platform-for.html","position":1.875,"last_edited":1424392129543,"task_id":"1e15e912-1a6b-45ed-b1aa-baece5388c18"}
{"desc":"#gce\nseriously think through options for separating compute from storage\n\nIdeas:\n\n- store all snapshots of project in google cloud storage as a bup repo\n- when project isn't used for a while, delete it from compute vm or move to slower storage\n- when start project\n\n\n","position":-6.625,"last_edited":1424398734540,"task_id":"8730470e-01c1-4a9a-930a-9f88fd5fe451"}
{"desc":"(1:00?) #upgrade\nupgrade to codemirror 5.0\n\nhttp://codemirror.net/codemirror-5.0.zip","position":-6.6875,"last_edited":1424468345411,"task_id":"8e8f3a41-75ec-4d02-91f4-8963afbc1055","done":1424468345411}
{"desc":"(1:03) #now (1:00?) #today\nget rid of password confirm and combine first/last name\nto reduce friction.  \npassword reset works fine.","position":-6.8125,"last_edited":1424472220316,"task_id":"a72958f2-e731-47e7-ae58-0571cbd8d331","done":1424472219915}
{"desc":"(0:15?) #ui\nshould we get rid of the files/projects/messages buttons in the log -- I don't think they are anything but confusing?","position":-6.84375,"last_edited":1424546252781,"task_id":"f2e40a7f-9e59-427f-b65a-ff928ea113ff"}
{"desc":"(1:00?) #today #now\ninvestigate longterm saving projects to google cloud storage via bup or https://github.com/moinakg/pcompress\n\n- Store bup repo of snapshots of project by simply updating, using rsync, and copying any files that might change.\n\n- Also use pcompress (?) to store complete copy of project as a single file, just in case (?).  Incremental based on timestamp (?).\n\n> HSY: except for the encryption (which is orthogonal anyways), this pcompress sounds similar to `lrzip`. It's an \"older\" standard linux utility and hence easily installed via apt-get install lrzip. `lrztar <directory>` and `lrzuntar` are all you need, see `man lrzip`. BTW: it compresses the sage binary tarball 7.6:1 in a few minutes (tried it for the last 6.5 release for ubuntu 14.4), which probably means it is mostly limited by I/O, not CPU.\n\n> William: I think bup is the best choice anyways.  I can't think of anything similarly efficient to solve this problem...\n\n- The current total space used by all bup repos is about 1.3TB, which would cost \\$34/month to store in Google cloud storage. \n\n- I think this isn't a viable approach since compute will often not be a GCE, but this must be at google... (?).\n\nOther ideas to test:\n\nIn each DC have a single global dedup'd compressed ZFS pool with rsync of all projects and also all bups.  It's fine if it is fairly slow. \n\n- Define the dream API I want, then list different ways of implementing it with different backends.\n- I can use that api to save/restore the bup repos to get fine-grained snapshots if I want.\n\nAPI: \n\n        storage save path     # efficiently saves changes to path since last save\n\n        storage restore path  # efficiently restores path from last save.\n\nSolutions:\n\n- rsync to/from a single big volume\n- rsync to/from distributed volumes with a cassandra index\n- rsync to/from gluster (or other cluster) filesystem\n- google cloud storage with incremental tarballs and repack every so often.\n- google cloud storage using bup repos and gsutil rsync\n- many individual persistent disks?!  (massively wasteful)\n- sparse image files stored in google cloud storage\n\nOR... don't change anything but just improve my current implementation to do solve additional problems:\n\n   - disk space running out: rebalance, increase volume size.\n   \nThe ultimate would be:\n\n   - a magic single multi-data-center global distributed filesystem that compute machines can mount, which is super fast.  Involves a local cache.\n   \nTest this:\n\n - glusterfs us in my sagemathcloud project on 3 nodes\n - another glusterfs in europe in my sagemathcloud project on 3 nodes\n - could then make the \"open project on compute\" process first check\n   for cached local files, and if not copy them from the glusterfs.\n   If so, just update them.  With state stored in cassandra (?) or filesystem.\n - then I could delete any projects from compute vm's without any loss in functionality (just speed), which since \n - could also move a project from any vm to another (even in same dc) without loss of files, just speed.\n - would just have to make the \"write to gluster\" operation write to all glusters via an rsync in parallel\n \n \nNone of the above solves any problems really.  Instead I should just improve the current system, which is pretty damn good -- just not finished.  I can decouple compute and storage enough this way. \n\n - add command to move all data for a project from one node to another in the same datacenter\n - make bup_repair properly get results in small batches rather than all at once.\n \n \nOr... the other option, which is:\n\n- In each DC have a single NFS server, and have all compute vm's mount from it over the LAN.\n- It will be a single zfs pool (so can snapshot) -- about 4TB for starters (could try to dedup compute data if possible... if that fails, rebuild without dedup.)\n\nNO.  That centralizes everything, which doesn't scale.  Even now it would barely work at UW.\n\n","position":-6.859375,"last_edited":1424661119535,"task_id":"e9b674f3-a438-473d-aa33-a90d5779005c","done":1424661119126}
{"desc":"(0:38) (0:10?) #today\nraise password reset email timeout to 1 hour.\n","position":-6.9296875,"last_edited":1424548524194,"task_id":"006a97da-3c28-433b-8ee8-ba5069a8a7cf","done":1424548523779}
{"desc":"(0:15?) #today\nchange collaboration link to go straight to \n","position":-6.84765625,"last_edited":1424460022382,"task_id":"3620595e-8d88-4446-9b65-09dcea44093e","deleted":true}
{"desc":"#terminal #feature\nterminal menu bar\n\nIdeas here: https://mail.google.com/mail/u/1/?pli=1&zx=v6crnjw8ww15#inbox/14b9843b3d96f348","position":-6.85546875,"last_edited":1424460770445,"task_id":"ddfadeda-e4bd-4ef1-81d4-abd473a739f4"}
{"desc":"#now (0:07) (0:30?) #upgrade #today\nupgrade to font-awesome 4.3.0","position":-6.85693359375,"last_edited":1424726001104,"task_id":"6ef80028-e10b-47b4-b5dc-07f887bf71e3","done":1424726000702}
{"desc":"#bug #ipython\ntoo many servers getting left around\n\nSee https://mail.google.com/mail/u/1/?pli=1#inbox\n","position":-6.89453125,"last_edited":1424726574297,"task_id":"e65ddfab-a356-4150-ada8-6e6a0d612797","done":1424726573888}
{"desc":"#now #today\n(1:30?) #gce\nsave money -- replace all cassandra disks by new 50GB ext4 format disks, and delete any log disks.\n\nThis must have taken 5 or 6 hours.  Very tedious!\n\n\n\n\t\n    sudo su\n    echo \"UUID=`ls -l /dev/disk/by-uuid/|grep -v sda|awk '{print $9}'|xargs` /mnt/cassandra   ext4 defaults 0 1 \" >>/etc/fstab\n    echo \"/home/salvus/salvus/salvus/scripts/restart_tincd; sysctl -w vm.max_map_count=131072\" > /etc/rc.local\n \tkillall java && sleep 10 && rsync -axvH /mnt/cassandra/ /mnt/x/ && sudo shutdown -h now\n \n on startup\n \n \tsudo chown salvus. -R /home/salvus/salvus/salvus/data/logs\n    sudo rmdir /mnt/x\n    \nFrom admin ipython:\n \n     h='smc5dc5'; [a.start(s, host=h, wait=False) for s in ['cassandra', 'stunnel', 'haproxy', 'hub', 'nginx']] ","position":-6.876953125,"last_edited":1424661104346,"task_id":"df18c746-be80-438e-9f3f-0bffaf943bc3","done":1424661080145}
{"desc":"#2\nconcerns -- what happens if copy files (using rsync) to a project (e.g. sending homework), but that project is not on?  \n\n- do files get properly saved ever?  \n- Will \"it\" just forgot and randomly loose files?\n\n> My memory is that this is already handled perfectly, but I want to double-check...","position":-6.857421875,"last_edited":1424791634214,"task_id":"0c2e72f9-18bd-46ac-8a60-506b838f45d7"}
{"desc":"(1:15?) #2\nfix monitor script so it can run on GCE rather than UW.\n- [ ] (easy) write to database via hostname (not using tinc network)\n- [ ] (harder) send email (can't use gmail; use sendgrid)","position":-6.8564453125,"last_edited":1424726754368,"task_id":"a196661c-4ded-4abe-baa9-ba293b2a6b88"}
{"desc":"","position":-6.85595703125,"last_edited":1424707490941,"task_id":"0c148baf-fc39-4a39-9491-da30065e7647","deleted":true}
{"desc":"(0:10?) #today\nexport a public version","position":-0.76953125,"last_edited":1424709582231,"task_id":"19468fef-d3ba-4e97-9c0b-630cefc824fe","done":1424709581826}
{"desc":"(0:05) (0:15?) #today #install #now\nsome pip install requests\n\nhttps://mail.google.com/mail/u/1/#inbox/14bb082788a79dfc","position":-6.8565673828125,"last_edited":1424726569197,"task_id":"5a819142-d614-4476-86b9-692b63c1aecf","done":1424726568791}
{"desc":"(2:00?) #feature\nimprove the project log\n\nsome ideas  -- https://mail.google.com/mail/u/1/#inbox/14bad8af5d04134d","position":-6.85650634765625,"last_edited":1424710174640,"task_id":"38037606-4819-4a1e-8586-8fe998e3506b"}
{"desc":"(1:00?) #bug #install #2\nfix fenics install\nhttps://mail.google.com/mail/u/0/#inbox/14bb737b3d010595","position":-6.8568115234375,"last_edited":1424753279200,"task_id":"7c42ca95-7450-492f-b2d0-7f45860ddcf8"}
{"desc":"(2:16) (0:40?) #0 #upgrade #today\nmajor new cassandra driver: https://mail.google.com/mail/u/0/#inbox/14bb78b75048c585\n\n- problem: weird stuff with uuid's/buffers and data types breaking the driver...","position":-6.85723876953125,"last_edited":1424843719149,"task_id":"a446fb16-b230-4734-8489-709186b5abe1","done":1424843718743}
{"desc":"(1:00?) #2\nmake this url display terms of usage, read from a static file (?)\n\n  https://cloud.sagemath.com/terms","position":-6.856903076171875,"last_edited":1424716982865,"task_id":"7a7906f6-d8c4-4408-a3cd-725acf85429c"}
{"desc":"(1:00?) #admin\ntry to get google cloud monitoring/logging to work","position":1.9375,"last_edited":1425503209396,"task_id":"f77ced38-65c2-4f8b-81ac-4f89179b990d","done":1425503208977}
{"desc":"#today #now (0:05) (0:10?) update usage numbers","position":-0.7578125,"last_edited":1424726274687,"task_id":"8b3807a3-0691-4679-86fa-8f096ed43d57","done":1424726274271}
{"desc":"(1:30?) #vm #upgrade #install\n\n- [ ] `cd ~/salvus/salvus; npm install --upgrade cassandra-driver`.  VERY IMPORTANT to do this on the smc template for Sage-math-inc project.\n\n- [ ] `pip install ggplot`\n- [ ] as root in /root: `./update_salvus`\n- [ ] `apt-get update; apt-get dist-upgrade; apt-get autoremove`\n        \n- [ ] patch sage: `cd /usr/local/sage/current && git checkout src/bin/sage-env && git pull https://github.com/williamstein/sage-smc.git sagemathcloud && git log && ./sage -br`\n\n- [ ] as salvus user after `umask 022`; \n\n\t\techo \"import sys; sys.path.extend(['/usr/lib/python2.7/dist-packages/', '/usr/lib/pymodules/python2.7', '/usr/lib/x86_64-linux-gnu/root5.34/'])\"$'\\n' >  /usr/local/sage/current/local/lib/python/sitecustomize.py\n        \n","position":-0.76171875,"last_edited":1424885802440,"task_id":"d236f82b-5f81-4495-8d37-ae5cd2a90258"}
{"desc":"(3:30) (1:00?) #0 #today\nmake it so that changing the password invalidates all remember me cookies for a given user.\n\n- [x] (0:34) make a plan\n\nWe have a big (about 70K entries) table right now that has all the cookies along with the account_id as a value.\n\nIf we come up with a new approach that uses the same cookies, we can iterate through that huge table of all cookies and read it, then write to a new table.\n\nHow about this:\n\n- [x] (0:27) (0:20?) Whenever we *create* a remember_me cookie, we also write the key of that cookie to a set attached to the account.  `alter table accounts add remember_me          set<varchar>;`\n- [x] (0:23) (0:20?) Whenever we change the password, we delete all cookies with keys in that accounts table, thus invalidating them.\n- [x] (1:13) (0:20?) Every 5 minutes (?) the hub will check that the cookie used to login a connected user is still valid.  If it disappears from the table, the user is logged out.  This means implementing a message from hub to client that logs a user out.\n- [x] (0:58) (0:45?) #now Once above is working, we run through the entire 70K big table of existing cookies and write them to the accounts table, so that the above works.\n\n","position":-0.7587890625,"last_edited":1424915734457,"task_id":"0f753947-13da-46be-a63c-c491d3592941","done":1424915734051}
{"desc":"(0:55) #now (1:00?) #terminal #bug #today\noutput doubling race condition\n\nhttps://mail.google.com/mail/u/1/#inbox/14bbbc497857c33c\n\nDid some things -- not sure if this will fix it or not.  If I ever hit it again, i need to know if it is entirely client-side by testing another console connected, looking at server logs, etc...","position":-6.8572540283203125,"last_edited":1424823566664,"task_id":"4ea24d63-885b-4e55-9d31-c7f9331ebac2","done":1424823566257}
{"desc":"(2:22) #now (0:30?) #database #admin #2 #today\nrewrite bup_repair to stream output rather than do one big query\n\nThis took much longer than expected since I did it *right* with integrated streaming support, which is used also for the user search functionality (which was about to die). ","position":-6.8572998046875,"last_edited":1424900938476,"task_id":"49b0ce1a-bd86-4541-8060-7a8a229c8515","done":1424900938074}
{"desc":"(1:30?) #com\nmake it so users can move their project to an available location of their choice.","position":-6.853515625,"last_edited":1424843879180,"task_id":"52fe0cf8-0a34-4c63-9141-9ec83cb54fe3"}
{"desc":"(0:28) (1:00?) #now #today #bug #0\nfix bug in load of remote files\n\nreport: https://mail.google.com/mail/u/0/#inbox/14bbd4b67dc9e275\n\nto sage-devel: https://mail.google.com/mail/u/0/#sent/14bbd62bdc92cec1?compose=new","position":-6.857269287109375,"last_edited":1424813067674,"task_id":"8dfedcd1-a7b6-4345-b79c-1564fb43a839","done":1424813067271}
{"desc":"(0:15?) #monitor\nmake check_hub_* also check for \n\n\tgrep \" Uncaught exception:\" hub*.log\n","position":-0.759765625,"last_edited":1424843772264,"task_id":"9ffbf978-c690-4601-a00d-51720d3492d1"}
{"desc":"(0:26+) (3:00?) #bug try/test -- rewrite how #ipython sync works to use the syncdb infrastructure (which itself could get improved later).  \n\nMotivation: https://mail.google.com/mail/u/1/?pli=1&zx=v6crnjw8ww15#inbox/14bc33cc2777f52e\n\nCurrently my \"sync friendly\" version of the ipynb file (the syncdb file) is basically NOT sync friendly, especially as the ipython notebook cell format has got a lot more complicated.  Ideas?\n\nRight now it looks like sync's that result in cell contents that are invalid json just result in an error.  But the client that caused that error doesn't know to retry. Think this through.  Possibly consider using a json patching to upstream... then use usual mechanism, i.e., some notion of purely local operational transform, repeated until it succeeds.  Could also apply to syncdb.","position":-0.75830078125,"last_edited":1424992396280,"task_id":"bc41b5a4-3208-4a35-86a2-97047e0a3d44"}
{"desc":"#database #backup\nadditional plane text backups using database streaming.\n\nuse the streaming functionality of cassandra to make periodic \"proper\" CSV/text backups of the non-binary content of the database; can be nice to query from grep, etc.","position":-0.758544921875,"last_edited":1424914167484,"task_id":"6510c04c-bc2d-4797-a069-3135587d7828"}
{"desc":"#bug\nwhy does this project have 5 locations?\n\n      { project_id: '9a1eb6c1-9b97-48d8-aa42-306cb460439c',\n        bup_location: '3056288c-a78d-4f64-af21-633214e845ad',\n        bup_last_save:\n         { '3056288c-a78d-4f64-af21-633214e845ad': Wed Feb 25 2015 22:49:48 GMT+0000 (UTC),\n           '630910c8-d0ef-421f-894e-6f58a954f215': Wed Feb 25 2015 21:05:14 GMT+0000 (UTC),\n           '94d4ebc1-d5fc-4790-affe-ab4738ca0384': Wed Feb 25 2015 22:49:48 GMT+0000 (UTC),\n           'a7cc2a28-5e70-44d9-bbc7-1c5afea1fc9e': Wed Feb 25 2015 22:49:48 GMT+0000 (UTC),\n           'eec826ad-f395-4a1d-bfb1-20f5a19d4bb0': Wed Feb 25 2015 22:49:48 GMT+0000 (UTC) },\n        source_id: '3056288c-a78d-4f64-af21-633214e845ad',\n        timestamp: Wed Feb 25 2015 22:49:48 GMT+0000 (UTC),\n        targets:\n         [ '3056288c-a78d-4f64-af21-633214e845ad',\n           '630910c8-d0ef-421f-894e-6f58a954f215',\n           '94d4ebc1-d5fc-4790-affe-ab4738ca0384',\n           'a7cc2a28-5e70-44d9-bbc7-1c5afea1fc9e',\n           'eec826ad-f395-4a1d-bfb1-20f5a19d4bb0' ] } ]","position":-0.7586669921875,"last_edited":1424914594518,"task_id":"a2f80f7d-1969-4229-987d-9d1a96d428db"}
{"desc":"(1:00) #latex #bug #today\npages get randomly re-ordered, etc. when editing latex documents.  Instead, clean this up:\n\n- create one preview html div on load for each document page, as soon as we know number of pages\n- when changing state of a page (loading new version), show a spinner or something\n- always get the actual pages right","position":-0.758056640625,"last_edited":1424932758845,"task_id":"8e1ba602-a6c4-488b-827f-355f4aecb275","done":1424932758428}
{"desc":"#ipython #bug\n","position":-0.7579345703125,"last_edited":1424929208276,"task_id":"b59b023d-9eee-41d1-bf43-c199bc870f9b","deleted":true}
{"desc":"#bug #2\ncode folding in latex documents now broken in new codemirror.\n\ne.g., in this file: https://cloud.sagemath.com/projects/50c2cd00-d0b7-4b9d-858a-657b587c0767/files/revision-critical-paper/chen.tex","position":-7.5,"last_edited":1424992412631,"task_id":"d6a1d123-60d0-45c0-b9cd-55edbd17bf69"}
{"desc":"(0:15?) #com\nmachine type in database\n\n- add new database column to storage_servers indicating whether the machine is free or premium (or dedicated... needs to be flexible)","position":-6.856874465942383,"last_edited":1425017818161,"task_id":"065db1d1-a137-4c1d-864b-9d1803311369"}
{"desc":"(0:30?) #com\n- [ ] start one testing premium machine running at us-central1f on academic GCE project.\n- [ ] add to storage_servers table.","position":-6.856868743896484,"last_edited":1425017798494,"task_id":"be9e7714-14c7-40af-8f38-8a05ae3209d6"}
{"desc":"#com \nadd function to bup_server to make a project premium, e.g., `premium_replication` which can either enable or disable replication of the project to premium machines in dc's. \n\nWill need something in database that indicates *what* is happening to the project, so multiple hubs don't compete to break it in half and make it vanish from existence.\n\nenable:\n\n- [ ] ensures it is replicated to one premium machine in each dc, if there are any\n- [ ] removes from all non-premium machines\n\ndisable:\n\n- [ ] make replicated to choice of non-premium machine in each dc that has one\n- [ ] removes from premium machines\n\nWhen user tries to do anything with a project that is getting upgraded/downgraded, show them an appropriate status message.","position":-6.856822967529297,"last_edited":1425017814547,"task_id":"16faee40-0392-4396-bab5-12d83074095f"}
{"desc":"#com #soon\nbup_storage.py defaults\n\n(temporarily at very beginning, could just hard code something.)\n\n- [ ] add different default parameters to bup_storage.py for when a project starts on a premium machine versus a free machine.  This could just be a command line option that is passed to bup_storage.py from bup_server.  And it could be an option shen starting bup_server (?).  Or it could an option that is passed to bup_server with the start message.\n\n- [ ] alternatively, completely remove defaults from bup_storage.py.  They should instead be passed as options (from hub, got from database). ","position":-6.856812238693237,"last_edited":1424996412166,"task_id":"2f0680db-f7d2-429e-8279-c6bcad3d574e"}
{"desc":"#com #database\n\n- [ ] add column in database to projects that is called 'plan'.  this has\n    - account_id of the user that is paying for this plan\n    - the current plan name itself, e.g., 'premium'\n    - log: append-only list of objects '{timestamp:..., action:}' or maybe just a map from timestamps to actions, where the actions are 'start', 'stop'.\n    - include plan name in project information, so client can display it somehow, at top in project listings with better icon, etc.\n\n- [ ] add a column in database to accounts that is called pay_projects.  This is a set of project_id's of projects that the user is paying for.\n\n- [ ] add a column in database to accounts that has payment information.  This would be a token that identifiers the user's account at STRIPE.\n\n- [ ] new table in database called \"customer_activity\" that has columns:\n\n\t- timestamp_uuid as primary key, so can easily query by date range (is this even possible -- if not use a uuid and a date string??)\n    - account_id\n    - some sort of description of activity\n\n- [ ] Add functions to cassandra:\n\t- set_plan(plan, account_id)   where plan is one of 'free', 'premium'.\n      which will put something in all of the relevant tables above.\n    - is_paying_customer(account_id)  -- whether or not user is a paying customer (so cc number is known)","position":-6.856880187988281,"last_edited":1425017810427,"task_id":"c879fae5-e210-4f07-8968-d5eb2230de83"}
{"desc":"#com #soon\nif a user is paying for the project, don't allow them to be deleted from the project.","position":-6.856812953948975,"last_edited":1424996407663,"task_id":"bcec95e7-61a4-49fc-a058-bfa3c79bba56"}
{"desc":"single sign on/oauth2, etc.\n\nhttps://mail.google.com/mail/u/1/?pli=1&zx=v6crnjw8ww15#inbox/14bc1d38c071e9ff","position":-6.856813669204712,"last_edited":1424995958624,"task_id":"ec51f454-c972-439c-ad71-2a5288d1396c"}
{"desc":"(2:00?) #com\n\n- [ ] update base template vm for machines\n- [ ] add one high-mem premium machine to each of dc5 and dc6 with 200GB zpool for each (?). ","position":-6.856845855712891,"last_edited":1424996631016,"task_id":"c6bac0c9-64d7-41fa-a179-fbcbd7f5feee"}
{"desc":"#billing #stripe #today\n","position":-6.856900215148926,"last_edited":1425434827912,"task_id":"36a56a48-630d-4acc-9244-67d809dee8d4","deleted":true}
{"desc":"#gce\nprobably just disable the asia dc -- there is just so little usage. Not sure.","position":-6.85688591003418,"last_edited":1425249914905,"task_id":"f80a96f8-4252-448e-adb3-c1920fcf2fb3"}
{"desc":"#billing #stripe #today\n\n\t\talter table accounts add stripe_customer_id   varchar;\n        alter table projects add stripe_subscriptions set<varchar>;\n\nimplement each of the api functions I defined in message/client\n\n- [x] (2:00) (0:30?) stripe_create_card\n- [x] (0:29) (0:30?) stripe_delete_card\n- [x] (0:11) (0:30?) stripe_update_card\n- [x] (0:45?) stripe_get_customer\n- [x] (0:15) (0:45?) stripe_get_plans\n- [x] (1:15) (0:45?) stripe_create_subscription\n- [x] (0:25) (0:30?) stripe_cancel_subscription\n- [x] (0:45) (0:30?) stripe_update_subscription\n- [ ] (0:45?) stripe_get_charges\n\n- [x] (0:17) (1:00?) create new stripe.[coffee/html/css] files and Stripe object, created at right time, etc., with all ui stuff invisible until creation, so can release safely.\n- [ ] (1:00?) list subscriptions\n- [x] (0:45) (0:30?) add new subscription\n- [ ] (1:30?) edit subscription (includes picking a project that it applies to, and which billing method to use).\n- [ ] (0:30?) remove subscription\n- [ ] (1:00?) listing billing methods\n- [ ] (0:45?) add new billing method\n- [ ] (1:30?) edit billing method (switch to using https://github.com/stripe/jquery.payment for input formatting/validation)\n- [ ] (0:30?) remove billing method\n- [ ] (0:45?) display payment history\n- [ ] (0:45?) admin setting so admins can change the stripe api keys","position":-6.8569016456604,"last_edited":1425508135298,"task_id":"7e645381-0a3b-47be-9fc2-273d30916907"}
{"desc":"","position":-6.856888771057129,"last_edited":1425251397117,"task_id":"6877a20e-3766-42f6-b028-0413ff1ecc94","deleted":true}
{"desc":"#bug\n\nThe new +New button ignores the directory when uploading files, etc. !?","position":-6.856894493103027,"last_edited":1425398820445,"task_id":"3ad16685-8063-48ee-befd-76aa7874caff"}
{"desc":"#2 #bug\nlook carefully into whether images get their ttl properly extended on worksheet save for R images.\n\nExample with problems:\n\n   https://cloud.sagemath.com/projects/868087e6-5719-485a-b908-6217b437742c/files/Mathematical%20Statistics.sagews\n   \nRequestor: https://mail.google.com/mail/u/2/#inbox/149dd9a5ff8e5aeb   ","position":-6.856900930404663,"last_edited":1425445869000,"task_id":"ec91c0e1-ecb8-4676-b929-5afafe3cad0d"}
{"desc":"#3\nimplement copy-from-history button\n\n- pop up a dialog that requests filename, defaulting to current filename.\n- have a warning that a file will be overwritten (if it will be)\n- motivated by https://mail.google.com/mail/u/0/#inbox/14be518bc6d7a2ae","position":-6.856901288032532,"last_edited":1425482639115,"task_id":"b540570a-6b76-4b32-92e7-6df8ef9ddb51"}
{"desc":"(2:30?) #1 #com\nimplement account deletion\n\n- needed for commercialization\n- will have to decide where to \"put\" the corresponding projects -- maybe a special DELETED project (?).\n- otherwise probably straightforward.","position":-6.856901466846466,"last_edited":1425500835125,"task_id":"086eecf6-2c66-46c0-ab64-25a6db52be40"}
{"desc":"implement realtime messaging\n\n- Article about google pub/sub: https://news.ycombinator.com/item?id=9145908","position":-7.25,"last_edited":1425503235787,"task_id":"31295a17-1dd7-4396-8073-6f832ac25df0"}