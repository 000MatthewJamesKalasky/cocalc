# VM image update
(see laptop todo list in main dev project)
 - [ ] update salvus repo
 - [ ] `unset MAKE; npm install net-ping`
 - [ ] Do this:
         sudo su
         apt-get install libcap2-bin; setcap cap_net_raw,cap_net_admin=eip /home/salvus/salvus/salvus/data/local/bin/node
 - [ ] this:
         apt-get install biber

 - [ ] fix ext:

         zfs create pool/ext
         zfs set quota=1G pool/ext
         chmod a+rw /pool/ext
         mkdir -p /pool/ext/sage-6.2.rc2/ext/
         rsync -axvH /usr/local/sage/current/local/share/sage/ext/ /pool/ext/sage-6.2.rc2/ext/
         rm -rf /usr/local/sage/current/local/share/sage/ext/
         ln -s /pool/ext/sage-6.2.rc2/ext /usr/local/sage/current/local/share/sage/ext
         zfs destroy -r pool/octave
         rm /usr/local/sage/current/local/share/sage/ext/octave
         mkdir /usr/local/sage/current/local/share/sage/ext/octave
         chmod a+rwx -R /pool/ext/


# todo

- [ ] build: add fix ext permissions stuff above to build.

- [ ] tinc if foo-bared.  I can't get vm's on cloud3 to connect to cloud3 even from cloud3. Currently using cloud2 as tinc server for vm's on cloud3.
Also the cloud3 tinc is only connecting to a few other servers.  UGH.

- [ ] first task list attempt...

- [ ] the .trash directory is appearing all over the place.

    alter table projects add  task_list_id uuid;
    /* also add all the task schema */

- [x] biber: https://mail.google.com/mail/u/0/?shva=1#inbox/145ebb10a70a83ab

We are going to need to use a message queue system to do todo lists with sync, since they work at a level different than projects (so we can't use projects).
This looks intriguing: http://nanomsg.org/index.html
For now, we'll just do the sync whenever the user clicks on the tasks tab and have a refresh button.   Stupid but will be fine for now.

- [ ] add back max height in worksheet



---

- [ ] restart cassandra nodes at GCE with more RAM, now that they are going to be used soon.

- [ ] wlimit the rsync even more to 1000 as a temporary measure to reduce impact on system performance.

- [ ] move replication rsync'ing so it doesn't use the tinc vpn at all -- I can add additional columns in the database giving ip addresses that should be used in various contexts for routing.  Complicated but do-able.

- [ ] add task list data field, for more flexibility
- [ ] front end global task list for each project
- [ ] account task list for each user

HMM.  I'm feeling very, very *uncomfortable* about using the database for the task list.

It means:

   - no easy backups
   - no snapshots
   - no rollbacks
   - no revision control
   - no scripting
   - have to reimplement everything related to auth, which is dangerous
   - can't use other tools to edit.
   - no realtime view of other people editing -- which is a BASIC ENGINEERING REQUIREMENT for smc

It's just bad from a software engineering point of view.
And it's not clear there is a big plus.  I could have account-wide stuff somehow via projects...

I'm happy with the UI part though.  That's where most of the work is going to go, as it turns out.
The DB/backend stuff is pretty straightforward.

Alternative: do like the log and chat... sort of.  Could be:

  - JSON task list item per line and use differential sync, but with atomic edits.
  - The *implementation* would be 100% client-side.


OK, I'm going to try.



IDEAS:

  - take a codemirror syncdoc but make it so it's all widgets.  will suck.

  - use a sync string, make event driven, but also use basically the same code I just wrote and continue with it.  So:
     - have a syncdoc string that is one line per task, where that line is json.
     - first line is meta-info about the whole task list
     - what about nesting though which is easy with db and hard this way.
     - what about homework assignments

  - make the file the database... or add some new event-based pushing database functionality to the local hub and build task lists on that?
    file = database = revision control/snapshot/etc. friendly.
    So it's one line per "database record".  BUT the structure/view of the document is determined by the
    schema I designed.
    I could make a special wrapper around synchronized string that provides a database-like API with events on it.
    but it is still a file, so properly snapshotted, versioned, etc.
    Have queries (all in memory in the browser).

    SyncDB


- [ ] make hub's point at their local db *ONLY*

- [ ] consider deleting all the `save_log.json` files, which since I turned off, are worthless.

- [ ] idea: consider writing a node.js-based cassandra FUSE implementation.  This would take a few days, but be much better async wise (and use binary cql) than the python fuse cassandra driver.   I would use this to implement a global eventually consistent shared filesystem (with quotas, etc.), which is clearly needed.

- [ ] update bup -- https://mail.google.com/mail/u/0/?shva=1#inbox/145be2291f56302b

- [ ] spell checking: https://mail.google.com/mail/u/0/?shva=1#inbox/145e1770e9db6d41

- [ ] make "vm.py" also properly try to shutdown vm's (hence export zpool's) for a minute before destroying them, like `vm_gce.py` does.

- [ ] "apt-get install nethogs" everwhere.

- [ ] mod_pagespeed module? "the js files is really long over here ... and even if you do not activate most of its features, it would still enable this spdy http protocol extension to mix up multiple data transfers in a single tcp connection"

- [ ] get rid of sockjs: support only websockets
         https://github.com/topcloud/socketcluster

- [ ] figure out how to make the browser connect to hub in the right dc directly, with each project.

- [ ] bup better than my stupid cache is... https://mail.google.com/mail/u/0/?shva=1#inbox/145be2291f56302b

- [ ] admin: make the monitor connect to all bup servers and verify that they are accepting connections; e.g., under duress they port where they are serving may change.;   OR, at least check that `bup_server` is running on all storage machines.

- [ ] add a way to specify static ip address (created if not exist) to vm_gce.py and admin.py

- [ ] make a clone vm and test out what upgrading to cassandra2 requires.

- [ ] control+v to paste issue: https://mail.google.com/mail/u/0/?shva=1#inbox/145bebfd87489cf8

- [ ] hosts file... DNS server

- [ ] temporary band-aide for replication in face failure: write something that, for each project touched in the last week (say), does an rsync out from it's master location to the two slaves.

- [ ] fix the add collaborator search to not display results randomly

- [ ] upgrade to codemirror 4.1: https://mail.google.com/mail/u/0/?shva=1#inbox/145896f4d974137d

- [ ] publishing with constraints

- [ ] change proxy server to use master and properly setup proxy server: https://github.com/nodejitsu/node-http-proxy

- [ ] bup storage: the `save_log` is possibly a BAD, BAD idea. Look:
  root@compute12dc0:/bup/bups/3702601d-9fbc-4e4e-b7ab-c10a79e34d3b# ls -lht conf
  total 383M
  -rw------- 1 root root 382M Apr 26 19:03 save_log.json

- [ ] increasing quota -- I should make an admin interface for this...

        x={};require('bup_server').global_client(cb:(e,c)->x.c=c)
        p=x.c.get_project('4255de6e-adc9-4a1e-ad9c-78493da07e64')
        p.set_settings(cb:console.log, cores:12, cpu_shares:4*256, memory:12, mintime:24*60*60)   # mintime is in units of seconds.

        x={};require('bup_server').global_client(cb:(e,c)->x.c=c)
        p=x.c.get_project('3bdfd30d-7c9d-424e-9902-cf13ce925821')
        p.set_settings(cb:console.log, cores:2, cpu_shares:256, memory:16, mintime:9999999999999999)   # mintime is in units of seconds.

- [ ] project folder connections (?)

       zfs set sharenfs=on bup/projects
       sudo zfs set sharenfs='rw=@10.1.1.0/16',no_subtree_check,async,no_root_squash bup/projects
       apt-get install  nfs-kernel-server

   Seems very flaky, and only mildly faster or maybe even *SLOWER* than sshfs, at least over our network.

   This seems very nice... and works fantastically!

      sshfs -o cache_timeout=10 -o kernel_cache -o auto_cache -o uid=1959631043 -o gid=1959631043 -o allow_other -o default_permissions 10.1.1.5:/projects/test/sage compute1


      cd /projects/3702601d-9fbc-4e4e-b7ab-c10a79e34d3b; mkdir -p projects/edf7b34d-8ef9-49ad-b83f-8fa4cde53380; sshfs -o cache_timeout=10 -o kernel_cache -o auto_cache -o uid=1959631043 -o gid=1959631043 -o allow_other -o default_permissions 10.1.3.5:/projects/edf7b34d-8ef9-49ad-b83f-8fa4cde53380 projects/edf7b34d-8ef9-49ad-b83f-8fa4cde53380

      fusermount -u edf7b34d-8ef9-49ad-b83f-8fa4cde53380

    # mounting student projects

    coffee> x={};require('bup_server').global_client(cb:(e,c)->x.c=c)
    coffee> p=x.c.get_project('cc96c0e6-8daf-467d-b8d2-354f9c5144a5')
    coffee> p.get_location_pref(console.log)
    undefined 'b9cd6c52-059d-44e1-ace0-be0a26568713'
    coffee> x.c.servers.by_id['b9cd6c52-059d-44e1-ace0-be0a26568713'].host
    '10.1.15.5'

    # then at the shell

    export project_id=cc96c0e6-8daf-467d-b8d2-354f9c5144a5; export host=10.1.15.5; export uid=447893796

    mkdir -p students/$project_id && sshfs -o cache_timeout=10 -o kernel_cache -o auto_cache -o uid=$uid -o gid=$uid -o allow_other -o default_permissions $host:/projects/$project_id students/$project_id; chown $uid:$uid students/$project_id

    CRITICAL: we must *also* use bindfs with the --create-for-user= option!!

    bindfs --create-for-user=275991804 --create-for-group=275991804 -u 1959631043 -g 1959631043


     - when a client *initiates* a move, it will query the db for any mounts and then inform the bup_servers of them. Thus the move logic is event driven, where the event is "move a project".   If the global client doing the moving can't contact the local bup_server, it will keep trying... (?)


- [ ] re-key ssl cert: http://support.godaddy.com/help/article/4976/rekeying-an-ssl-certificate

- [ ] make it so `bup_server` will refuse to start if some sanity checks regarding the filesystem fail, e.g., that bup/projects is mounted as  /projects

- [ ] implement a gossip protocol to use when deciding viability of compute nodes, rather than just trying for 15 seconds and timing out.   Try longer if gossip is good; try less if bad.

- [ ] redo file copy button to just be a straight cp.  BUT -- need to also fix FUSE mounting of bup to have proper permissions, or this leads to problems.    Pretty broken right now.

- [ ] put this script in base template vm's:

        root@compute18dc0:~# more update_salvus
        su - salvus -c "cd salvus/salvus; . salvus-env; git pull; ./make_coffee"
        cp /home/salvus/salvus/salvus/scripts/bup_storage.py /usr/local/bin/
        chmod og-w /usr/local/bin/bup_storage.py
        chmod a+rx /usr/local/bin/bup_storage.py

      and make it so gce base machines can at least get from the github repo.

- [ ] bug: snapshot browser file search doesn't work... for obvious reason: it is searching on the wrong thing!

- [ ] project undelete doesn't work.

- [ ] rewrite `bup_server` to use a local sqlite database; then state is preserved upon restart/crash/reboot/etc.

- [ ] "pip install --user pymc": https://mail.google.com/mail/u/0/?shva=1#search/Carlos+Rodriguez/14541f56e95e0756

- [ ] code to "rebuild/repair a node" -- hmm; because of this maybe need some way to know when a project was last sync'd based on filesystem

- [ ] --delete and --update together with rsync - what happens? -- we might as well make the replication actually a merge of newest files!

 - [ ] after repairing cassandra data reduce the write consistency level when making new projects... maybe. (?)

 - [ ] I'm also trying to install pymc (python montecarlo) but when I run it, it complains that the ver of numpy is too old... any tips on how to upgrade numpy or how to make pymc work?....; github ticket #2

 - [ ] put project creation date in project


 - [ ] (in progress on cloud3) create a full offsite-able backup of all bup repos of projects in dc1, and also the database nodes in dc1.

 - [ ] run through and do "bup ls master" on every repo in an offline archive, and investigate/fix ones that don't work, if any.

 - [ ] fix gce boot -- right now it boots up but doesn't mount the zfs pool -- or rather it starts getting rsync'd too before finishing the mount (?).  This is very bad.  Maybe don't go on VPN until /projects is mounted to avoid potential data loss.

 - [ ] setup so that cqlsh doesnt' need env variable but uses .cqlshrc

 - [ ] test ui changes on other browsers.

 - [ ] hourly or rolling snapshots of new *compute vm's* filesystems:
         - https://github.com/zfsnap/zfsnap/tree/legacy

 - [ ] test/fix ui changes on other browsers.

 - [ ] add a bigger (?) timeout between vm stop/start (?)

 - [ ] function to "truly" move a project within a given data center

 - [ ] write clean() -- for each project on a given host that hasn't been used in the last n days, delete .sagemathcloud, etc., directories

 - [ ] install something randy needs:  I think this will be possible in the release planned for this summer, but for now it would be nice to use Jake's mpld3 package, which doesn't seem to be installed.  I tried downloading and following the instructions at   https://github.com/jakevdp/mpld3 but didn't have permissions.  Is this something you could install globally?

 - [ ] make this standard  -- https://github.com/biocore/scikit-bio   -- see https://mail.google.com/mail/u/0/?shva=1#inbox/1454ce211132e2bf

 - [ ] MAYBE -- or maybe not -- change bup_storage to never delete account: it's very useful for linking projects and sharing files to have account available at all times.  will make, e.g., persistent sshfs possible; make sure .ssh is not ssh excluded from rsync

- [ ] have stable ipv6 project ip addresses be would be a huge *win*.  LXC would make that possible.

- [ ] deal with the exception around this - codecs.open(self.save_log,'a',"utf-8-sig").write(json.dumps(r)+'\n')

- [ ] go through and chown/sync every project systematically; evidently I didn't in the current migration, so I'll just put a chown in the start script for now -- this slows things down, but is temporary.

- [ ] make it so move is never automatic but prompted?

- [ ] automated rolling snapshots of bup/projects

- [ ] add bup quota as a standard part of settings, and refuse to make further snapshots if bup usage exceeds 3 times user disk quota.  This will avoid a horrible edge case.   Critical that this produces an error that the user learns about.  This will happen for some users.  Alternatively, I could periodically rebuild those bup repos with many snapshots deleted - that would be much nicer and is totally do-able.

- [ ] script to cleanup bup repos, e.g., delete tmp files, maybe recompact, etc.

- [ ] manual project move system -- bring it back...

- [ ] 3d graphics improvements - check out http://clara.io/, which is based on threejs, but does realtime sync, etc. THAT's what we want. https://news.ycombinator.com/item?id=7709928

- [x] change default browser font to Monospace (browser default); makes the most sense!

- [x] get GCE VM restart to robustly work with all proper mounting.

- [x] change the sage extensions instructions for ipython everywhere to "%load_ext sage"

- [x] systematic chown -- permissions: need to go through and fix all perms once and for all...
     - [x] add a bup_storage.py chown <project_id> command, which avoids the .snapshots directory
     - [x] write a little python script that runs the chown thing on each project
     - [x] launch it all over

- [x] setup a small test SMC in Europe for testing:
    - [x] define and spin up a web machine and compute machine
    - [x] configure zfs pool for compute machine
    - [x] add secrets for stunnel
    - [x] add web machine to database firewall -- for this machine only 10.1.10.2
    - [x] start hub, nginx, stunnel, haproxy on the web machine... in a way that doesn't make all the other haproxies use it
    - [x] modify `bup_server` code and db so we can set a compute server to be "experimental", hence will be exluded when new projects created
ALTER TABLE storage_servers ADD experimental   boolean;
    - [x] add bup server for new compute machine to db as experimental

    coffee> x={};require('bup_server').global_client(cb:(e,c)->x.c=c)
    coffee> x.c.register_server(host:'10.4.1.3', dc:3, experimental:true, cb:console.log)

    # make this add the given target machine to locations
    x={};require('bup_server').global_client(cb:(e,c)->x.c=c)
    p=x.c.get_project('9834eb82-b34c-41b1-ba3e-60910aa46e12')
    p.set_last_save(last_save:{'ec2818ce-213a-4318-8f8b-6adaff99b696':0}, cb:console.log, allow_delete:true)
    p.move(target:'ec2818ce-213a-4318-8f8b-6adaff99b696',cb:console.log)
    p.set_settings(cb:console.log, cores:4, cpu_shares:1024, memory:32, mintime:9999999999999999)   # mintime is in units of seconds.

    - [x] add it as a host for projects
    - [x] after sync, move project(s) there.

- [x] delete old vm images


- [x] cassandra 2 upgrade
    HOW?
       - upgrade cassandra, node, nginx, haproxy in base vm's
       - [x] make a testing vm with /mnt/cassandra from cassandra3
       - [x] remove it from tinc network and directly ssh in
       - [x] try to start cassandra 2.0.7 and see what happens, etc., following the official instructions about how to upgrade
       - [x] debug until it works
       - [x] make cassandra snapshot across both dc's:
           nodetool snapshot salvus -t before-c2
       - [x] make zfs snapshot across both dc's
       - [ ] upgrade dc1


                cd salvus/salvus/; . salvus-env

                nodetool upgradesstables system IndexInfo
                nodetool upgradesstables salvus plans
                nodetool upgradesstables salvus project_users
                nodetool upgradesstables salvus stats
                nodetool upgradesstables salvus uuid_value
                nodetool upgradesstables salvus snap_servers
                nodetool upgradesstables salvus accounts
                rm -v /mnt/cassandra/lib/data/salvus/stats/salvus-stats-ib-*
                rm -v /mnt/cassandra/lib/data/salvus/snap_servers/salvus-snap_servers-ib-*
                rm -v /mnt/cassandra/lib/data/salvus/uuid_value/salvus-uuid_value-ib-*
                rm -v /mnt/cassandra/lib/data/salvus/project_users/salvus-project_users-ib-*

                nodetool drain

                sudo zfs snapshot cassandra@before-c2-2

                cloud.stop('cassandra',host='cassandra1')

                sudo shutdown -h now

                cloud.restart('vm',hostname='cassandra1')

                cloud.start('cassandra',host='cassandra1')

ERROR [main] 2014-05-09 14:02:12,796 CassandraDaemon.java (line 497) Exception encountered during startup
java.lang.RuntimeException: Incompatible SSTable found.  Current version jb is unable to read file: /mnt/cassandra/lib/data/system/IndexInfo/system-IndexInfo-ib-33.  Please run upgradesstables.

ERROR [main] 2014-05-09 14:09:53,536 CassandraDaemon.java (line 497) Exception encountered during startup
java.lang.RuntimeException: Incompatible SSTable found.  Current version jb is unable to read file: /mnt/cassandra/lib/data/salvus/plans/salvus-plans.plans_current_idx-ib-1.  Please run upgrades
stables.


       - [x] upgrade dc0

---
## next vm update

 - [x] sage -sh; umask 022; pip install gmpy2
 - [x] apt-get install libapr1-dev

---

- [x] cassandra in gces

  ALTER KEYSPACE "salvus" WITH REPLICATION = {'class' : 'NetworkTopologyStrategy', 'DC0':3, 'DC1':3, 'DC2':3, 'DC3':3};



- [x] check on backups
- [x] upgrade all hosts
- [x] restart all older cassandra nodes, but with new configuration
- [x] add 2 web hosts to each gce dc  (launched)
- [x] nodetool repair? -- it worked; improve script to cover all nodes (via output of nodetool status) and start again with "every".
- [x] make a graph of tinc connection times between nodes
      - trying out node-net-ping
      Had to do
          setcap cap_net_raw,cap_net_admin=eip  /projects/3702601d-9fbc-4e4e-b7ab-c10a79e34d3b/salvus/salvus/data/local/bin/node
      Can get ping time as follows:
          w=require('misc').mswalltime; t=w(); session.pingHost('10.1.1.5', (e,r) -> console.log(w(t)))

- [x] add extra information / improve tinc key distribution / deal with firewall issues / static ips, etc., so all of tinc network is optimal

    Make communication between 10.4.1.5 and 10.4.1.3 fast.

Tinc notes:

       ProcessPriority = high
       LocalDiscovery = yes

OK, as far as I can tell, LocalDiscover is totally useless and doesn't work, as is all other automatic discovery.
Instead, we have to explicitly specify ConnectTo parameters in the tinc.conf file, to connect to *everything* else.
AND we also need to copy the public keys around as well.
But doing this is really hard.
I'm really puzzled.
Maybe the problem is how I'm setting up the subnets or something.

OK, I think the only solution is the following:

  (1) in vmgce.py we have to do this:
       (1) attempt in parallel to copy over public key file to all machines that have
       the Address= line to new machine; note which succeed.
       (2) in conf, put in ConnectTo, *every* host that we successfully copied to in step 1.

  (2) don't forget -- current compute nodes, etc., aren't setup properly....

Wait, some changes are needed due to how Google bills things: "Packets sent to an instance using an external address are billed as external traffic, even if the sender is in the same network"

Question: do the internal addresses work across DC's? Yes.
So this means we need to use:

    Address=external address for external machines that need to initiate a direct connect to gce nodes
    Address=internal address for gce nodes.

This complicates things a little.
I could add a parameter for key distribution that is ignored by tinc, but used by my scripts:

    ExternalAddress =

OR, I could copy the public key directly from the new host to the other hosts... (but can't due to ssh keys not allowing that).

  --> OR, I could distribute keys using the vpn instead of the Address=line.  I.e., use "Subnet = 10.3.1.2".



- [x] point europe web host at europe database -- encourage testing
- [x] sync-related timeouts: raise everything from 10 to at least 30.
- [x] make the rsync replication be rate limited -- it does noticeably

- [x] biber: https://mail.google.com/mail/u/0/?shva=1#inbox/145ebb10a70a83ab


- [x] testing swap on compute10
        zfs create pool/swap -V 32G -b 4K
        mkswap -f /dev/pool/swap
        zfs set compression=lz4 pool/swap
        zfs set dedup=off pool/swap
        swapon /dev/pool/swap

