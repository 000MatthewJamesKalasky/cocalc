Aug 10, 2012:
------------

I'm setting up VM's on my infrastructure.  Right now I have:

   * virtualbox machines running in spare cycles on geom.math.washington.edu (24-core, 128GB RAM, 128GB SSD for salvus):

configuring:

   * salvus1: 8GB disk, 8GB RAM, 8 cores: machine -- stunnel, haproxy, nginx, database (master), tornado
   * salvus4: 8GB disk + 8GB extra /tmp disk, 16 cores; 16GB RAM: sage server

   * virtualbox machines running in spare cycles on combinat.math.washington.edu (64-core, 192GB RAM, 64GB HD for salvus)

   * salvus5: 8GB disk, 8GB RAM: machine -- stunnel, haproxy, nginx, database (slave), tornado
   * salvus8: 8GB disk + 8GB extra /tmp disk, 16 cores; 16GB RAM: sage server

   * 1 virtual machine at servedby.net


!!!

combinat is massively faster than geom at running virtual machines.  
Either there is something really weird with the disk (which I doubt, since
boxen had the same problem), or the instruction set support, or BIOS 
configuration, or something. Very weird. 


Aug 27, 2012:
------------

My primary goal for this morning is getting the basic demo to work
using Cassandra, having completely removed postgreSQL + memcached.
Once that is done, I'll work on deployment.  I'm going to investigate
partial other-people-wrote-them replacements for admin.py, just
because that script is complicated, and generic:
   * ansible
   * "open source configuration management software" (wikipedia)

  * As of 6:08am (in about 15-20 minutes of time) my primary goal is done -- it works.

I think I'll deal with this first:

  [x] restart of cassandra via my admin script fails, since maybe it takes too long to shut down

OK, that was easy...

Then onto serious stuff.

Testing out Ansible (http://ansible.github.com/) 

   easy_install jinja2 pyyaml paramiko
   export ANSIBLE_LIBRARY=/Users/wstein/salvus/salvus/data/local/ansible
   export ANSIBLE_HOSTS=/Users/wstein/salvus/salvus/conf/ansible_hosts
   git clone git://github.com/ansible/ansible.git
   cd ansible && python setup.py install

Setting up a public git server on combinat1:

  http://git-scm.com/book/en/Git-on-the-Server-Public-Access


salvus@combinat1:~$ apt-get install apache2
salvus@combinat1:~$ git clone salvus salvus.git
salvus@combinat1:~$ cd salvus.git/.git/hooks;  cp post-update.sample post-update; cd
salvus@combinat1:~$ more /etc/apache2/httpd.conf 
<VirtualHost *:80>
    DocumentRoot /home/salvus/salvus.git/.git
    <Directory /home/salvus/salvus.git/.git>
        Order allow,deny
        allow from all
    </Directory>
</VirtualHost>

Then:
  http://combinat1.salv.us/
gives the repo. 

   export ANSIBLE_LIBRARY=/Users/wstein/salvus/salvus/data/local/ansible
   export ANSIBLE_HOSTS=/Users/wstein/salvus/salvus/conf/ansible_hosts

OK, done (in seconds), and now this works to automatically update 
all nodes to latest version.

  ansible all -m shell -a "cd salvus; git pull http://combinat1.salv.us" -u salvus

Useful:

  ansible cassandra -a "df -h" -u salvus

Build cassandra on all nodes:

  ansible all -m shell -a "cd salvus/salvus; ./build.py --build_cassandra" -u salvus



I'm now working on diving up the nodes and deploying. 

[cassandra]
servedby1.salv.us -- just asked them to give me 64GB disk space
bsd1.salv.us -- creating and easily adding a 64GB disk:

   VBoxManage createhd --filename cassandra.vdi --size 64000 --variant Fixed   # takes about 10 minutes, at least?
   ansible bsd1 -a "shutdown -h now" -u salvus -s -K  # shutdown bsd1...
   VBoxManage storageattach bsd1 --storagectl "SATA Controller" --medium cassandra.vdi --port 2 --type hdd      
   nohup VBoxHeadless -s bsd1 --vrde off &

# format:
   ssh salvus@bsd1
   sudo su
   fdisk /dev/sdb   # make 1 new partition that is the whole thing
   mkfs.ext4 /dev/sdb1
# get uuid:
   ls -lh /dev/disk/by-uuid
   702faff3-0626-439f-94f9-c4174efd68de
# edit /etc/fstab:
   UUID=702faff3-0626-439f-94f9-c4174efd68de /cassandra               ext4    errors=remount-ro 0       1
# make path:
   mkdir /cassandra
   chown salvus. /cassandra

   mount -a
   
   
  

combinat4.salv.us -- 
   1. need to use lvm to expand my existing 64GB partition:
- stop all vm's:

ansible combinatvm -a "shutdown -h now" -u salvus -s -K
ps ax |grep VBox  # confirm it worked (on combinat)

- kill vpn server:

2134 ?        Ss     0:34 /home/salvus/salvus/salvus/data/local/sbin/tincd

- umount the volume

df -h
/dev/mapper/combinat-salvus    60G   50G  7.0G  88% /home/salvus


- lvextend -L+64G /dev/mapper/combinat-salvus
FAIL!
  Extending logical volume salvus to 123.60 GiB
  Insufficient free space: 16384 extents needed, but only 9968 available

I did math and got:

- lvextend -L+38.9G /dev/mapper/combinat-salvus
  Rounding up size to full physical extent 38.90 GiB
  Extending logical volume salvus to 98.50 GiB
  Logical volume salvus successfully resized

- e2fsck -f /dev/mapper/combinat-salvus
- resize2fs /dev/mapper/combinat-salvus
- mount /home/salvus
- df -h
/dev/mapper/combinat-salvus    99G   51G   44G  54% /home/salvus

   2. make a new 40GB lvm drive  -- that's all the space we have.  This is just an initial testing deployment!

# it took only about 2 minutes!


   3. attach it to combinat4

VBoxManage storageattach combinat4 --storagectl "SATA Controller" --medium cassandra.vdi --port 2 --type hdd      

# oops -- already had users.vdi -- this was designed for running sage.  need to use a different vm!
# should use combinat2.salv.us

VBoxManage storageattach combinat4 --storagectl "SATA Controller" --medium none --port 2 --type hdd 

VBoxManage closemedium disk cassandra.vdi
VBoxManage list hdds
mv cassandra.vdi ../combinat2

# fix what I broke:
VBoxManage storageattach combinat4 --storagectl "SATA Controller" --medium tmp.vdi --port 2 --type hdd 

cd ../combinat2
VBoxManage storageattach combinat2 --storagectl "SATA Controller" --medium cassandra.vdi --port 2 --type hdd      


# woah -- I got the hostnames wrong!
VBoxManage modifyvm combinat2 --name combinat4a
VBoxManage modifyvm combinat4 --name combinat2
VBoxManage modifyvm combinat4a --name combinat4

# repeat steps as for bsd1 above to format and mount disk as /cassandra
done



----

OK, now that I know how I'm going to setup things up for the initial
release, and have sufficient disk space, back to learning about
ansible!

Aug 29, 2012:
------------

It's 5:08pm.  I will now quickly check if there any obvious things
better than ansible, but if not, I'll start setting up my
infrastructure using it.

I'm just going to go with ansible. Why?  Because it is actually very
close to what I already wrote with my admin.py, at least in spirit.  
Use ssh, etc. 

I setup git@combinat1 as follows:

1. sudo adduser git

2. git@combinat1:~$ git --bare init
Initialized empty Git repository in /home/git/

3. On my laptop:
blastoff:salvus wstein$ git push git@combinat1:. master

4. make .ssh/authorized_keys be the file with keys for
   all the non-sage servers:

blastoff:ansible wstein$ scp authorized_keys git@combinat1:.ssh/

5. Change shell to git-shell for security purposes:
   chsh -s /usr/bin/git-shell git

6. test with ansible

I have to change this in /etc/ssh/ssh_config:

   StrictHostKeyChecking no

on every host, so it doesn't ask for the ssh key the first time.

  ansible combinat4.salv.us -m shell -a "echo 'StrictHostKeyChecking no' >> /etc/ssh/ssh_config" -u salvus -s -K


---

First thing to do on Aug 30:

   - finish adding strict host checking and authorized keys even for the sage servers; we want them
     to be up to date with the VPN, etc. 


August 30, 2012:

Goal for today: nail down control of infrastructure. 

More precisely, type password once and do all of following:

   * ability to update salvus git repo on all nodes easily

   * start sage servers

   * start nginx, haproxy, 


   * start cassandra

   * monitor to see what is running or not

   * 

   
---

OK, after using ansible more, I do not like it.  But there are things
I like about it.  However, those things are all really paramiko
things, so I'm getting rid of ansible from salvus, and just adding
paramiko, then finishing my admin.py script.


---

My tinc vpn is still not in great shape.  I really need to make the
configuration much more centralized and systematic, e.g., all the
tinc.conf files for all nodes need to be in the repo.  

I've been having trouble on my laptop since I didn't have
   TCPonly = yes
set in tinc.conf.

I want to install paramiko everywhere....

-----

OK, I need to do this ... TODAY:

 [ ] start nginx running on several nodes automatically
... etc.

HOW.

 Problem 1: How to get status of nginx.  There is a pidfile.

  
Aug 31, 2012:
-------------
  cd salvus/salvus/data/secrets/
  scp -r wstein@salvus0.salv.us:/Users/wstein/salvus/salvus/data/secrets/salv.us .
  scp wstein@salvus0.salv.us:/Users/wstein/salvus/salvus/data/secrets/tornado.conf .

Sept 1, 2012:
-------------
  
I'm trying to deploy now!  It'll be a lot of debugging of every component...

Obvious todos for today:

 [x] change firewall rules for servedby1, so it can take http/https traffic
 [x] godaddy: point salv.us as combinat and servedby1
 [x] apt-get upgrade vm machines -- on general principal
 [x] http://salv.us is the normal website served by apache; start on servedby1 also

---

Went to breakfast with Tish and did work on the vert ramp for 2 hours and cleaned house.

---

until 5:30
 [x] get cassandra setup and running on cluster

------

5:30-8:30: went skateboarding and hanging out with kids at Jefferson

------

9:00pm: 

 [ ] protect cassandra nodes as follows:
     use iptables to make a rule so all incoming packets from interfaces that are
     aimed at the cassandra ports are blocked, unless they come from one of a white-listed
     group of nodes.  These will be exactly the tornado nodes.   
     This way we still only need one VPN!

Goal: block incoming packets to Cassandra default ports: 7000, 7001, 9160 except
from a very specific list of hosts, namely all hosts running tornado.

ufw deny 7000,7001,9160

Sept 2, 2012:
-------------

 [x] I'm very confused about cassandra, due to *nothing* in the salvus keyspace working -- I get
     "Unable to complete request: one or more nodes were unavailable." 
     no matter what I do.  Why?

     - If I create a new columnfamily in the same keyspace, it fails:

      cqlsh:salvus> create columnfamily foo (key varchar primary key, value varchar);
      cqlsh:salvus> update foo set value='stein' where key='william';
      Unable to complete request: one or more nodes were unavailable.

     - if I create a new keyspace and table in it, things work fine:

cqlsh:salvus> CREATE KEYSPACE test2 WITH strategy_class = 'NetworkTopologyStrategy' AND strategy_options:DC0 = 1 AND strategy_options:DC1 = 1 and strategy_options:DC2 = 1;
cqlsh:salvus> use test2;
cqlsh:test2> create columnfamily foo (key varchar primary key, value varchar);
cqlsh:test2> update foo set value='stein' where key='william';
cqlsh:test2> select * from foo;
 KEY     | value
---------+-------
 william | stein

... And I made the whole schema and it seems to work fine?!

Maybe I mucked things up by accidentally using CQL2?

Delete everything and restart:

import admin, deploy; reload(admin); reload(deploy); h = deploy.hosts; s=deploy.services
h.ping('cassandra')
s.stop('cassandra')
s.status('cassandra')
h('cassandra', 'rm -rf /home/salvus/salvus/salvus/data/cassandra-0', timeout=10)
s.start('cassandra')
s.status('cassandra')

---

cassandra.init_cassandra_schema()
c = cassandra.cursor()
c.execute('select * from services')
# BOOM!
   OperationalError: Unable to complete request: one or more nodes were unavailable.


How to change keyspace options.   

Use cassandra-cli (!):

$ cassandra-cli -h servedby1.salv.us -k salvus
[default@salvus] update keyspace salvus with strategy_options={DC0:3, DC1:3, DC2:3};
50c04637-08eb-38a7-94bc-c84eeb0b82ac
Waiting for schema agreement...
... schemas agree across the cluster
[default@salvus] 
    
This does work.  This fixes the problem, strangely, and much to my surprise. 

Moreover, this sucker is frightenlying DURABLE.  I just tried killing 2 out of 3 of the nodes,
and everything "just works". Amazing. 


 [x] I see "Cluster: Test Cluster" when I do "describe cluster".  This should be "salvus"!

 [x] protect cassandra nodes as follows:
     use iptables to make a rule so all incoming packets from interfaces that are
     aimed at the cassandra ports are blocked, unless they come from one of a white-listed
     group of nodes.  These will be exactly the tornado nodes.   
     This way we still only need one VPN!

Goal: block incoming packets to Cassandra default ports: 7000, 7001, 9160 except
from a very specific list of hosts, namely all hosts running tornado.

>>> import admin, deploy; reload(admin); reload(deploy); h = deploy.hosts; s=deploy.services
>>> ','.join(h.ip_addresses('tornado cassandra laptop'))
10.38.1.5,10.38.1.7,10.38.1.10,10.38.1.1,10.38.1.11

ufw reset
ufw allow 
ufw allow proto tcp from 10.38.1.10 to any port 7000,7001,9160
ufw deny 9160
ufw deny 7000
ufw deny 7001
ufw enable
ufw status


So it's possible...

http://www.datastax.com/docs/1.1/references/firewall_ref

What I want for each cassandra node (which in some cases is also other services except sage):

0. Reset

  ufw --force reset  

1. Allow ssh, nginx, http, stunnel, tornado access from anywhere, which means anywhere in VPN.
  
  ufw allow 22
  ufw allow 80
  ufw allow 443
  ufw allow 5000
  ufw allow 8000
  ufw allow 8080

2. Allow all access from salvus0 (master control), tornado, and cassandra nodes.

  ufw allow from 10.38.1.1; ufw allow from 10.38.1.5; ufw allow from 10.38.1.7; ufw allow from 10.38.1.10; ufw allow from 10.38.1.11

3. Deny everything else. 

  ufw deny proto tcp to any port 1:65535; ufw deny proto udp to any port 1:65535

4. Start it

  ufw --force enable

[x] DAMN -- somehow I managed to firewall myself out of servedby1 anyways!  Time to learn how to get
a console on those boxes...  I will attempt with MS windows.

It turns out using Windows + Firefox *WORKS*.   Nothing else works, even IE....  To get the cursor out
of the VMRC console, hit "control+right_alt/option".


  [x] OK, make activating the above firewall automatic.

 [x] similar firewall for sage nodes:
      * accept 22 from everywhere
      * accept 6000 (the sage server port)  from all tornado nodes  and from salvus0
      * deny everything else
      * deny all outgoing

 [x] commit and gitpush to all nodes...

 [x] start stunnel's

 [x] make it so haproxy auto-determines nginx and tornado nodes, so they don't have to be passed in -- more precisely, the admin program finds this info at runtime. 

 [x] get haproxy setup and running on cluster

 [x] get nginx setup and running on cluster

 [x] get tornado setup and running on cluster:

   -- I forgot about the second inter-tornado-server communications tcp port.  The firewall is blocking it.
      But also, even with that disabled, it isn't working.  
      Oh, 7000 is the Cassandra port, isn't it?  Yep. 
      So... I'll make an option to pass in the port to Tornado, and have the port be 5001.

 [x] database is not working again !
     I'm going to try getting rid of "data centers", since after all with only one node per, we get *no* advantage yet.

 [x] get sage setup and running on cluster

 


Sept 3, 2012:
-------------

I'm pretty excited to wake up and see that salvus is still actually
working, after I turned it on yesterday.

----------------------------

 [x] sage machines don't have DNS so they can't lookup tinc network.  for now, unblock dns.

 [x] major security issue: make sure Sage/tornado servers *only* bind to VPN address!

 [x] make a fully working local deployment: OS X only

 [x] fixed that very annoying sage_server bug where computation doesn't terminate, even though it does; disable monitor.py as pointless

Next up

 [x] https://salv.us doesn't work in safari or opera
 [x] fix the damned SSL cert!
 [ ] make a fully working local deployment: 2 linux vm's
 [ ] update sockjs-tornado: https://github.com/mrjoes/sockjs-tornado
 [ ] delete files created by running sage process:
print os.popen('hostname').read()
open('/tmp/foo','w').write('a'*1000000)
print os.popen('ls -lh /tmp/').read()
 [ ] quota on sage process
 [ ] when fail to connect to sage server (i.e., maybe it is down), should try another, unless there are none left!


Sept 4, 2012:
------------

For better or worse, I just totally redid the VPN right now that all ports are opened. :-)

git pull https://github.com/williamstein/salvus.git
williamstein

admin.tinc_conf('servedby1',...)

vi data/local/etc/tinc/tinc.conf 
ConnectTo = bsd0
ConnectTo = combinat0
ConnectTo = servedby1

killall tincd
/home/salvus/salvus/salvus/data/local/sbin/tincd -D -d 3
----

starting everything again seemed flakie.  ick.

---

Now to try killing 2 out of 3 stunnel's and see what happens.

---

The non-local network speed is completely killing evaluation.
I need to make it so only the local cassandras are used, and
we don't make a new connection each time.
Likewise with sage compute.  Basically we need to implement the snitch,
since without that we're in pain.

[ ] Thought: i wonder if my websocket issues are related to
the haproxy load balancing alg?

...

[x] i just got obsessed with the ssl cert issue for a minute and found this site:

http://www.webhostingtalk.com/showthread.php?t=1038061

I think it means that I need to put the contents of gd_bundle.crt in
nopassphrase.pem, in the right order.  Wow that worked!  I just put
gd_bundle.crt at the end of nopassphrase.pem, and we're good to go!!


2012-09-04 at 10:37pm.

done * I'm going to move the Time: line in the UI to the top bar.
 * I might make the output expand in mobile mode, if I have the energy.  (nope!)
 * I might add a javascript spinner animation when the computation runs, if I have the energy.


Email to Jason Grout about architecture:
-----------------------------------------


BUG: sage server We should not allow incoming traffic to port 6000 even from the same machine!!!

salvus@bsd2:~/salvus/salvus$ sage -python sage_server.py -c -p 6000 --hostname bsd2.salv.us
PID = 17184
sage [0]: 2+3
5
sage [1]: 
Exiting Sage client.

Or in salvus:
print os.popen('echo "2+2" | sage -python sage_server.py -c -p 6000 --hostname bsd2.salv.us').read()

Maybe?
import os, socket   
print socket.gethostname()
os.system('echo "2+2" | sage -python /tmp/sage_server.py -c -p 6000 --hostname `hostname`.salv.us > /tmp/a')
print "out=", open('/tmp/a').read()


On Tue, Sep 4, 2012 at 9:00 AM, Jason Grout <jason.grout@drake.edu> wrote:
> On 9/3/12 8:39 PM, William Stein wrote:
>>
>> I did some design work and evaluation of infrastructure, then made choices
>> and implemented things. The technical aspects of design have changed
>> substantially from what I showed you in June, though its fundamentally
>> similar. Here are the main software components I'm using: * Tinc -- VPN *
>> Stunnel -- SSL termination * Haproxy -- load balancing and proxy server *
>> Cassandra -- database * Nginx -- static content * Tornado -- websocket
>> server; messaging; dynamic content * Sage -- math Also, I'm using *
>> Protobuf2 -- binary message format * Paramiko -- configuration management
>> (paramiko is an ssh implementation in Python)
>
>
> (yep; we're using paramiko to launch Sage processes and provide the HMAC
> initial key for messages.)
>
>
>>
>> Sage server processes with the library pre-imported runs as root in
>> completely separate virtual machines from anything else. When a server
>> receives a connection, it forks, and drops privileges to a *random*
>> user id (there is no need to actually create the user).   I haven't
>> setup automatic quotas in my current deployment yet, but I had that in
>> a previous version and will soon.          I had implemented the sage
>> server using SSL and passwords, but completely removed that.  Instead,
>> the server machine is on a VPN and has a firewall (setup using ufw)
>> that only allows packets in from a select white list of other machines
>> on the same VPN.
>
>
> That's interesting dropping to a user without creating an account. Do you
> clean up after that account after a computation session?

Yes. They can only create a limited number of files under /tmp and
those all get deleted when their session terminated.  The current
version of the code doesn't actually do the cleanup, though some
version I wrote this summer does.

> As for VPN: We had talked about having server machines talking directly with
> the client (e.g., websockets)---is that forwarded through the VPN, or are
> the websocket and dynamic data messages allowed to any computer from the
> server?

The processes running Sage *only* communicate with a separate computer
(on the same vpn) that is running a Tornado server.   The Tornado
server then talks
to the clients (albiet, through haproxy + stunnel).  So the architecture is:

   (Sage) <---> (Tornado) <---> (Haproxy)  <---> (Stunnel)  <---> (Client)
                                /|\
                                 |
                                \|/
                         (Cassandra)

I intend to support three types of clients:

       1. sockjs (so basically websocket or long polling)

       2. a classical JSON HTTP api, with polling, though I'll
severely restrict the amount of polling per second allowed.

       3. a standard SSL TCP socket .

Type 1 will be preferred for web browsers.    Type 2 will allow for
simplistic clients, easy use from urllib2 (say), and it should even be
possible to make it worth with something like lynx (no javascript),
though obviously that will require the user to click a link to see the
progress of their computation.   Type 3 will be for the command line
client, the desktop GUI application, and maybe a mobile app.


> I can imagine that the control messages (like start a new Sage
> instance) would only be allowed through the VPN.

When my script starts the Sage server process it also configures a
very restrictive firewall for that machine.  It blocks all incoming
traffic except for traffic on one port from the machines on the VPN
that are running tornado, and ssh so I can manage the machine.   It
blocks almost all outgoing traffic (some outgoing traffic is required
to stay on the VPN).

> What happens if a server
> is compromised? Can it compromise the rest of the network through the VPN
> easily?

It could disable the firewall, then send bogus messages to tornado
servers.  These would be annoying, but I don't think they would cause
very serious trouble or lead to information leakage.  The tornado
servers know that the machine is a
sage server, so it can't send messages as if it were not a sage
server.  It also can't change it's ip address or identification on the
VPN, because it would have to put new keys on the vpn server nodes.

> In other words, do you absolutely trust any message over the VPN is
> valid, without authentication?



>
>
>
>
>>
>> I had to switch from PostgreSQL to Cassandra because of my longterm
>> estimated storage requirements, and my design goals regarding salvus
>> being able withstand certain types of hardware and network failures.
>> For similar reasons, I switched from OpenVPN (which has a single point
>> of failure) to Tinc, which is a p2p vpn with no single points of
>> failure.  With my current *design*, you can just randomly kill quite a
>> few machines, and the system just stays up.
>>
>> Also, as I explained to you, I had planned to implement storage of
>> user data on nodes, replicated using git and coordinates using
>> PostgreSQL... but it turns out that Cassandra already does everything
>> I was planning to implement and much, much more regarding distributed
>> storage of data.  So that was another reason to use Cassandra instead
>> of PostgreSQL.  I'm also not using memcached anymore, since Cassandra
>> -- properly configured -- does much the same thing, with much less and
>> cleaner client code needed.
>
>
> Using git to synchronize things, we also had versioned data.  Are you still
> trying to version data in Cassandra?  Are you storing *all* user data/files
> in Cassandra?  I'm not familiar enough with Cassandra to really have a feel
> for how data needs to be structured, but it seems like you need to basically
> implement GridFS for Cassandra.

No.  I will simply be storing git bundles in Cassandra.  All data is
still versioned, and I do not have to implement 'GridFS'.


2012-09-05 at 12:36pm. 
----------------------

I'm doing budget work for a grant right now.   We have:

  (10648*1.269 + 500)/.8 = 17515.3900000000 = amount I need to bring
  in to pay myself and for hosting costs

I will thus make getting $20K/month my goal for income from Salvus for this year. 

At $99/year (or $10/month), that would require 2400 users.  That's my
target for paying customers for year 1.


Sept 18, 2012:
--------------
    [ ] implement a private cloud setup on my laptop with 3 vm's
        - setup rados cluster
        - setup ceph and test
        - try with ganeti, unless it is too hard; if so, switch to a java stack?

    [ ] implement private cloud setup on 01-04salvus machines.
    [ ] order four new machines for UW tower using DOD + Sage Foundation + combinat

    [ ] admin script: make it do the ssh stuff in parallel, somehow.
    [ ] expand deploy config to make full use of the new machines.
    [ ] upgrade tornado in salvus -- 2.4 is out!
    [ ] salvus programming:
         [ ] automated sage server disk quotas 
         [ ] remove all files after session terminates
         [ ] make choice of sage server determined by speed/success with previous choice
         [ ] make choice of database determined by speed/success with previous choice
         [ ] update jquery
         [ ] desktop -- codemirror 2 editor
         [ ] update jquery-mobile
         [ ] mobile -- growing output
         [ ] optional user login/accounts: local/google/facebook -- terms of usage agreement
         [ ] nice way to browse/delete from/rank/publish past computations
         [ ] communication between tornado servers sending the proto messages to correct recipients
         [ ] way to send generic json message from sage process to browser; ensure fast
         [ ] persistent sessions (options appears on login)
         [ ] embed ability to compute via "mashup" in other webpages via a javascript snippet.
 

------------


----------------------------------------------------

Ceph is starting to finally do something useful...  This is a
nontrivial piece of software!

1. Install newest ceph/rados:

wget -q -O- https://raw.github.com/ceph/ceph/master/keys/release.asc | sudo apt-key add - 
echo deb http://ceph.com/debian/ $(lsb_release -sc) main | sudo tee /etc/apt/sources.list.d/ceph.list 
sudo apt-get update && sudo apt-get install ceph

2. Config:
   - make /dev/sdb1 with a btrfs formated filesystem on each node
   - do *NOT* put anything in /etc/fstab!

3. I made a file ceph1.conf: 
   - on salvus1 in /var/lib/ceph.

Here is the one I used here:

#----------------------------------------

[mon]
    mon data = /var/lib/ceph/mon

[mon.1]
   host = salvus1
   mon addr = 192.168.56.150:6789

[mon.2]
   host = salvus2
   mon addr = 192.168.56.151:6789

[mon.3]
   host = salvus3
   mon addr = 192.168.56.152:6789


[mds]
  mds data = /var/lib/ceph/mds

[mds.1]
  host =salvus1

[mds.2]
  host =salvus2

[mds.3]
  host =salvus3

[osd]
    osd data = /var/lib/ceph/osd
    osd journal = /var/lib/ceph/osd/journal
    osd journal size=100; journal size, in megabytes

[osd.0]
    host = salvus1
    btrfs devs = /dev/sdb1

[osd.2]
    host = salvus2
    btrfs devs = /dev/sdb1

[osd.3]
    host = salvus3
    btrfs devs = /dev/sdb1

#----------------------------------------

4. Setup password-less login as root via ssh from salvus1 to salvus2 and salvus3.

 a. On all nodes -- Edit /etc/ssh/sshd_config to allow root login, then 
        /etc/init.d/ssh restart
    Also, set root passwd.

   ssh-keygen -b 2048
   ssh-copy-id salvus2
   ssh-copy-id salvus3

5. Push out config:

    mkcephfs -a -c /var/lib/ceph/ceph1.conf --mkbtrfs 

6. Try it:

    root@salvus1:~# mount -t ceph `hostname`:/ /ceph
    root@salvus1:/# df -h |grep mnt
    192.168.56.150:/   24G  2.8G   22G  12% /ceph

and similar from other machines.

7. WTF?  I made 100 files on salvus3 and have:

root@salvus2:/ceph/z# time ls
real	0m3.998s

repeatedly.  Why is it so incredibly slow?  The network connection is very fast -- 24MB/s.

I also tried copying a 100MB file to /ceph, and it takes
forever... basically hanging/killing the whole thing!  This is
ridiculous.  

I tried turning of noatime, and that fixed nothing.
Transfer of a single 100MB file takes the time it should.
But ls of a bunch of small files is insanely slow.

The slowness problem completely vanishing by using ceph-fuse: 

    mkdir /ceph
    ceph-fuse -m `hostname` /ceph

This is fast.

----

Yeah!  Now back to:

  http://blog.bob.sh/2012/02/basic-ceph-storage-kvm-virtualisation.html

  "A Basic Ceph Storage & KVM Virtualisation Tutorial"

Making snapshots works:

mkdir /ceph/test
cd /ceph/test
touch a b c
mkdir .snap/my_snapshot

Trying out RBD:

   modprobe rbd  # necessary!
   rbd create mydisk --size 150 # in megabytes
   rbd list
   echo "192.168.56.150 name=admin rbd mydisk" > /sys/bus/rbd/add
   ls -lh /dev/rbd/rbd/mydisk
        lrwxrwxrwx 1 root root 10 Sep 18 20:20 /dev/rbd/rbd/mydisk -> ../../rbd0
   mkfs -t ext4 /dev/rbd/rbd/mydisk
   mkdir /mnt/mydisk
    mount /dev/rbd/rbd/mydisk /mnt/mydisk

Benchmark:

  python -c "for i in range(1000): open(str(i),'w').write(str(i))"

Is fast!

  python -c "open('a','w').write('1'*(10**8))"

Now try to make a KVM VM:

   rados mkpool vm_disks 
   qemu-img create -f rbd rbd:vm_disks/box1_disk1 1G
   
# disk.xml:

<disk type='network' device='disk'>
  <source protocol='rbd' name='vm_disks/box1_disk1'/>
  <target dev='vda' bus='virtio'/>
</disk>

###########################

Too slow network on my laptop because of network...

-----------------

Setup on 01-02salvus in server room with Rados and Ganeti.

1. Setup password-less root around

root@01salvus:~# ssh-keygen -b 2048
root@01salvus:~# ssh-copy-id 02salvus.math.washington.edu
root@01salvus:~# ssh-copy-id 03salvus.math.washington.edu
root@01salvus:~# ssh-copy-id 04salvus.math.washington.edu

Making the /etc/hosts file contain:

128.95.224.230 01salvus 01salvus.math.washington.edu
128.95.224.232 02salvus 02salvus.math.washington.edu
128.95.224.237 03salvus 03salvus.math.washington.edu
128.95.224.240 04salvus 04salvus.math.washington.edu


On salvus01 and salvus02:
-------------------------

Making /etc/hostname file contain:  
   02salvus.math.washington.edu

Then type "service hostname restart"

2. Create an LVM logical volume to use for Rados:

  lvcreate --name ganeti_test --size 20G lvm
  lvdisplay
  lvscan

Do the same on 02salvus:

  lvcreate --name ganeti_test --size 20G 02salvus   # i named the lvm differently
  
3. Create ceph config file:

/var/lib/ceph/ceph0.conf

[mon]
    mon data = /var/lib/ceph/mon
[mon.1]
   host = 01salvus
   mon addr = 128.95.224.230:6789
[mon.2]
   host = 02salvus
   mon addr = 128.95.224.232:6789

[mds]
  mds data = /var/lib/ceph/mds
[mds.1]
  host = 01salvus
[mds.2]
  host = 02salvus

[osd]
    osd data = /var/lib/ceph/osd
    osd journal = /var/lib/ceph/osd/journal
    osd journal size=500; journal size, in megabytes
[osd.1]
    host = 01salvus
    btrfs devs = /dev/lvm/ganeti_test
[osd.2]
    host = 02salvus
    btrfs devs = /dev/02salvus/ganeti_test

4. Push out config and make filesystem:

   mkcephfs -a -c /var/lib/ceph/ceph0.conf --mkbtrfs 

5. Test it out:
   # on each of 01salvus and 02salvus:

   /etc/init.d/ceph start
   mkdir /mnt/ceph; ceph-fuse -m `hostname` /mnt/ceph

   
root@01salvus:/var/lib/ceph# df -h |grep ceph
/dev/mapper/lvm-ganeti_test   20G  504M   18G   3% /var/lib/ceph/osd
ceph-fuse                     40G  5.1G   35G  13% /mnt/ceph
   
---------

Success.  Now moving on to ganeti. 

GANETI test install:

1. Prereq:
  # apt-get install kvm python-paramiko  iputils-arping ndisc6 python-pyparsing  python-pyinotify python-pycurl socat qemu ganeti2 ganeti-htools ganeti-instance-debootstrap

   root@02salvus:/mnt/ceph# kvm --version
   QEMU emulator version 1.0 (qemu-kvm-1.0), Copyright (c) 2003-2008 Fabrice Bellard

2. Configuration:

   - fix the /etc/hostname file to the fqdn, etc.

   - I'm not doing any network config.  It seems that Ubuntu + kvm
     already does that and creates virbr0.

   - I'm going to start by trying the older ganeti-2.4.5 that comes with
     Ubuntu.  If that fails, I'll try ganeti-2.6.0 from source, which is here:
          http://code.google.com/p/ganeti/downloads/list
          wget http://ganeti.googlecode.com/files/ganeti-2.6.0.tar.gz

   - FAIL: it turns out that RADOS support is new; it's not in ubuntu's ganeti.
     So, uninstall that and build from source.

apt-get remove ganeti2 ganeti-htools
wget http://ganeti.googlecode.com/files/ganeti-2.6.0.tar.gz
cd ganeti-2.6.0/
./configure --localstatedir=/var --sysconfdir=/etc
make -j20
make install

Logout, then login, then:

root@01salvus:~# gnt-cluster  --version
gnt-cluster (ganeti v2.6.0) 2.6.0



3. Initialization:

Lots of crap, but see the line at the very, very end of this step.

   - I uncommented a line in /etc/default/ganeti-instance-debootstrap 
       EXTRA_PKGS="acpi-support-base,console-tools,udev,linux-image-amd64"

   - CLUSTERNAME:   Huh?  
      I added *this* to /etc/hosts on 01salvus and 02salvus, 
      and made the CLUSTERNAME "ganeti".

# this ip is something I don't have allocated to me, but it is in my range, so 
# I'm "taking" it.  

128.208.160.190 ganeti

root@01salvus:/etc/default# gnt-cluster init ganeti

Oops:
Failure: prerequisites not met for this operation:
error type: wrong_input, error details:
Error: volume group 'xenvg' missing
specify --no-lvm-storage if you are not using lvm

gnt-cluster init --enabled-hypervisors=kvm --vg-name=lvm --no-drbd-storage --master-netdev=virbr0 ganeti


gnt-cluster destroy --yes-do-it
# fail, so

rm -rf /var/lib/ganeti/; mkdir /var/lib/ganeti


   gnt-cluster init --enabled-hypervisors=kvm  --no-drbd-storage --no-lvm-storage --master-netdev=virbr0 --ipolicy-disk-templates=rbd ganeti 

<ARGH>

No dice.  This Ganeti is seeming really frustrating.  Given how broken the demo
in the video is, etc., I'm not enthuised.  Also, given that I will at some point
want a hybrid cloud with Amazon, I'm even less enthuised.

The main other options are:

  * CloudStack.
  * OpenNebula
  * Eucalyptus

CloudStack does work with RADOS/Ceph:

   http://ceph.com/docs/master/rbd/rbd-cloudstack/
   "You can use RBD to run instances on in Apache CloudStack.
   This can be done by adding a RBD pool as Primary Storage."
   Then many caveats.
</ARGH>

---------------

Back to trying ganeti...  I had hit this:

  http://theengguy.blogspot.com/2012/06/ganeti-not-working-in-ubuntu-1204.html

as I found looking in:

   /var/log/ganeti/commands.log 

I did this as they recommend:

###########################################
aptitude install build-essential dpkg-dev
apt-get source python-pycurl
aptitude build-dep python-pycurl
aptitude install libcurl4-openssl-dev
cd pycurl-7.19.0
perl -p -i -e "s/gnutls/openssl/" debian/control
dpkg-buildpackage -rfakeroot -b
cd ..
dpkg -i python-pycurl_7.19.0-4ubuntu3_amd64.deb
###########################################


Now I hit:

     "Error 60: SSL certificate problem"

I note that I completely misunderstood the "--master-netdev" parameter, and this could be the problem.

   gnt-cluster init --master-netdev=eth0 --enabled-hypervisors=kvm  --no-drbd-storage --no-lvm-storage  --ipolicy-disk-templates=rbd ganeti 


-----------

I'm going to try my own python install from scratch in /root, since this is just 
messed up.

 cd 
 mkdir local
 wget http://python.org/ftp/python/2.7.3/Python-2.7.3.tar.bz2
 tar xf Python-2.7.3.tar.bz2 
 cd Python-2.7.3/ 
 ./configure --prefix=/root/local
 make install
 
# Modify /root/.bash_profile so /root/local/bin is in front of path
# and logout/login/confirm.

   export PATH=$HOME/local/bin:$HOME/local/sbin:$PATH

# Install easy_install:

 wget http://pypi.python.org/packages/source/s/setuptools/setuptools-0.6c11.tar.gz
 tar xf setuptools-0.6c11.tar.gz 
 cd setuptools-0.6c11/   
 python setup.py install

# Now install deps:

 easy_install pyopenssl paramiko pycurl pyparsing pyinotify  simplejson

# Install ganeti into this Python

wget http://ganeti.googlecode.com/files/ganeti-2.6.0.tar.gz
tar xf ganeti-2.6.0.tar.gz
cd ganeti-2.6.0/
./configure --localstatedir=/var --sysconfdir=/etc --prefix=/root/local
make
make install


gnt-cluster init --master-netdev=virbr0 --enabled-hypervisors=kvm  --no-drbd-storage --no-lvm-storage  --ipolicy-disk-templates=rbd ganeti 

To get it to actually use my new python, I have to install my replace command and do:

root@01salvus:~/local/sbin# replace /usr/bin/python "/usr/bin/env python" *

# same in /root/local/lib/ganeti

... and we have exactly the same problem still.

Report of the same symptom:

  https://groups.google.com/forum/?fromgroups=#!topic/ganeti/P7hWx_GLAbY

And... indeed, that was the problem!!!
The problem is that there is an old ganeti process running from the last attempt.
Killing that, cleaning everything up, etc., and everything is fine.

I ran using the system-wide version, and all is good, so I will not be
using the version in /root/.

Hmm.  The master device really is on the web and using "128.208.160.190".

THIS WORKED:

gnt-cluster init --master-netdev=virbr0 --enabled-hypervisors=kvm  --no-drbd-storage --no-lvm-storage  --ipolicy-disk-templates=rbd ganeti 

--
4. Join other node to cluster:

 gnt-node add 02salvus.math.washington.edu

I had *problems* with this until I removed the 02salvus name from the /etc/hosts file!
I.e., I *had* to remove the short version!!! Crazy.

root@01salvus:/usr/local/lib/python2.7/dist-packages/ganeti# gnt-node list
Node                         DTotal DFree MTotal MNode MFree Pinst Sinst
01salvus.math.washington.edu      -     -  62.9G  1.0G 61.7G     0     0
02salvus.math.washington.edu      -     -  62.9G  1.0G 61.7G     0     0

--

5. Actually use it?

root@01salvus:/usr/local/lib/python2.7/dist-packages/ganeti# gnt-os list
Can't get the OS list

This is because I didn't do this:
 
  http://docs.ganeti.org/ganeti/current/html/install.html#installing-the-operating-system-support-packages

which is explained wrong!!!!!  What the FUCK.

./configure --prefix=/usr --localstatedir=/var --sysconfdir=/etc --with-os-dir=/srv/ganeti/os
make && make install

This must be done on all nodes.

 cd /tmp
 wget http://ganeti.googlecode.com/files/ganeti-instance-debootstrap-0.9.tar.gz
 tar xzf ganeti-instance-debootstrap-0.9.tar.gz
 cd ganeti-instance-debootstrap-0.9
 ./configure --prefix=/usr --localstatedir=/var --sysconfdir=/etc --with-os-dir=/srv/ganeti/os
  make && make install

---

Now it actually works to do "gnt-os list". !!


Add this to /etc/hosts:
   192.168.122.10 test1

Now try to create an instance:
   gnt-instance add -n 01salvus:02salvus -o debootstrap -t rbd -s 10G test1

This fails with:

  Hypervisor parameter validation failed on node 01salvus.math.washington.edu: Parameter 'kernel_path' fails validation: not found or not a file (current value: '/boot/vmlinuz-3-kvmU')

So we do this (as suggested at http://geekfun.com/2009/07/14/tips-on-using-ganeti-to-manage-a-kvm-based-virtual-machine-cluster-on-ubunty-jaunty-jackelope-9-04/):

  cd /boot
  ln -s vmlinuz-3.2.0-29-generic vmlinuz-3-kvmU

Next error on trying again with the same input:

  error type: wrong_input, error details:
  OS name must include a variant

# from https://groups.google.com/forum/?fromgroups=#!topic/ganeti/YVk1MBJzR-E


   gnt-instance add -n 01salvus -o debootstrap+default -t rbd -s 4G test1

Next error -- checking bridges on destination node '01salvus.math.washington.edu': Missing bridges xen-br0 

   gnt-instance add -n 01salvus -o debootstrap+default --net 0:link=virbr0 -t rbd -s 4G test1

# the docs say "bridge=", but they are wrong.  

Fails due to lack of /sys/bus/rbd/devices, so I do

    modprobe rbd

and try again:

It got further, but failed with:

This may result in very poor performance, (re)-partitioning suggested.
I: Retrieving Release
E: Failed getting release file http://ftp.us.debian.org/debian/dists/lenny/Release,

which indeed does not exist.
...


The debootstrap instance is too old, so try with a newer one, on ALL NODES:

 cd /tmp
 wget http://ganeti.googlecode.com/files/ganeti-instance-debootstrap-0.11.tar.gz
 tar xzf ganeti-instance-debootstrap-0.11.tar.gz
 cd ganeti-instance-debootstrap-0.11
 ./configure --prefix=/usr --localstatedir=/var --sysconfdir=/etc --with-os-dir=/srv/ganeti/os
  make && make install


Try again:

  gnt-instance remove test1
  gnt-instance add -n 01salvus -o debootstrap+default --net 0:link=virbr0 -t rbd -s 4G test1

This file shows the install is working:

  root@01salvus:/var/log/ganeti/os# tail -f  add-debootstrap+default-test1-2012-09-19_17_41_42.log


After installing (about 20 minutes?), the instance fails to start:


  Failure: command execution error:
  Could not start instance: Error while executing backend function: 'pool'

This is because it's completely broken for everybody.  Gees.

  https://groups.google.com/forum/?fromgroups=#!topic/ganeti/tV4MVBJyS-Q

I tried applying the one-line patch there and no change...

   gnt-instance startup test1

I'm going to try the latest git repo of ganeti...

If that doesn't work, it's time to go for DRBD, which is clearly much
more well supported.

   git clone git://git.ganeti.org/ganeti.git
   apt-get install autogen automake pandoc python-docutils python-sphinx graphviz
   ./autogen.sh
   ./configure --localstatedir=/var --sysconfdir=/etc
   make && make install

This did not work.  Hmm.

----

What to do.  

I should try a local simple storage first before even trying drbd.

Try again with plain with a 1-node cluster:

  gnt-instance remove test1

gnt-node remove 02salvus.math.washington.edu
gnt-cluster destroy --yes-do-it
gnt-cluster init --master-netdev=virbr0 --enabled-hypervisors=kvm  --no-drbd-storage --vg-name=lvm ganeti 

  gnt-instance add -n 01salvus -o debootstrap+default --net 0:link=virbr0 -t plain -s 4G test1

This did fully work.

I tried the Ganeti Web Manager and it really sucks. 
I'm officially rejecting ganeti as a possible component of salvus.  Oh well. 

Options:
  * list here: http://www.linux-kvm.org/page/Management_Tools
  * CloudStack
  * OpenNebula
  * Eucalyptus
  * OpenStack


* the openstack distributed storage: http://www.openstack.org/software/openstack-storage/

Cloudstack on ubuntu 12.04:
  http://www.cloudstack.org/forum/6-general-discussion/11158-302-on-ubuntu-1204.html

---------------

OpenNebula:

Looks like the only big-usage academic/truly free cloud option. 

Packages:

  http://dev.opennebula.org/packages/opennebula-3.6.0/Ubuntu-12.04/

wget http://dev.opennebula.org/packages/opennebula-3.6.0/Ubuntu-12.04/Ubuntu-12.04-opennebula_3.6.0-1_amd64.deb
dpkg -i Ubuntu-12.04-opennebula_3.6.0-1_amd64.deb
apt-get install -f -y
----

I'm trying out OpenNebula via a VM manager as explained here: http://opennebula.org/cloud:sandbox:kvm

  ./installer.sh frontend 128.208.160.190

Domain frontend created from /root/frontend.xml

Appliance started, called frontend. The IP is 128.208.160.190

A copy of the frontend VM description file is located at
~/frontend.xml. You can use it to start the frontend again executing:
 $ virsh create ~/frontend.xml


ICK.
--

CloudStack... doesn't seem to support Ubuntu 12.04 and seems weird.

===================

Eucalyptus: looks interesting. Try it.

Following http://www.eucalyptus.com/download/eucalyptus/ubuntu-12
  

  wget http://www.eucalyptus.com/sites/all/modules/pubdlcnt/pubdlcnt.php?file=/sites/all/files/c1240596-eucalyptus-release-key.pub&nid=878
  apt-key add *eucalyptus-release-key.pub

  echo "deb http://downloads.eucalyptus.com/software/eucalyptus/3.1/ubuntu precise main" > /etc/apt/sources.list.d/eucalyptus.list 

  echo "deb http://downloads.eucalyptus.com/software/euca2ools/2.1/ubuntu precise main" > /etc/apt/sources.list.d/euca2ools.list

  apt-get update

  apt-get install eucalyptus-cloud eucalyptus-cc eucalyptus-sc eucalyptus-walrus 
  apt-get install  eucalyptus-nc


  /usr/sbin/euca_conf --initialize

￼service eucalyptus-cloud start

 service eucalyptus-cc start
￼service eucalyptus-nc start

root@01salvus:~# netstat -a |grep 8773
tcp        0      0 *:8773                  *:*                     LISTEN     
udp        0      0 228.7.7.3:8773          *:*                                
udp        0      0 01salvus.math.wash:8773 *:*                                
root@01salvus:~# netstat -a |grep 8774
tcp        0      0 *:8774                  *:*                     LISTEN     
root@01salvus:~# netstat -a |grep 8775
tcp        0      0 *:8775                  *:*                     LISTEN     

Registering COMPONENTS:

  /usr/sbin/euca_conf --register-walrus --partition walrus --host 01salvus --component walrus-01salvus

￼/usr/sbin/euca_conf --register-cluster --partition cluster01 --host 01salvus --component cc-01salvus

  /usr/sbin/euca_conf --register-sc --partition cluster01 --host 01salvus --component sc-01salvus

￼/usr/sbin/euca_conf --register-nodes "01salvus"

CREDENTIALS:

  /usr/sbin/euca_conf --get-credentials admin.zip
  unzip admin.zip
  source eucarc

WEB INTERFACE:

  ssh -L 8443:localhost:8443 salvus@01salvus

Then visit:
  
  https://localhost:8443/

Login and password are *both* "admin".


Eucalyptus seems to have thought through security much more clearly
than the other systems.  Bravo!!

The admin interface mostly works, though search is broken.

   root@01salvus:~/eucalyptus# eustore-describe-images
   1107385945 centos      x86_64  2012.07.05     CentOS 5 1.3GB root, Hypervisor-Specific Kernels
   root@01salvus:~/eucalyptus# eustore-install-image -i 1107385945  -b walrus -k "Hypervisor-Specific Kernels"

That actually does download it.  But fails because kernel specified wrong...

I have no idea how to specify k... and yet my second guess works:


   root@01salvus:~/eucalyptus# eustore-install-image -i 1107385945  -b walrus -k kvm

--------------------------------------

My god, this shit is just so heavy.   What about plain libvirt?

apt-get install virtinst virt-viewer

virt-install -n test1 -r 512 -f ./test1.img -s 5 -c ./ubuntu-12.04-server-amd64.iso -w network:virbr0 -m 54:52:00:00:10:01 -v --os-type=linux --noautoconsole

FAIL.

As root:

virt-install -n test1 -r 512 -f ./test1.img -s 5 -c ./ubuntu-12.04-server-amd64.iso -m 54:52:00:00:10:01 -v --os-type=linux --noautoconsole


Hmm, learn about http://virt-tools.org/

This article looks good:  http://www.sysadminworld.com/2012/ubuntu-12-04-kvm-virtualization/

chmod a+rw ubuntu-12.04-server-amd64.iso

virt-install -v --graphics vnc,port=8123 --name test2 --ram 1024 --disk path=/tmp/test2.img,size=4  -c ubuntu-12.04-server-amd64.iso 


Try as normal user (salvus) and fail.
Found this page: http://forums.gentoo.org/viewtopic-t-925976-start-0.html
Says to do:

  gpasswd -a qemu kvm

ie add "qemu" to kvm group, due to privilage issues.
That does not help.

What did fix the problem is doing:

root@01salvus:/dev# ls -lh kvm
crw-rw---- 1 root kvm 10, 232 Sep 19 21:57 kvm
root@01salvus:/dev# chmod a+rw kvm
root@01salvus:/dev# ls -lh kvm
crw-rw-rw- 1 root kvm 10, 232 Sep 19 21:57 kvm

which is pretty dangerous/evil, at least if this were a big multi-user system, which it isn't.


#  apt-get remove eucalyptus-cloud eucalyptus-cc eucalyptus-sc eucalyptus-walrus  eucalyptus-nc



---

Have to spoof mac for wifi router to let me back!

sh-3.2# ifconfig en0
en0: flags=8863<UP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST> mtu 1500
	ether 10:40:f3:91:6b:26 
	inet6 fe80::1240:f3ff:fe91:6b26%en0 prefixlen 64 scopeid 0x4 
	inet 169.254.76.216 netmask 0xffff0000 broadcast 169.254.255.255
	media: autoselect
	status: active



...

OK, after a very long day of this, I've learned one thing:

   USE libvert directly!

There is no significant value added for me in using any of the private cloud systems. 

By using libvirt I can even using Ceph/RDB if I want and it is fast enough...

Now I need to properly learn kvm and libvirt.

---------------

Sept 20, 2012:
-------------

virt-install + rados block device?

Trying stuff from here:  http://blog.bob.sh/2012/02/basic-ceph-storage-kvm-virtualisation.html

  modprobe rbd
  rados mkpool vm_disks
  qemu-img create -f rbd rbd:vm_disks/test1 4G

# get around lack of functionality in virt-install:
root@01salvus:/tmp/virt# pwd
/tmp/virt
root@01salvus:/tmp/virt# more disk.xml
<disk type='network' device='disk'>
  <source protocol='rbd' name='vm_disks/test1'/>
  <target dev='vda' bus='virtio'/>
</disk>




Also, make sure to enable caching or it will be dog slow!
   See http://ceph.com/wiki/QEMU-RBD

Too frustrating with shitty internet!




Wait:

   http://www.osrg.net/sheepdog/

   "Sheepdog is a distributed storage system for QEMU/KVM. It provides
   highly available block level storage volumes that can be attached
   to QEMU/KVM virtual machines. Sheepdog scales to several hundreds
   nodes, and supports advanced volume management features such as
   snapshot, cloning, and thin provisioning."

   Heh, that's what I *need*!!!


Try it out:

   sudo su
   aptitude install corosync libcorosync-dev liburcu-dev
   git clone git://github.com/collie/sheepdog.git
   cd sheepdog
   ./autogen.sh
   ./configure 
   make
   make install

   lvcreate --name sheep_test --size 20G lvm
   lvscan
   mkfs.btrfs /dev/lvm/sheep_test
   mkdir /mnt/sheep
   mount /dev/lvm/sheep_test /mnt/sheep
   df -h
   sheep /mnt/sheep

It just doesn't work at all.  aRgh.

#

I will now plan out the functionality I want with VM's, exactly.
----------------------------------------------------------------

VM types:
=========
Salvus:
   * sage server -- small, but LOTS of ephemeral machines
   * web: tornado, etc. -- small; ephemeral
   * cassandra server -- lots of *disk* that must not be redundant or replicated
                      -- several on each node
                      -- might as well use raw lvm

Sage build machines:
   * not my problem

Sage web services:
   * sagemath.org, etc.  very stable

Sage compute services:
   * ?


For each salvus machine, at least, I need to:

    - decide on machine name and ip address ---> put in cassandra db
    - generating private and public keys --> publish public key (in cassandra db)
    - adding public keys to all tinc servers --> (tell them to update from db)
    - making sure the machine knows its private keys --> create a tiny img with the key and any other config info
    - decide on which node to start the machine --> put in cassandra db
    - copy/update any files to that machine's local storage --> paramiko
    - start machine --> paramiko

Also we will have to monitor machines:

    - for each machine listed in cassandra db:
        - ping it and record time (or timeout)
        - use paramiko to login and check on top status

Stopping machines:

    - change cassandra db 

-------------------

At the core, I need to write something like this:

class VirtualMachine(object):
    def __init__(self, host, ip_address, machine_type):
    def start(self):
    def stop(self):
    def restart(self):
    def status(self):

-----

I just looked at the openstack website again, and watched a video, and it looks
fantastic featurewise.  It's written mostly in python.  Maybe give it a try:

http://www.hastexo.com/resources/docs/installing-openstack-essex-20121-ubuntu-1204-precise-pangolin

No, it is soooo complicated!  And probably not mature.

---------



Sept 22, 2012:
==============

This is a session of playing with libvirt/kvm, just to get comfortable.  It's nice!

   [ ] setup a template libvirt VM for running other services
   [ ] setup a template libvirt ubuntu VM for running Sage 

Started these installs at 6:45am
 
# base salvus image
virt-install --os-variant=ubuntuprecise -v --graphics vnc,port=8122 --name salvus --vcpus=2  --ram 4096 --disk path=salvus.img,size=4 -c ubuntu-12.04-server-amd64.iso 

 virsh -c qemu:///session undefine salvus





# salvus_sage
virt-install -v --graphics vnc,port=8124 --name salvus_sage --vcpus=16 --ram 8192 --disk path=salvus_sage.img,size=12 -c ubuntu-12.04-server-amd64.iso 

ssh -L 8124:localhost:8124 salvus@01salvus.math.washington.edu

XML definitions of machines are here:

$HOME/.libvirt/qemu/

virsh -c qemu:///session start salvus_web
virsh -c qemu:///session start salvus_sage
virsh -c qemu:///session list

To ssh into a machine:

virsh -c qemu:///session qemu-monitor-command --hmp salvus_sage 'hostfwd_add ::2222-:22'   # NOT persistent?!
ssh localhost -p 2222

I'm going to try to install sage from source on there -- see how long it takes, etc. 

I'm also going to try a build test on geom -- I hope it works!


Sept 23, 2012
============

I tried benchmarking factorial(10^7) in a new virt-install'ed vm, and
it was 14 seconds, versus around 15 seconds not in the VM.

The other main issue is how incredibly slow the network is.  Maybe
that is easily fixed by re-installing the OS, since I seriously tested
a lot on here.  On geom -- using kvm -- the network is fine. 

Just curious if this was because of running using qemu but without kvm?

It's kvm.  Weird. 

My next guess is it is because limited hardware of the CPU is made available to the
machine, so optimizations aren't done.

I'll start a test of this.

In the vm, we have:

salvus@salvus-sage:~$ cat /proc/cpuinfo |grep flags|tail -1
flags		: fpu de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 syscall nx lm nopl pni cx16 popcnt hypervisor lahf_lm svm abm sse4a

On the host we have:
salvus@01salvus:~$ cat /proc/cpuinfo |grep flags|tail -1
flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc extd_apicid aperfmperf pni pclmulqdq monitor ssse3 cx16 sse4_1 sse4_2 popcnt aes xsave avx lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 nodeid_msr topoext perfctr_core arat cpb npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold


The flags are restricted for live migration.   But we will only migrate between opterons, so...


virsh capabilities

<capabilities>

  <host>
    <uuid>44454c4c-4c00-1047-8030-b1c04f395631</uuid>
    <cpu>
      <arch>x86_64</arch>
      <model>Opteron_G3</model>
      <vendor>AMD</vendor>
      <topology sockets='1' cores='4' threads='2'/>
      <feature name='wdt'/>
      <feature name='skinit'/>
      <feature name='osvw'/>
      <feature name='3dnowprefetch'/>
      <feature name='cr8legacy'/>
      <feature name='extapic'/>
      <feature name='cmp_legacy'/>
      <feature name='pdpe1gb'/>
      <feature name='fxsr_opt'/>
      <feature name='mmxext'/>
      <feature name='aes'/>
      <feature name='sse4.2'/>
      <feature name='sse4.1'/>
      <feature name='ssse3'/>
      <feature name='ht'/>
      <feature name='vme'/>
    </cpu>

Putting this in the machine's XML had no effect at all.


From the man virt-install page:

 --cpu host
           Expose the host CPUs configuration to the guest. This enables the guest to take advantage of many of the host
           CPUs features (better performance), but may cause issues if migrating the guest to a host without an identical
           CPU.

I'll try a new install with "--cpu host" and see what happens.   I'm going to do this on 03salvus too, to see if the networking issues
is resolved.

virt-install -v --cpu host --graphics vnc,port=8124 --name salvus_sage --vcpus=16 --ram 8192 --disk path=salvus_sage.img,size=12 -c ubuntu-12.04-server-amd64.iso 

NOTE: The physical network on all the salvus nodes is way too slow. It's 10MB/s, but should be 100?!
The card is: Broadcom NetXtreme II BCM5716 1000Base-T
Not a problem with combinat using *NOT* the same card.

Exactly this problem: http://www.linuxquestions.org/questions/linux-networking-3/broadcom-nic-slow-854656/

Compiling the driver on 03salvus.

Neat trick here:  http://ubuntuforums.org/showthread.php?t=1870780&page=2

   apt-get install apt-file
   apt-file search linkage.h

But we need:

  apt-get install linux-source

  Reboot!


Trying to build driver, but still hitting the error:

  ln -s /usr/src/linux-headers-3.2.0-31/include/asm-generic /usr/src/linux-headers-3.2.0-31/include/asm
  rm /usr/src/linux-headers-3.2.0-31/include/asm

This got nowhere:
  ln -s /usr/src/linux-headers-3.2.0-31-generic/arch/x86/include/asm /usr/include/asm

---

According to http://manpages.ubuntu.com/manpages/lucid/man4/bce.4freebsd.html
"1000baseSX   Sets 1000Mbps operation.  Only full-duplex mode is supported
                  at this speed."

So the question is how to use the builtin driver, but set to the above speed. 

Try:

  ifconfig eth0 media 1000baseSX

  apt-get install ethtool

  ethtool eth0

   root@03salvus:/etc# /sbin/mii-tool 
   eth0: negotiated 100baseTx-FD, link ok
  ...


 ifconfig eth0 media 1000baseTx

What I want: [   31.797673] bnx2 0000:01:00.0: eth0: NIC Copper Link is Up, 1000 Mbps full duplex
root@combinat:/home/wstein# /sbin/mii-tool 
eth0: negotiated 1000baseT-FD flow-control, link ok


What I got:  [   25.759307] bnx2 0000:01:00.0: eth0: NIC Copper Link is Up, 100 Mbps full duplex
root@03salvus:/etc# /sbin/mii-tool 
eth0: negotiated 100baseTx-FD, link ok

----

I think it is the CABLE!  That's what this page says:  http://ubuntuforums.org/archive/index.php/t-1283713.html

Moreover, 02salvus and 04salvus do connect fine at gigabit. 

Solution: swap out the cables on 01salvus and 03salvus!!!!


---

I should test virtualbox on ...


In the meantime, on 01salvus, I'll try this:

   virt-install -v --cpu host --graphics vnc,port=8125 --name salvus_sage_cpuhost --vcpus=16 --ram 8192 --disk sparse=true,path=salvus_sage_cpuhost.img,size=14 -c ubuntu-12.04-server-amd64.iso 

   ssh -L 8125:localhost:8125 salvus@01salvus.math.washington.edu

   virsh -c qemu:///session start salvus_sage_cpuhost

   virsh -c qemu:///session qemu-monitor-command --hmp salvus_sage_cpuhost 'hostfwd_add ::2223-:22'   # NOT persistent?!

   scp -P 2223 sage-5.3.tar localhost:

-----------

So finally, I'll do this whole test on 02salvus, which has a solid network, and is pretty clean.


   scp 01salvus:.screenrc .
   virt-install -v --cpu host --graphics vnc,port=8125 --name salvus_sage_cpuhost --vcpus=16 --ram 8192 --disk path=salvus_sage_cpuhost.img,size=14 -c ubuntu-12.04-server-amd64.iso    
   ssh -L 8125:localhost:8125 salvus@02salvus.math.washington.edu

   virsh -c qemu:///session start salvus_sage_cpuhost
   virsh -c qemu:///session qemu-monitor-command --hmp salvus_sage_cpuhost 'hostfwd_add ::2222-:22'   
   ssh localhost -p 2222
   wget http://sage.math.washington.edu/home/release/sage-5.3/sage-5.3.tar
   sudo apt-get install g++ gfortran m4 make dpkg-dev libssl-dev
   sleep 6000; export MAKE="make -j16"; tar xvf sage-5.3.tar; cd sage-5.3; make
   

I solved the networking slowdown issue!
(Thanks to https://help.ubuntu.com/community/KVM/Networking)

    <interface type='user'>
      <mac address='52:54:00:be:75:c2'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
      <model type='virtio'/>  <!-- ADD THIS LINE to ~/.libvirt/qemu/salvus_sage_cpuhost.xml --> 
    </interface>

This should be something I can do via the virt-install line with:

   -net nic,model=virtio -net user

so, this should be it:

   virt-install -v --cpu host -net nic,model=virtio -net user --graphics vnc,port=8125 --name salvus_sage_cpuhost --vcpus=16 --ram 8192 --disk path=salvus_sage_cpuhost.img,size=14 -c ubuntu-12.04-server-amd64.iso    

---

Sept 24, 2012
=============

I had no real time to work today at all, because of teaching, shoping,
the vert ramp paperwork and banking stuff, etc.  Until right now.  I
did get to fix the network and do a clean re-install of 01salvus.

Great discussion of the qcow2 image format: http://people.gnome.org/~markmc/qcow-image-format.html

I'm going to take another stab at making template virtual machines for:

 (1) sage, (2) web, (3) cassandra

Step 1: make a base template


   virt-install -v --cpu host --network user,model=virtio --graphics vnc,port=8125 --name salvus_base --vcpus=2 --ram 4096 --disk path=salvus_base.img,size=8 -c ubuntu-12.04-server-amd64.iso
 
   ssh -L 8125:localhost:8125 salvus@01salvus.math.washington.edu

Setup LVM with one big root partition and no swap (swap will be on another image).

   virsh -c qemu:///session start salvus_base
   virsh -c qemu:///session qemu-monitor-command --hmp salvus_base 'hostfwd_add ::2222-:22'   

Sept 25, 2012
=============

  [ ] setup 01salvus, 02salvus, 03salvus, 04salvus to each be tinc VPN servers
      * for that, should install salvus on each, which requires a bootstrap
            git clone https://github.com/williamstein/salvus.git   # and type login: williamstein, password: ....
      * I fixed the /etc/hosts files back to stock, plus put in shortcuts for the cluster.
      * on each node:  git clone 01salvus:salvus/
      * fixed build system
            git push https://github.com/williamstein/salvus.git

* woops:
    WARNING: IPython History requires SQLite, your history will not be saved
     WARNING: Readline services not available or not loaded.WARNING: The auto-indent feature requires the readline libraryPython 2.7

Fix:
    . salvus-env
    sudo apt-get install libncurses-dev
    easy_install readline
Ick:
    WARNING: IPython History requires SQLite, your history will not be saved

Fix:
    . salvus-env
    sudo apt-get install libsqlite3-dev
    ./build.py --build_python

sudo apt-get install libncurses-dev libsqlite-dev
cd salvus/salvus; . salvus-env; easy_install readline ; ./build.py --build_python

Finally:

   >>> import admin
   >>> admin.tinc_conf('01salvus', ['combinat0', 'bsd0', 'servedby1'])
.... argh

First... should I run "repair" on a cassandra node? I haven't in week(s).  Here's how:
1. Look in conf/deploy/services for the cassandra nodes.
2. ssh salvus@bsd1.salv.us "cd salvus/salvus/; . salvus-env; nodetool repair"

  [ ] working with the vpn is too difficult right now, because list of
      hosts isn't all stored in a database.  fix this.

Options:

  - right now the vpn info *is* in a database, namely the git repo.
    But pushing and pulling to/from that every time I modify the vpn
    seem bad. 

  - use Cassandra to store the vpn info:
       - advantage: no single point of failure
       - disadvantage: only a few nodes can connect to cassandra, so we'll have to 
         run another service just to deal with this. 
       - service is called tinc_directory.py
       - the *only* machines that need access to the directory database are the
         virtual machine hosts, since they will be configuring the tinc directory
         when they provision a machine.
       - actually, this service could just be rolled into the tornado
         server itself, since it can talk to cassandra, and any host
         on the vpn can talk to the tornado server.
       - bootstrap?!  there is a catch 22, since when no vm's are
         started, obviously nothing can work!  How about the program
         that starts up a node reconfigures tinc with the latest host
         files, etc., as soon as it can contact a tornado node and
         that node works... but until then it uses the last
         potentially stale default hosts info.
       - problem: if I do this via tornado, I have to somehow deal with authentication, 
         since it is necessary to write to the database when provisioning new nodes.
         i don't have to worry about ssl, since all traffic is encrypted.
  
  - use a sqlite file on the "master control node" --> SPOF
  
  - put a postgresql server back into salvus just for this --> SPOF

  - get better at just using the git repo for this (or is this is a
    non-option if we start provisioning hundreds of machines?)

  - use control node filesystem:
      - admin can only be done from vm host nodes

  - for automatically provisioning new vm's there is already a SPOF, namely the
    machine on which the vm is running.  So we really only need to put the public
    key for the vm in the hosts/ directory for *that* one machine.  There is no
    point in putting it anywhere else.   This could be dynamic: when we start a VM,
    we generate the keys and put them in the hosts/ directory; when we stop the VM,
    we remove them.  That's it.       Since we have far fewer VM hosts, we just continue
    as is with them.   This would all be pretty easy.

Let's get 01salvus on our VPN:

salvus@01salvus:~$ cd salvus/salvus/
salvus@01salvus:~/salvus/salvus$ . salvus-env
salvus@01salvus:~/salvus/salvus$ ipython

   >>> import admin; admin.tinc_conf('01salvus', ['combinat0', 'bsd0'])

sudo apt-get install liblzo2-dev

NOTE: VPN thoughts for later...: According to
http://www.tinc-vpn.org/pipermail/tinc/2010-March/002276.html the
algorithm to determine path is very naive -- one of the shortest
paths.  However, in tinc 1.1 (which I would have to upgrade to) they
use a much better algorithm.  To build 1.1 on OSX, do this first
export CFLAGS="-fnested-functions"

TODO: should add "servedby1" to list of servers above, but then have
to change the firewall rules for that machine to allow connections
from salvus nodes.

For other nodes:

       cd salvus/salvus/; . salvus-env; git pull 01salvus:salvus/; sudo apt-get install liblzo2-dev; ./build.py --build_tinc; ipython
       import admin, os; admin.tinc_conf(os.popen('hostname').read().strip(), ['combinat0', 'bsd0'])

       git add .; git commit -a -v

Back on 01salvus:

git pull 02salvus:salvus/
 
  [x] just installed updates on bsd.math and restarted those virtual
      machines.  some of the combinat vm's were mysteriously crashed!

  [ ] test setup salvus-base to be on vpn but not public.

  [ ] test out copy on write with my template vm above:

qemu-img create -f qcow2 -b winxp.img test01.img

  [x] *** System restart required ***  -- wonder about -- 
Answer: http://askubuntu.com/questions/28530/how-can-i-tell-what-package-requires-a-reboot-of-my-system
salvus@01salvus:~$ more  /var/run/reboot-required.pkgs
linux-image-3.2.0-31-generic
linux-base
dbus

  [ ] 01salvus needs -- apt-get update; apt-get upgrade 


-------

Sept 26, 2012

I'm still learning how to make template base virtual machines, etc.

This looks relevant:

  http://www.linux-kvm.com/content/how-you-can-use-qemukvm-base-images-be-more-productive-part-1

$ qemu-img create -b  salvus_base.img -f  qcow2    salvus_base-clone.img
$ ls -lh salvu*.img
-rw-r--r-- 1 salvus salvus 193K Sep 26 20:20 salvus_base-clone.img
-rwxrwxr-x 1 salvus salvus 8.0G Sep 26 20:19 salvus_base.img


$ virt-install -v --cpu host --network user,model=virtio --graphics vnc,port=8126 --name salvus_base_clone --vcpus=2 --ram 4096 --disk path=salvus_base-clone.img  --boot hd

FAIL

$ virsh --connect qemu:///session destroy salvus_base_clone
$ virsh --connect qemu:///session undefine salvus_base_clone

I want to start a "transient" domain.

"Rebasing base images" (i.e., merging):

  http://www.linux-kvm.com/content/be-more-productive-base-images-part-2

  qemu-img convert windows-clone.qcow2 –O qcow2 windows-marketing.qcow2



$ virt-install -v --cpu host --network user,model=virtio --graphics vnc,port=8126 --name salvus_base_clone --vcpus=2 --ram 4096 --import --disk salvus_base-clone.img --noautoconsole

FAIL

Test the image


# apt-get install guestmount
# add salvus user to fuse group !!!
mkdir x
guestmount -a salvus_base-clone.img --rw x -i    


From a random place I found this:

virt-install --vcpus $CPUS \
  --ram $MEM --import \
  --name $TARGET_NAME \
  --disk $TARGET_IMG,device=disk,bus=virtio,format=qcow2 $DISK_SWAP \
  --vnc --noautoconsole --force \
  --network=$NETWORK,mac=$MAC \
  $NETWORK2

#And made this, which worked!!

$ virt-install --cpu host --network user,model=virtio --graphics vnc,port=8126 --name salvus_base_clone --vcpus=2 --ram 4096 --import --disk salvus_base-clone.img,device=disk,bus=virtio,format=qcow2 --noautoconsole

# are macs of 2 machines made this way different?

qemu-img create -b  salvus_base.img -f  qcow2    salvus_base-clone1.img  # 0.070s
qemu-img create -b  salvus_base.img -f  qcow2    salvus_base-clone2.img  # 0.080s

virt-install --cpu host --network user,model=virtio --graphics vnc,port=8120 --name salvus_base_clone1 --vcpus=2 --ram 4096 --import --disk salvus_base-clone1.img,device=disk,bus=virtio,format=qcow2 --noautoconsole  # 5.829s

virt-install --cpu host --network user,model=virtio --graphics vnc,port=8121 --name salvus_base_clone2 --vcpus=2 --ram 4096 --import --disk salvus_base-clone2.img,device=disk,bus=virtio,format=qcow2 --noautoconsole  # 

  ssh -L 8120:localhost:8120 -L 8121:localhost:8121 salvus@01salvus

MACs are different.  Good.

Hmm. but the network sucks.  need to retry that parameter and fix this.

  virsh -c qemu:///session list
  virsh -c qemu:///session qemu-monitor-command --hmp salvus_base_clone2 'hostfwd_add ::2222-:22'   


I wonder about recompacting .img, but don't really need to worry about that...

Network performance testing:

  sudo apt-get install iperf

iperf -s # on server
iperf -c servername # on client

Hit control-c

From this we see that the connection 01salvus <--> virtual machine: is
very fast, but the additional NAT is very slow.  Maybe with tinc this
doesn't matter.

Through tinc, between combinat0 and 01salvus (physical machines), we get:

   [  3]  0.0-10.0 sec   270 MBytes   226 Mbits/sec

which is I guess OK, given that it is encrypted, etc. 

From combinat1 -- a *VirtualBox* VM -- to 01salvus over tinc:

   salvus@combinat1:~$ iperf -c 01salvus.salv.us
   ------------------------------------------------------------
   Client connecting to 01salvus.salv.us, TCP port 5001
   TCP window size: 16.4 KByte (default)
   ------------------------------------------------------------
   [  3] local 10.38.1.7 port 39425 connected with 10.38.1.14 port 5001
   [ ID] Interval       Transfer     Bandwidth
   [  3]  0.0-10.0 sec  43.6 MBytes  36.4 Mbits/sec
 
To combinat.math.washington.edu from combinat1:

   [  3]  0.0- 3.0 sec   187 MBytes   522 Mbits/sec

Over usual network:

salvus@combinat1:~$ iperf -t 3 -c 01salvus.math.washington.edu
------------------------------------------------------------
Client connecting to 01salvus.math.washington.edu, TCP port 5001
TCP window size: 23.5 KByte (default)
------------------------------------------------------------
[  3] local 10.0.2.15 port 38370 connected with 128.95.224.230 port 5001
[ ID] Interval       Transfer     Bandwidth
[  3]  0.0- 3.0 sec   168 MBytes   469 Mbits/secx	

So tinc is *killing* us here!

Change to *only* use combinat0 as tinc server.... and the result is just acceptable (though it varies):

salvus@combinat1:~$ iperf  -c 01salvus.math.washington.edu
------------------------------------------------------------
Client connecting to 01salvus.math.washington.edu, TCP port 5001
TCP window size: 23.5 KByte (default)
------------------------------------------------------------
[  3] local 10.0.2.15 port 38374 connected with 128.95.224.230 port 5001
[ ID] Interval       Transfer     Bandwidth
[  3]  0.0-10.1 sec   124 MBytes   104 Mbits/sec


I reniced to -19 the tinc process on combinat0 and I got:

[  3]  0.0-10.1 sec   342 MBytes   285 Mbits/sec

W00t

Not every time, but it is something.  Check this out:

salvus@combinat1:~$ iperf  -c 01salvus.math.washington.edu
------------------------------------------------------------
Client connecting to 01salvus.math.washington.edu, TCP port 5001
TCP window size: 23.5 KByte (default)
------------------------------------------------------------
[  3] local 10.0.2.15 port 38385 connected with 128.95.224.230 port 5001
[ ID] Interval       Transfer     Bandwidth
[  3]  0.0-10.0 sec   577 MBytes   484 Mbits/sec

Moral: always "nice -19" the tincd.

  nice --19 /home/salvus/salvus/salvus/data/local/sbin/tincd

I did that on 01salvus, combinat1, and combinat0, and now I get
consistently 340+ Mbits/sec, which is consistent with compression.  So
Virtualbox networking is just fine, and tincd is not evil.

Need to install salvus in the base image.

1. Delete the machines I made:

virsh --connect qemu:///session undefine salvus_base_clone1
virsh --connect qemu:///session undefine salvus_base_clone2

virsh --connect qemu:///session destroy salvus_base_clone2


virsh --connect qemu:///session list --all

2. Start base machine:

virsh --connect qemu:///session start salvus_base
virsh -c qemu:///session qemu-monitor-command --hmp salvus_base 'hostfwd_add ::2222-:22'   
ssh localhost -p 2222

3. Make it better:

sudo apt-get install git iperf
sudo apt-get autoremove

sudo apt-get install liblzo2-dev libssl-dev libreadline-dev  libsqlite3-dev libncurses5-dev emacs23


I just did 

   wget http://sage.math.washington.edu/home/release/sage-5.3/sage-5.3.tar

when logged in via "ssh localhost -p 2222" and that ssh network thing died, as did the whole
NETWORK!  But not the vm. Restarting the network on the host fixed the problem temporarily.
This is of course very worrisome. 

----

I'm putting the vm base on the vpn, since that's what really matters.

DONE.

I get this consistently:

salvus@salvus-base:~/salvus/salvus$ iperf -P 3 -t 5 -c combinat0.salv.us
------------------------------------------------------------
Client connecting to combinat0.salv.us, TCP port 5001
TCP window size: 22.9 KByte (default)
------------------------------------------------------------
[  3] local 10.38.1.254 port 33181 connected with 10.38.1.2 port 5001
[ ID] Interval       Transfer     Bandwidth
[  3]  0.0- 5.0 sec   121 MBytes   202 Mbits/sec

No hangs.

salvus@salvus-base:~/salvus/salvus$ iperf -c 01salvus.salv.us
------------------------------------------------------------
Client connecting to 01salvus.salv.us, TCP port 5001
TCP window size: 22.9 KByte (default)
------------------------------------------------------------
[  3] local 10.38.1.254 port 47411 connected with 10.38.1.14 port 5001
[ ID] Interval       Transfer     Bandwidth
[  3]  0.0-10.0 sec   256 MBytes   215 Mbits/sec

See tinc taking a full core in top:

 9163 root       1 -19 15116 1540 1168 R   98  0.0   1:13.61 tincd    

---

1. Now add more cores and RAM and build Sage on base VM.

  emacs /home/salvus/.libvirt/qemu/salvus_base.xml

and change vcpu from 2 to 16.
    change ram from 4194304 to 16777216

2. start:

    virsh --connect qemu:///session start salvus_base

3. Notice that it's on the vpn!

   mosh salvus@10.38.1.254  # works fine

4. Consistent scp speed from 01salvus to vm:     26.0MB/s

salvus@01salvus:~$ scp ubuntu-12.04-server-amd64.iso 10.38.1.254:
ubuntu-12.04-server-amd64.iso                                                                                         14%  101MB  26.0MB/s   00:22 ETA

5. start some other useful stuff installing:  
 
     sudo apt-get install texlive

6. start sage build:

    export MAKE="make -j8"; make ptestlong


 ssh -L 8125:localhost:8125  salvus@01salvus

----

Idea to fix network problem:

This is the problem, exactly.
   http://serverfault.com/questions/362038/qemu-kvm-virtual-machine-virtio-network-freeze-under-load

Will it be as good?

     "Preliminary tests suggest that I don't have the problem if I
     substitute e1000 for virtio in the first -net flag to qemu-kvm."

I'm going to try a speed test on this, by cloning the other img (live?!), then
making a new machine using that. 


   qemu-img convert salvus_base.img nettest.img


   virt-install -d --cpu host --network user,model=e1000 --graphics vnc,port=8122 --name nettest --vcpus=2 --ram 4096 --import --disk nettest.img,device=disk,bus=virtio,format=qcow2 --noautoconsole 

This fails with "Unable to read from monitor: Connection reset by peer", no matter what I do . Weird. 
I think I'll just wait and setup a copy-on-write image as before. 

  qemu-img create -b  salvus_base.img -f  qcow2    salvus_base-clone1.img

  virt-install --cpu host --network user,model=e1000 --graphics vnc,port=8120 --name salvus_base_clone1 --vcpus=2 --ram 4096 --import --disk salvus_base-clone1.img,device=disk,bus=virtio,format=qcow2 --noautoconsole  

Make another clone right now with virtio

  qemu-img create -b  salvus_base.img -f  qcow2    salvus_base-virtio.img

  virt-install --cpu host --network user,model=virtio --graphics vnc,port=8121 --name salvus_base_virtio --vcpus=2 --ram 4096 --import --disk salvus_base-virtio.img,device=disk,bus=virtio,format=qcow2 --noautoconsole 

It really is necessary to generate new keys, etc. 

THIS IS REALLY THE BUG:

  https://bugs.launchpad.net/ubuntu/+source/qemu-kvm/+bug/997978

IMPORTANT: **************************
Apply fix there:

   sudo apt-get install python-software-properties; sudo add-apt-repository ppa:ubuntu-virt/kvm-network-hang; sudo apt-get update; sudo apt-get install qemu-kvm

 virsh --connect qemu:///session start salvus_base_virtio

... and this seems to be working so far!


Problems left to solve before I can just code this:

[ ] How to mount an img as salvus user (not root)

     virsh -c qemu:///session list
     sudo apt-get install libguestfs-tools
     virt-filesystems -a salvus_base-virtio.img

Fix was easy - this is a slight security risk though (see http://bugs.debian.org/cgi-bin/bugreport.cgi?bug=615029).

     sudo chmod a+r /boot/vmlinuz-* # used by guestmount
     mkdir mnt; guestmount -i -a salvus_base-virtio.img --rw mnt
     ls mnt 
     fusermount -u mnt

[ ] get pid of kvm process

     virsh --connect qemu:///session start salvus_base_virtio

This yields 52227, so we have that to check with:

     ps ax |grep kvm|grep salvus_base_virtio

Consider using python bindings for some of this?

     apt-get install ipython

ipython

In [1]: import libvirt
In [2]: import sys
In [3]: conn = libvirt.openReadOnly(None)
In [4]: conn
Out[4]: <libvirt.virConnect instance at 0x208c128>
In [5]: dom0 = conn.lookupByName("salvus_base_virtio")
In [6]: dom0
Out[6]: <libvirt.virDomain instance at 0x1e8fb90>
In [7]: dom0.info()
Out[7]: [1, 4194304L, 4194304L, 2, 8270000000L]
In [12]: dom0.isActive()
Out[12]: 1
In [17]: %timeit dom0.isActive()
1000 loops, best of 3: 389 us per loop

So instead of watching for the pid, I can call isActive periodically.
This means that my vm.py script/daemon must use the system-wide python.
That should be OK.

Fix autoboot thing.


   sudo apt-get install iotop


This makes it so I don't have to prefix virsh with that --connect stuff:

   export VIRSH_DEFAULT_CONNECT_URI="qemu:///session"

Put in .bash_profile on all four machines...

Renaming from salvus_base to template?  Actually salvus_base is a good name.

virsh start salvus_base

root@salvus-base:/boot/grub# vi /etc/grub.d/00_header 
root@salvus-base:/boot/grub# update-grub2

OK, done.

Make a "backup" of my image:

   rsync -axvH images/ 02salvus:images/

--RESIZING MY ROOT DISK IMG-----------------------------
My / disk is almost full, which could be annoying and complicating for
now good reason down the line.  Let's deal with this now, following:

   http://serverfault.com/questions/324281/how-do-you-increase-a-kvm-guests-disk-space

First:

  time qemu-img resize images/salvus_base.img +8G

  virsh start salvus_base


root@salvus-base:/home/salvus# fdisk /dev/sda

Command (m for help): p

Disk /dev/sda: 17.2 GB, 17179869184 bytes
255 heads, 63 sectors/track, 2088 cylinders, total 33554432 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0x000d72a0

   Device Boot      Start         End      Blocks   Id  System
/dev/sda1   *        2048      499711      248832   83  Linux
/dev/sda2          501758    16775167     8136705    5  Extended
/dev/sda5          501760    16775167     8136704   8e  Linux LVM


* delete partitions 2 and 5

New: extended partition:

Command (m for help): n
Partition type:
   p   primary (1 primary, 0 extended, 3 free)
   e   extended
Select (default p): e
Partition number (1-4, default 2): 
Using default value 2
First sector (499712-33554431, default 499712): 
Using default value 499712
Last sector, +sectors or +size{K,M,G} (499712-33554431, default 33554431): 
Using default value 33554431

Command (m for help): p

Disk /dev/sda: 17.2 GB, 17179869184 bytes
255 heads, 63 sectors/track, 2088 cylinders, total 33554432 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0x000d72a0

   Device Boot      Start         End      Blocks   Id  System
/dev/sda1   *        2048      499711      248832   83  Linux
/dev/sda2          499712    33554431    16527360    5  Extended

New logical partition:

Command (m for help): n
Partition type:
   p   primary (1 primary, 1 extended, 2 free)
   l   logical (numbered from 5)
Select (default p): l
Adding logical partition 5
First sector (501760-33554431, default 501760): 
Using default value 501760
Last sector, +sectors or +size{K,M,G} (501760-33554431, default 33554431): 
Using default value 33554431

Fix type


   Device Boot      Start         End      Blocks   Id  System
/dev/sda1   *        2048      499711      248832   83  Linux
/dev/sda2          499712    33554431    16527360    5  Extended
/dev/sda5          501760    33554431    16526336   8e  Linux LVM

"write" out and reboot.


pvdisplay
pvresize /dev/sda5
pvdisplay
lvresize /dev/salvus-base/root -l +2048
resize2fs /dev/salvus-base/root

It worked:

root@salvus-base:/home/salvus# resize2fs /dev/salvus-base/root
resize2fs 1.42 (29-Nov-2011)
Filesystem at /dev/salvus-base/root is mounted on /; on-line resizing required
old_desc_blocks = 1, new_desc_blocks = 1
Performing an on-line resize of /dev/salvus-base/root to 4130816 (4k) blocks.
The filesystem on /dev/salvus-base/root is now 4130816 blocks long.

root@salvus-base:/home/salvus# df -h
Filesystem                     Size  Used Avail Use% Mounted on
/dev/mapper/salvus--base-root   16G  6.5G  8.3G  44% /
udev                           7.9G  4.0K  7.9G   1% /dev
tmpfs                          3.2G  248K  3.2G   1% /run
none                           5.0M     0  5.0M   0% /run/lock
none                           7.9G     0  7.9G   0% /run/shm
/dev/sda1                      228M   47M  169M  22% /boot
root@salvus-base:/home/salvus# df -h /
Filesystem                     Size  Used Avail Use% Mounted on
/dev/mapper/salvus--base-root   16G  6.5G  8.3G  44% /

---------------------------

  cd vm
  qemu-img create -b salvus_base.img -f qcow2 10.38.1.20.img
  mkdir mnt; guestmount -i -a 10.38.1.20.img --rw mnt

Sept 28:
========
In base vm with 16 cores, the time to doctest all sage (after doing it once already):

salvus@salvus-base:~/sage-current$ time sage -tp 20 devel/sage/sage/

real    7m23.826s
user    98m13.080s
sys     13m17.094s

I'm curious how long on the non-VM (bare metal hardware). 
Also, I'm curious how long while building on the bare metal hardware at the same time...

MAIN Goal now is getting vm.py to actually work.

On bare metal, repeatedly:
salvus@01salvus:~/sage-5.3$ time ./sage -tp 20 devel/sage/sage
real    6m2.230s
user    82m51.347s
sys     11m13.054s

That's pretty cool -- kvm isn't costing us much at all -- it's 81% of
bare metal overall.
---

Working on loging now that I can start vm's, etc.

salvus@01salvus:~/salvus/salvus$ . salvus-env 
salvus@01salvus:~/salvus/salvus$ ./vm.py --ip_address=10.38.1.38 --hostname=01salvus1 --logfile=log -d


# need to do this in the base vm:
sage -sh
easy_install protobuf

On the host machines:

   sudo apt-get install guestmount
   sudo chmod a+r /boot/vmlinuz-* # used by guestmount

Trying on 02salvus:

./vm.py --ip_address 2.1
virsh -c qemu:///session qemu-monitor-command --hmp 10.38.2.1 'hostfwd_add ::2222-:22'   
ssh localhost -p 2222


On 04salvus:

sudo apt-get install virtinst

---

7:10am Sept 30:
===============

I just changed the /etc/rc.local's to 
 
   nice --19 /home/salvus/salvus/salvus/data/local/sbin/tincd

I can autostart/stop/etc. VM's on all *salvus machines.  However, the
tinc vpn thing doesn't work on 03salvus and 04salvus.  I have no idea
why.  Here is my plan to find out:

[] start a VM on 03salvus
       salvus@03salvus:~$ cd salvus/salvus/; . salvus-env; ./vm.py --ip_address=3.1

[] see if it works magically; if so, done -- try 04salvus 
   DOESN'T WORK

[] start a VM on 01salvus, and verify that vpn does work.
       salvus@01salvus:~$ cd salvus/salvus/; . salvus-env; ./vm.py --ip_address=2.1

[] swap in various ways the two vm credentials exactly and see what happens:

   1. add host public key for 3.1 vm to 01salvus server and try to connect manually:
       salvus@03salvus:~$ virsh -c qemu:///session qemu-monitor-command --hmp 10.38.3.1 'hostfwd_add ::2222-:22'   
       salvus@03salvus:~$ ssh localhost -p 2222
       salvus@10dot38dot3dot1:~$ sudo scp /home/salvus/salvus/salvus/data/local/etc/tinc/hosts/10dot38dot3dot1 salvus.math.washington.edu@01salvus:salvus/salvus/data/local/
       salvus@10dot38dot3dot1:~$ vi /home/salvus/salvus/salvus/data/local/etc/tinc/tinc.conf  # change 03salvus to 01salvus
       salvus@10dot38dot3dot1:~$ sudo su
       root@10dot38dot3dot1:/home/salvus# killall tincd
       root@10dot38dot3dot1:/home/salvus# /home/salvus/salvus/salvus/data/local/sbin/tincd -D -d3
    ## this actually works!
  2. OK, test that network is solid:
salvus@10dot38dot2dot1:~$ iperf -s
...
salvus@10dot38dot3dot1:~$ iperf -c 10.38.2.1
------------------------------------------------------------
Client connecting to 10.38.2.1, TCP port 5001
TCP window size: 22.9 KByte (default)
------------------------------------------------------------
[  3] local 10.38.3.1 port 48455 connected with 10.38.2.1 port 5001
[ ID] Interval       Transfer     Bandwidth
[  3]  0.0-10.0 sec   213 MBytes   179 Mbits/sec

  3. Inspect how tinc is configured on 03salvus versus 01salvus.

salvus@01salvus:~$ ls -lh salvus/salvus/data/local/etc/tinc/
lrwxrwxrwx 1 salvus salvus   27 Sep 27 09:06 hosts -> ../../../../conf/tinc_hosts
-rw------- 1 salvus salvus 1.7K Sep 27 09:06 rsa_key.priv
-rw-rw-r-- 1 salvus salvus   55 Sep 27 09:06 tinc.conf
-rwx------ 1 salvus salvus   61 Sep 27 09:06 tinc-up

salvus@03salvus:~$ ls -lh salvus/salvus/data/local/etc/tinc/
total 12K
lrwxrwxrwx 1 salvus salvus   27 Sep 28 22:51 hosts -> ../../../../conf/tinc_hosts
-rw------- 1 salvus salvus 1.7K Sep 28 22:51 rsa_key.priv
-rw-rw-r-- 1 salvus salvus   55 Sep 28 22:51 tinc.conf
-rwx------ 1 salvus salvus   61 Sep 28 22:51 tinc-up

   4. Try to connect to 03salvus from 02salvus:

salvus@02salvus:~$ vi /home/salvus/salvus/salvus/data/local/etc/tinc/tinc.conf  # comment out others -- put only 03salvus
salvus@02salvus:~$ more /home/salvus/salvus/salvus/data/local/etc/tinc/tinc.conf  
Name = 02salvus
ConnectTo = 03salvus
#ConnectTo = combinat0
#ConnectTo = bsd0
salvus@02salvus:~$ sudo su  
[sudo] password for salvus: 
root@02salvus:/home/salvus# killall tincd
root@02salvus:/home/salvus# /home/salvus/salvus/salvus/data/local/sbin/tincd -D -d3

    THIS FAILS.  

    Conclusion: Something wrong with 03salvus's config -- not the VM, but the server.  Inspect further. 
  
    Ahhh.  03salvus and 04salvus got new public keys.  I probably
    never updated those in the base template vm.  So the problem is
    that clients are not trusting the server (not the other way
    around).  This is straightforward to fix, and really obvious.  I
    must have been tired/stupid/rushed to not instantly realize this
    before.

  (done)   * make pub keys correct in repo on 01salvus (and github)
  (done)   * shutdown my vm's
     * start the base template vm.
salvus@01salvus:~$ virsh start salvus_base

     * pull repo

salvus@01salvus:~$ ssh base.salv.us
salvus@salvus-base:~$ cd salvus/; git pull

test that can connect to 03 and 04

root@salvus-base:/home/salvus/salvus/salvus/data/local/etc/tinc# scp hosts/salvus_base salvus@03salvus.math.washington.edu:salvus/salvus/data/local/etc/tinc/hosts/

works!

salvus@01salvus:~$ virsh -c qemu:///session qemu-monitor-command --hmp salvus_base 'hostfwd_add ::2222-:22'   
salvus@01salvus:~$ ssh localhost -p 2222


     * apt-get update; apt-get upgrade
re-edit /etc/grub.d/00_header
     * reboot, reboot  -- works, works
     * shutdown 

     * (done) test on 01salvus
     * push out new vm template to (rsync):

salvus@01salvus:~$ time rsync --sparse vm/images/salvus_base.img 03salvus:vm/images/salvus_base.img
salvus@03salvus's password:

real    2m59.049s
user    1m42.946s
sys     0m8.365s

This --sparse option does work:

salvus@03salvus:~/vm/images$ du -sch *
7.8G    salvus_base.img
7.8G    total

     * test 

salvus@03salvus:~$ cd salvus/salvus/; . salvus-env; ./vm.py --ip_address=3.1

WORKS!!

[x] also test on 02 and 04 after rsync --sparse...

works.


[x] I also need to make the tinc.conf on all salvus nodes like this:

ConnectTo = 01salvus
ConnectTo = 03salvus
ConnectTo = 04salvus
ConnectTo = servedby1
ConnectTo = combinat0
ConnectTo = bsd0

vi /home/salvus/salvus/salvus/data/local/etc/tinc/tinc.conf
sudo su
killall tincd && sleep 1 && nice --19 /home/salvus/salvus/salvus/data/local/sbin/tincd

   - 02salvus -- done
   - 01salvus
   - 03salvus
   - 04salvus

[x] I need to pull in the latest repo (with new public keys) to all salvus nodes, and to combinat0, bsd0, servedby1:

salvus@servedby1:~$ mv salvus salvus0
salvus@servedby1:~$ git clone https://github.com/williamstein/salvus.git
salvus@servedby1:~$ mv salvus0/salvus/data salvus/salvus/

   [x] servedby1
   [x] combinat0
   [x] bsd0


[x] I need to un-firewall 01-04salvus for servedby1



---

It looks like I get another 20 minutes before this Tish adventure on a boat today...

What to do?

[x] debug the VirtualMachine process object at a low level:

In [1]: import admin
In [2]: v = admin.VirtualMachine('2.1', vcpus=3, ram=3)

[ ] have some fun:

  >>> v = [admin.VirtualMachine('2.%s'%i, vcpus=2, ram=2)  for i in range(1,9)]
  >>> [x.start() for x in v]

TOP:

64739 salvus    20   0 3973m 257m 6092 S   91  0.4   0:07.10 kvm                                                       64814 salvus    20   0 2412m  26m 6036 S   75  0.0   0:02.31 kvm                                                       64827 salvus    20   0 2412m  26m 6036 S   51  0.0   0:01.64 kvm                                                       64685 salvus    20   0 4045m 260m 6092 S   31  0.4   0:07.64 kvm                                                       64711 salvus    20   0 3973m 256m 6092 S   25  0.4   0:07.56 kvm                                                       64726 salvus    20   0 3757m 261m 6092 S   24  0.4   0:07.17 kvm                                                       64672 salvus    20   0 4109m 253m 6092 S   24  0.4   0:07.47 kvm                                                       64701 salvus    20   0 4050m 250m 6092 S   15  0.4   0:07.47 kvm       

salvus@01salvus:~/salvus/salvus$ df -h
Filesystem                 Size  Used Avail Use% Mounted on
/dev/mapper/01salvus-root   55G   24G   29G  46% /


salvus@01salvus:~$ virsh list --all
 Id Name                 State
----------------------------------
  1 10.38.2.1            running
  2 10.38.2.5            running
  3 10.38.2.2            running
  4 10.38.2.3            running
  5 10.38.2.7            running
  6 10.38.2.4            running
  7 10.38.2.6            running

salvus@01salvus:~/salvus/salvus$ free
             total       used       free     shared    buffers     cached
Mem:      65948776   44262736   21686040          0     254824   39749772

In a VM:
Cpu0  :  0.0%us,  0.0%sy,  0.0%ni,100.0%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st
Cpu1  :  0.8%us,  0.8%sy,  0.0%ni, 98.3%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st
Mem:   2051596k total,   282656k used,  1768940k free,    23988k buffers

VPN Network seems rock solid.

Interestingly, 2 iperf's at once don't slow things down much, if at all.  good.

Stopping. good.

Cleanup. good.

TODO: make status output different, e.g., maybe it could give status for the kvm process too or something?
      as it is, then %mem, etc., are useless....

In [8]: [str(x.status()) for x in v]
Out[8]: 
["{'%mem': '0.0', 'pid': '4842', '%cpu': '0.0', 'cputime': '00:00:00', 'etime': '01:33', 'start': '09:32:00', 'vsize': '80592', 'rss': '8244'}",
 "{'%mem': '0.0', 'pid': '4847', '%cpu': '0.0', 'cputime': '00:00:00', 'etime': '01:33', 'start': '09:32:00', 'vsize': '80592', 'rss': '8244'}",
 "{'%mem': '0.0', 'pid': '4864', '%cpu': '0.0', 'cputime': '00:00:00', 'etime': '01:33', 'start': '09:32:00', 'vsize': '80592', 'rss': '8244'}",
 "{'%mem': '0.0', 'pid': '4882', '%cpu': '0.0', 'cputime': '00:00:00', 'etime': '01:33', 'start': '09:32:00', 'vsize': '80592', 'rss': '8244'}",
 "{'%mem': '0.0', 'pid': '4900', '%cpu': '0.0', 'cputime': '00:00:00', 'etime': '01:33', 'start': '09:32:00', 'vsize': '80592', 'rss': '8240'}",
 "{'%mem': '0.0', 'pid': '4915', '%cpu': '0.0', 'cputime': '00:00:00', 'etime': '01:33', 'start': '09:32:00', 'vsize': '80592', 'rss': '8244'}",
 "{'%mem': '0.0', 'pid': '4935', '%cpu': '0.0', 'cputime': '00:00:00', 'etime': '01:33', 'start': '09:32:00', 'vsize': '80592', 'rss': '8244'}"]

Time to stop 8 vms:

In [9]: time [x.stop() for x in v]
CPU times: user 0.01 s, sys: 0.03 s, total: 0.04 s
Wall time: 5.72 s


4:17pm Sept 30:
===============

I need to have three files in my deploy list:

  - hosts
  - services
  - infrastructure

At some point, these could become three tables in a sqlite database
that is distributed (with a table that holds every query that changes
the table, and gets played eventually to all nodes).

I really need some database-like thing -- with no single point of failure, etc., --
to keep track of this configuration info, in the long run.

   http://zookeeper.apache.org/   -- not so impressed.  

Another option would be to use git.  Just store config info in these
text files in a git repo, and pull/push it around as needed.  This
might work?  It's what I'm doing now, basically.

Right now I'm going to figure out the file-based infrastructure implementation and get
it implemented. 

git pull wstein@salvus0.salv.us:/Users/wstein/salvus/

 [x] Define format via sample config file.  Will be much like Service class.
 [x] Test/debug 

 [x] Starting Sage service: test/debug as good example:

$ cqlsh -3 servedby1.salv.us
cqlsh> select * from salvus.sage_servers ;

Sage client test:

blastoff:salvus wstein$ time echo "os.popen('hostname').read()" | ./sage_server.py -c --hostname 10.38.2.1 -p 6000
PID = 2723
sage [0]: '10dot38dot2dot1\n'
sage [1]: 
Exiting Sage client.
real	0m0.329s
user	0m0.116s
sys	0m0.069s


   [x]      - turning on the firewall kills tinc unless I ssh in via 2222 then it comes back and works. 

ufw --force reset && ufw allow 22 && ufw allow out 22 && ufw allow out 53 && ufw allow 655 && ufw allow out 655 && ufw allow proto tcp from 10.38.1.5 to any port 6000 && ufw allow proto tcp from 10.38.1.7 to any port 6000 && ufw allow proto tcp from 10.38.1.1 to any port 6000 && ufw allow proto tcp from 10.38.1.11 to any port 6000 && ufw deny proto tcp to any port 1:65535 && ufw deny proto udp to any port 1:65535 && ufw default deny outgoing && ufw --force enable

 [x] when restarting a vm, the admin Service object caches an ssh session to the old machine, which fails.   need to delete from cache if it fails.

 [x] not necessary -- we just background doing the setting and it is very fast. Maybe when starting sage process, make setting the firewall part of the process not the Service. ?

??? [ ] Dependencies.  When stopping services, should stop other services
     first?  E.g., if we're stopping a vm, we should stop the services
     on the vm.  This would be good for the sage services, since their
     status is in the cassandra db, and will be wrong if they are not
     properly stopped.


----

Disk benchmarking:

  time dd bs=1M count=512 if=/dev/zero of=test conv=fdatasync; rm test

sage.math: 64.9 MB/s

sage.math over NFS: 82.5 MB/s

sage.math ramdisk (tmpfs):  933 MB/s

disk.math locally:  39.5 MB/s  (my home dir) 
                    226 MB/s   in /lvm/data/home/, which has the RAID

combinat locally:  73.7 MB/s

04salvus: 135 MB/s

kvm on 02salvus with default cache option (whatever that is):  40.9 MB/s
kvm on 02salvus with cache=writeback:  around 104MB/s


virtualbox on combinat: 63.4 MB/s

virtualbox on bsd: absurd/inconsistent numbers -- must be cached in ram

---
Another benchmark:

salvus@10dot38dot2dot1:~$ wget http://sage.math.washington.edu/home/release/sage-5.4.rc0/sage-5.4.rc0-sage.math.washington.edu-x86_64-Linux.tar.gz
2012-09-30 19:49:14 (49.7 MB/s) - `sage-5.4.rc0-sage.math.washington.edu-x86_64-Linux.tar.gz' saved [738098205/738098205]
salvus@10dot38dot2dot1:~$ time tar xf sage-5.4.rc0-sage.math.washington.edu-x86_64-Linux.tar.gz 
real	0m46.797s
user	0m18.109s
sys	0m7.236s
salvus@10dot38dot2dot1:~$ time sync
real	0m0.319s
user	0m0.000s
sys	0m0.004s

Versus native disk on 01salvus:
salvus@01salvus:~/tmp$ wget http://sage.math.washington.edu/home/release/sage-5.4.rc0/sage-5.4.rc0-sage.math.washington.edu-x86_64-Linux.tar.gz
2012-09-30 19:50:02 (82.0 MB/s) - `sage-5.4.rc0-sage.math.washington.edu-x86_64-Linux.tar.gz' saved [738098205/738098205]
salvus@01salvus:~/tmp$ time tar xf sage-5.4.rc0-sage.math.washington.edu-x86_64-Linux.tar.gz 
real	0m22.142s
user	0m17.841s
sys	0m6.420s
salvus@01salvus:~/tmp$ time sync
real	0m22.804s
user	0m0.000s
sys	0m0.048s


----> Combining with the sync time, it is almost the same...  Note that the vm in the test had very little ram. 
Try again with a VM with lots (32gb) of RAM:

salvus@10dot38dot2dot1:~$ free
             total       used       free     shared    buffers     cached
Mem:      32951292     438400   32512892          0      14248      43396
-/+ buffers/cache:     380756   32570536
Swap:            0          0          0
salvus@10dot38dot2dot1:~$ wget http://sage.math.washington.edu/home/release/sage-5.4.rc0/sage-5.4.rc0-sage.math.washington.edu-x86_64-Linux.tar.gz
100%[===============================================================================================================================================>] 738,098,205 86.4M/s   in 8.5s    
salvus@10dot38dot2dot1:~$ time tar xf sage-5.4.rc0-sage.math.washington.edu-x86_64-Linux.tar.gz 
real	0m32.921s
user	0m18.241s
sys	0m9.941s
salvus@10dot38dot2dot1:~$ time sync
real	0m8.841s
user	0m0.000s
sys	0m0.020s
---------------

Seems great.

====================
[ ] Implement support for attaching persistent images to VM

   [ ] semantics

I don't see I'm going to get any significant advantage by using LVM's
directly over sparse images.  And sparse images will be way, way
easier to manage, right?  Which is easier to manage?

   - lvm volume: - always there as a separate volume
                 - lvm supports snapshots
                 - will be clear when starting VM what to do regarding persistent disk
                 - trivial for host to also mount
                 - much hard to move from one machine to another
   - qcow2 image: - sparse
                  - easy to move to another machine
                  - easy to make a complete backup of it.
                  - supports 128-bit AES encryption.  This would be
                    *perfect* to have -- then even if an image is
                    copied/taken/etc, nothing is lost. 
                    Obviously, this will entail typing a password
                    when the vm is started, but that's ok, since
                    I can automate that through admin.py.
                  - of course, one can just encrypt an lvm filesystem.

ecryptfs: https://help.ubuntu.com/12.04/serverguide/ecryptfs.html
apt-get install ecryptfs-utils


root@10dot38dot3dot1:/home/salvus# mount -t ecryptfs secret secret
time dd bs=1M count=512 if=/dev/zero of=test conv=fdatasync; rm test
root@10dot38dot3dot1:/home/salvus#  time dd bs=1M count=512 if=/dev/zero of=test conv=fdatasync; rm test
536870912 bytes (537 MB) copied, 5.77535 s, 93.0 MB/s
root@10dot38dot3dot1:/home/salvus/secret# time dd bs=1M count=512 if=/dev/zero of=test conv=fdatasync; rm test
536870912 bytes (537 MB) copied, 9.3332 s, 57.5 MB/s
real	0m9.336s

*So, there is definitely a performance hit.*

Backups:

I do not *need* to make complete backups of these for my application.
I do need to keep (and test, etc.) the specially made incremental
backups that Cassandra (the DB) produces.  But that is *it*.

     ./vm.py ... --disk=cassandra1.img,64,backup.img,10


[ ] create img file, format ext4:
      
salvus@02salvus:~/tmp$ qemu-img create -f qcow2 ~/vm/images/10.38.2.1-cassandra1.img 64G
Formatting '/home/salvus/vm/images/10.38.2.1-cassandra1.img', fmt=qcow2 size=68719476736 encryption=off cluster_size=65536 

salvus@02salvus:~/tmp$ ls -lh ~/vm/images/10.38.2.1-cassandra1.img 
-rw-r--r-- 1 salvus salvus 193K Sep 30 22:22 /home/salvus/vm/images/10.38.2.1-cassandra1.img

# FAIL!!!
# mkfs.ext4 -f ~/vm/images/10.38.2.1-cassandra1.img 
# mount -t ext4 -o loop ~/vm/images/10.38.2.1-cassandra1.img  mnt

Try this:
sudo apt-get install libguestfs-tools
FAIL.

LAME: I have an idea -- put special code to do *all* the provisioning logic
in salvus itself, and just call it in /etc/rc.local.

Bad idea. 

!! this works !! Try this:

   sudo apt-get install guestfish

No way, guestfish has exactly the functionality I want in something
called the "Prepared Disk Images" section!

  guestfish -N fs:ext4:1G quit   # this works

# the above always creates test1.img in the current directory.

[x] just to prove this worked, mount it

  mkdir mnt
  guestmount -a test1.img -m/dev/vda1 --rw mnt

salvus@02salvus:~/tmp$ df -h mnt
Filesystem      Size  Used Avail Use% Mounted on
guestmount      7.9G  146M  7.4G   2% /home/salvus/tmp/mnt

[ ] tell VM to mount it under /mnt/

Do a test run to see how things come up:

   qemu-img create -b ~/vm/images/salvus_base.img -f qcow2 root.img
   fusermount -u mnt

   virt-install --cpu host --network user,model=virtio --name test1 --vcpus=8 --ram 4096 --import --disk root.img,device=disk,bus=virtio,format=qcow2 --disk test1.img,device=disk,bus=virtio,format=qcow2 --noautoconsole    

virt-install --cpu host --network user,model=virtio --name test1 --vcpus=8 --ram 4096 --import --disk root.img,device=disk,bus=virtio,format=qcow2 --filesystem /home/salvus/tmp/mnt/,/mnt/foo  --noautoconsole    
virsh -c qemu:///session qemu-monitor-command --hmp test1 'hostfwd_add ::2222-:22'   


   virt-install --cpu host --network user,model=virtio --name test1 --vcpus=8 --ram 4096 --import --disk root.img,device=disk,bus=virtio,format=qcow2 --disk test1.img --noautoconsole    

virsh --connect qemu:///session undefine test1

   virt-install --cpu host --network user,model=virtio --name test1 --vcpus=8 --ram 4096 --import --disk root.img,device=disk,bus=virtio,format=qcow2,cache=writeback --disk test1.img,bus=virtio,cache=writeback --noautoconsole    

# disk gets big when I do:  dd bs=1M count=512 if=/dev/zero of=test conv=fdatasync; rm test
# but speed is fine.

time qemu-img convert test1.img test2.img  # this results in disk that is small again!

 virt-install --cpu host --network user,model=virtio --name test1 --vcpus=8 --ram 4096 --import --disk root.img,device=disk,bus=virtio,format=qcow2,cache=writeback --disk test1.img,bus=virtio,cache=writeback --disk test2.img,bus=virtio,cache=writeback --disk test3.img,bus=virtio,cache=writeback  --noautoconsole    

# the devices are mounted in order as /dev/vda1 /dev/vdb1  /dev/vdc1 /dev/vdd1... in order of --disk on command line.

So this should now be dead easy to finish.	


-----------------

[ ] Database Encryption plan: Replace the script "salvus/salvus/data/local/bin/start-cassandra"
by making 

       ./cassandra.py 

do what start-cassandra does, but also have option to encrypt its path using ecryptfs.
It will have to interactively ask for the passphrase.  Then make admin.py support this.
NEVER pass the passphrase on the command line or as a saved file.

This is the right level at which to implement encryption for salvus. 
It will work on all of virtualbox/vmware/in the cloud/kvm, native hosts, etc. 
It greatly simplifies implementing my own private cloud functionality.
Also, the way ecryptfs works, evidently as long as you know the passphrase,
you can read the files. Full stop.   If somebody steals in .img file or
physical computer, nothing is compromised.  

Even better -- I should be able to set things up so when the salvus
user logs in via ssh (paramiko) to start cassandra, the encrypted img
is mounted.  That would be nice.  

This article looks potentially useful: 
   http://nwrickert2.wordpress.com/2012/01/17/experimenting-with-ecryptfs/

-----------------

7:34pm Oct 1:
===============

I get about an hour work on salvus this morning, and that's most
of what I get today.  Focus super hard and get the persistent image
working?


todo -- Bug to fix later -- status when machine is down, should reflect that instead of going boom!
In [21]: s.status('sage','10.')
INFO:root:python_c: cd "$HOME/salvus/salvus" && . salvus-env && python -c "import admin; print admin.Sage(id=0,**{'address': '10.38.1.100'}).status()"
INFO:root:10.38.1.100 --> 

Back to work on persistent disks:
-------------------
# /etc/fs

/dev/vdb1 /mnt/b ext4 defaults 0 1
/dev/vdc1 /mnt/c ext4 defaults 0 1

permission issues...

guestfish -N fs:ext4:1G

guestmount -a test1.img -m/dev/vda1 --rw mnt

then change owner.... works.


OK, everything works.  The only thing left for later is automatically increasing the size.

I'm my office at 9:54am.  Should prep for class but I want to do one more important thing:
   - make it so the base templates are in a different subdir, and that one can
     select the one to use.  This will make it much, much easier to 

Snuck it in -- done by 10:14am.

Now back to teaching prep.

NEXT thing I must address -- I think caching to the cassandra db is *really* slow every once in a while.
It should *NOT* block getting answers back!  Weird.  Anyways, _something_ is really slow every once
and a while for no reason.  

The problem was that I needed to run repair on the cassandra node on
bsd1.  It was jacked.  Giving inconsistent results, etc.  I also
turned off the sage servers on combinat, since they seemed slow,
maybe, possibly due to heavy load by other users....

Done teaching.  I'm going to see if I can understand this speed issue with salvus right now,
then go skate jefferson.

Plan:  [x] I set USE_EXECUTION_CACHE to False and the problem persists.  So it is *not* caused by that!
       [x] put in more debugging code... I'm guessing cassandra choosing sage server is the problem...?

[x] turn off use of cassandra as much as possible in one particular tornado instance and test.

[x] new plan

In [1]: import cassandra
In [2]: cassandra.set_nodes(['combinat4.salv.us','bsd1.salv.us','servedby1.salv.us'])
In [3]: cassandra.get_sage_servers()
Out[3]: 
[(u'combinat3.salv.us', 6000),
 (u'combinat2.salv.us', 6000),
 (u'bsd2.salv.us', 6000),
 (u'servedby2.salv.us', 6000)]


OK, the speed problem is *entirely* that opening a new connection to
cassandra is stupid and time consuming.

I ended up working for nearly 2 hours, but now the use of cassandra is massively better.
It automatically fails over only if there is a connection error or it takes more than 1
second to do something.  In that case, screw that server -- try another. 

Now we solidly get about 0.09 on fresh comps.

Oct 1, 2012 at 8:05pm:
======================

I'm tired, but could work on salvus for an hour if I can muster the strength.

Upcoming projects:

   - roll out salvus running on new 4-machine private cloud, including databases

   - change combinat to use kvm instead of vbox

   - polish admin.py some more.

   - start working on GUI and functionality now that I have a solid backend

   - support virtualbox VM's in my vm.py script, so I can manage all
     infrastructure via admin.py, instead of kvm only.

I'm going to look into making vm.py work with VirtualBox.  It would be handy.

I restructured the ~/vm/images path on 02salvus and in my code.  I need to update
code and make that change on 01,03,04salvus.  Doing that now.

   
   cd salvus; git pull wstein@salvus0.salv.us:/Users/wstein/salvus/; cd ~/vm/images; mkdir base; mv salvus_base.img base/salvus.img; emacs /home/salvus/vm/qemu/salvus_base.xml

DONE.

I also just updated the base VM and added ecryptfs-utils.

Re-shrink base image on 01salvus:

Before:
salvus@01salvus:~/vm/images/base$ du -sch salvus.img 
8.0G	salvus.img
8.0G	total
salvus@01salvus:~/vm/images/base$ du -sc salvus.img 
8359236	salvus.img
8359236	total

time qemu-img convert salvus.img tmp.img

After (surprisingly, not much better):
salvus@01salvus:~/vm/images/base$ du -sch tmp.img
7.8G	tmp.img
7.8G	total
salvus@01salvus:~/vm/images/base$ du -sc tmp.img
8098872	tmp.img
8098872	total


Test, then rsync out to other nodes.

cp salvus.img backups/salvus-2012-10-02.img
mv tmp.img salvus.img
./push

[x] enable passwordless login between 0k-salvus machines

  ssh-keygen -b 2048
  cd .ssh
  cp id_rsa.pub authorized_keys
  cd
  rsync -axvH  .ssh/ 02salvus:.ssh/
  rsync -axvH  .ssh/ 03salvus:.ssh/
  rsync -axvH  .ssh/ 04salvus:.ssh/



[ ] This site looks good for how to convert a qemu image to a vbox image.

  http://en.wikibooks.org/wiki/QEMU/Images#Exchanging_images_with_VirtualBox

  [ ] Convert salvus_base


  [ ] How to make a VirtualBox "differencing image".


[ ] I seriously need to code in admin.py to answer questions: 
     - "what would be deployed on a given node?"
     - what is actually deployed on a given node

[x] add base and disk support to admin.py for Vm class.

[x] disk space -- allocate 512GB lvm on 0ksalvus for persistent disk.

      lvcreate --name salvus_persistent --size 512G 01salvus
      mkfs.ext4 /dev/mapper/01salvus-salvus_persistent

Add to /etc/fstab:

        /dev/mapper/01salvus-salvus_persistent  /home/salvus/vm/images/persistent ext4 defaults 1 1

Then:

       mount -a; chown salvus. /home/salvus/vm/images/persistent/; rm -rf /home/salvus/vm/images/persistent/lost+found/

Then we can make a snapshot of persistent disks and back them up using rsync (or mirror) later...

Do this on 02,03,04salvus now:

lvcreate --name salvus_persistent --size 512G `hostname`; mkfs.ext4 /dev/mapper/`hostname`-salvus_persistent
echo "/dev/mapper/`hostname`-salvus_persistent  /home/salvus/vm/images/persistent ext4 defaults 1 1" >> /etc/fstab
mount -a; chown salvus. /home/salvus/vm/images/persistent/; rm -rf /home/salvus/vm/images/persistent/lost+found/

DONE.

  [X] no -- learn rsnapshot and setup nightly backups where 0ksalvus is backed up to (0k+1)salvus?

Discussion about backing up sparse files:

  http://www.backupcentral.com/phpBB2/two-way-mirrors-of-external-mailing-lists-3/rsnapshot-24/rsnapshot-questions-96266/

NO -- Potential plan:
   [ ] Make a 400G (or whatever is left) logical volume on each 0ksalvus node and mount it as /rsnapshot

   [ ] 01salvus backs up to 02salvus:/rsnapshot/01salvus/
       02salvus backs up to 03salvus:/rsnapshot/02salvus/
       03salvus backs up to 04salvus:/rsnapshot/03salvus/
       04salvus backs up to 01salvus:/rsnapshot/04salvus/

... I am not sure I agree with this plan. I should only backup what I
*need* to backup and no more.  And that's the cassandra data.

Instead, I should focus on how to do that safely, and archive those
snapshots.  Absolutely nothing else needs to be backed up.   

I should:
 
   (1) make sure all infrastructure configuration, setup, etc., is as
       *automated as possible* and stored in the github salvus repo.
   (2) become very good at backing up cassandra's DB, and have this backup 
       be something that is done from within the cassandra nodes, controlled
       by my admin scripts, so it works no matter how they are deployed.
   (3) ensure that *all* state relevant to the salvus app is stored in
       cassandra or github.
      

[ ] try exporting cassandra to json:

salvus@bsd1:~/salvus/salvus/data/cassandra-0/conf$ export CASSANDRA_CONF=`pwd`

FAIL -- confused by weird snapshot behavior...


[ ] clearing all old snapshots:

   h.nodetool('clearsnapshot')

[ ] making a new snapshot:

   h.nodetool('snapshot')

[ ] Given the simplicity of my data model(s) I could also write my own dump/restore...?

[ ] EVIDENTLY, It is *critical* that JNA is configured to use cassandra properly:

  sudo apt-get install libjna-java

I did not have this installed.

Fix on existing nodes:

   import admin, deploy; reload(admin); reload(deploy); h = deploy.hosts; s=deploy.services
   h('all','ufw disable; killall apt-get; apt-get -y --force-yes install libjna-java; ufw --force enable', sudo=True)
   # BETTER: 
         h.apt_install('all', 'libjna-java')

Fix on my base salvus image:

  virsh start salvus_base
  virsh -c qemu:///session qemu-monitor-command --hmp salvus_base 'hostfwd_add ::2222-:22'
  ssh localhost -p 2222
  login and "sudo apt-get install libjna-java", then shutdown.
  # push out the updated base image

Netflix's Priam does all the backup stuff I would want and much, much more: https://github.com/Netflix/Priam
Unfortunately, it is undocumented and only works on AWS with their API's so is useless to me.  However, 
it is important to note that they:

    1. do use the incremental backup feature
    2. regularly use their backups to create test clusters for testing things out based on live data.

2. would be good for me to do.

I just tried taking the stateless_exec snapshot directory from one of the cassandra nodes, and following
the directions here to restore to my laptop cassandra instance:

    http://www.datastax.com/docs/1.1/operations/backup_restore

It worked perfectly.  Very cool.

I'll just have to mull over cassandra backup.
In the meantime, doing:

  h.nodetool('snapshot')

and 
  
  h.nodetool('repair')

regularly is a good idea.

-----

I'm at home.  Some over-users have pseudo-crashed combinat again.
I need to move salvus to dedicated infrastructure asap.
Time to move to the salvus nodes.  I don't think there is anything
more I need to do in order to do this (except with cassandra).

Move to salvus:

(done)  1. Replace combinat.math.washington.edu in godaddy dns with ip of 01salvus.
(done)  2. Add ip for 02salvus also, for redundancy.

PLAN: 

  [ ] fix vpn broadcast everywhere and restart those tinc's

  [ ] make a completely *new* minimal deployment to 01salvus cluster
      with these rules/properties; except it will still use two old
      cassandras

  [ ] turn off old deployment 

  [ ] fix network stuff on all old machines by hand (since vbox isn't automatic).

  [ ] start old machines as part of new deployment. 

DETAILS tinc:

     [ ] change all tinc conf's so I have a class A vpn -- just change
         tinc-up generated by admin, by vm, and on every single node;
         I think this just means changing the netmask in tinc-up to
         255.0.0.0 and that is it.

try this on

   - my laptop
   - 01salvus
   - my vm template system


emacs salvus/salvus/data/local/etc/tinc/tinc-up; sudo killall tincd; sudo salvus/salvus/data/local/sbin/tincd


and test.

     [ ] remove combinat0 from hosts/ path on git repo
     [ ] change to using ip addresses everywhere (no more names)

ARGH -- weirdly all the new dell machines are inaccessible over the
network right now, so I can't do anything anyways.  i'll get up very
early tomorrow and work on this after I am doing preping to teach.

LET THIS BE A WARNING TO ME!  

Now salvus is running on servedby, combinat (for the sage sessions --
it isn't dead?), and bsd. But it is running!

---------

Oct 3, 2012:

  - i just tested switching to a class A network, and it works fine.  Yes.
  - i fixed the salvus machines and combinat -- indeed, it was ufw gone crazy!

--

I just got approved via wstein@uw.edu for 8 Google Compute Engine
instances for testing.   I will mention this in my talk tomorrow,
and start planning to set them up!


I'm going to try to make it so I can run vm's on geom just as easily as 0ksalvus. 

Packages (and important fix):

   sudo apt-get install python-software-properties; sudo add-apt-repository ppa:ubuntu-virt/kvm-network-hang; sudo apt-get update; sudo apt-get install qemu-kvm  guestfish guestmount virtinst; sudo apt-get remove unattended-upgrades; sudo apt-get autoremove

Base image:

   Copy/paste over ssh public key from 01salvus to authorized keys on salvus@geom

   salvus@geom: mkdir vm; mkdir vm/images/; mkdir vm/images/base

# Remove old .libvirt on geom and replace by one from 01salvus, and symlink to 

   salvus@geom: rm -rf ~/.libvirt; cd vm; ln -s ~/.libvirt/qemu .; scp -r 01salvus:.libvirt ~/

   salvus@01salvus:~/vm/images/base$ rsync --sparse -axvH ./ geom.math.washington.edu:vm/images/base/

# Making it so virsh works

   echo 'export VIRSH_DEFAULT_CONNECT_URI="qemu:///session"' >> ~/.bashrc; source ~/.bashrc
   virsh list --all

# Fix permissions so that guestmount works (run this *and* put in /etc/rc.local)

   sudo chmod a+r /boot/vmlinuz-* # used by guestmount  

Add salvus to the kvm, libvirt and fuse groups in etc/group
 

Build salvus:
 
   git clone https://github.com/williamstein/salvus.git
   cd salvus/salvus
   . salvus-env
   ./build.py --build_all

Setup vpn:

   admin.tinc_conf('geom0',['combinat0','bsd0','01salvus','02salvus','03salvus','04salvus','servedby1'])


Oct 3, 2012
===========

Working on setting up vpn and kvm for other hosts.  

I've change the tinc names to match hostnames.  This means I need to
update salvus on all tinc vpn servers, which are:

   '01salvus', '02salvus', '03salvus', '04salvus', 'geom', 'combinat', '

and also salvus_base, which I will want to push out.  So:

 # on 01salvus (say), but could do this on any 0ksalvus or geom or combinat:

    cd salvus; git pull
    virsh start salvus_base
    ssh base.salv.us
    cd salvus/salvus/
    git pull
    sudo shutdown -h now

# launch pushing out new vm:
(This should be in the git repo!)

salvus@01salvus:~$ more vm/images/base/push
#!/usr/bin/env python

import os

BASE = 'vm/images/base/'

for hostname in ['%02dsalvus.math.washington.edu'%k for k in [1,2,3,4]] + \
                ['%s.math.washington.edu'%h for h in ['geom','combinat']]:
    cmd = "rsync --sparse -uaxvH %s %s:vm/images/base/ &"%(BASE, hostname)
    print cmd
    os.system(cmd)

---

Setup to use 01salvus as base git repo:

  cd salvus; git remote rm origin; git remote add origin 01salvus.math.washington.edu:salvus/; git pull origin master
  

# on 01salvus:

   cd salvus; git pull
   ssh 01salvus "cd salvus; git pull origin master"
   ssh 02salvus "cd salvus; git pull origin master"
   ssh 03salvus "cd salvus; git pull origin master"
   ssh 04salvus "cd salvus; git pull origin master"
   ssh geom.math.washington.edu "cd salvus; git pull origin master"
   ssh combinat.math.washington.edu "cd salvus; git pull origin master"


OK, the above is now automated.  W00t!

Need a script to *restart* tinc.

---

Trying to get rid of last virtualbox vm on combinat.

This is: combinat4.salv.us = 10.38.1.10

I'm first just trying to copy off the cassandra-0 directory, which is actually quite small 

Making a test machine on geom:

  salvus@geom:~/salvus/salvus$ ./vm.py --ip_address=10.37.1.10 --disk=cassandra:64

However, strangely the persistent disk on there is *not* sparse.  WEIRD.  So I'll
try on 01salvus.

Actually, this is just something I didn't implement correctly!

I'll see if I can figure this out really quickly.

I think I fixed this problem -- it was due to moving a sparse img file
across filesystems using Python.  At least that was why there was a
problem on salvus.


NEXT, after I completely finish preping my talk, or at least want to work on salvus again,
I want to:

   - lay out my new deployment
   - implement features I need to make it work (especially using
     cassandra on persistent disk nodes where one gets /mnt/cassandra to start with)
   - start up a whole new thing just on 0ksalvus
   - test
   - stop old one
   - expand new one to encompass old one
   - turn off combinat4 
   - uninstall virtualbox on combinat
   - test that kvm works on combinat


===   

I'm going to give a talk tomorrow on Salvus.  My goals in giving this talk are:

  * clarify for myself what I've done and where I'm going with this project
  * clarify for myself what hardware and organizations are involved
  * clarify for myself what is left to do
  * get useful feedback, and maybe key help

Outline of talk.


Title slide


  Salvus - Adj. meaning well, unharmed, sound; alive; safe, saved
 
  Pictures


Project goals:

  * project to make Sage (and other math software) very robustly
    available over the internet to a large number of users.

  * generate revenue to support core sage development: working with UW
    C4C on license agreement, payment system, etc., they get 20% of
    revenue and the other 80% goes in the sage foundation account

  * if successful, spin off as a separate company in a few years.

Similar Projects:

  Programming, but no math:
     * http://c9.io -- "Your code anywhere, anytime... Write, run, and debug your code with our powerful and flexible cloud IDE."
     * https://www.appsoma.com/ -- "Code & run in the cloud"
     * http://www.heroku.com/ -- "cloud application platform"

  Math, but no programming or scalable computation:
     * http://www.wolframalpha.com/

  Sage:
     * http://sagenb.org -- online notebooks
     * http://aleph.sagemath.org -- single embeddable sage snippet

  Cloud hosting -- can do everything, but expensive and complicated...
     * http://aws.amazon.com/ -- Amazon Web Services
     * http://www.linode.com/ -- Deploy and Manage Linux Virtual Servers in the Linode Cloud.

Emails:

On Wed, Sep 26, 2012 at 9:54 AM, Chris Seberino <cseberino@gmail.com> wrote:
> I have a Xen Ubuntu Linux 1 Gb RAM virtual machine  from a provider called
> Linode.
>

Linode charges $39.95/month for your 1GB machine.  For a 4GB machine,
which would be more likely to work well, you would have to pay
$159.95/month.

My plan with http://salv.us/ -- which I'm working hard on right now --
is to buy a bunch of hardware (mostly done), host it at UW, write much
better more efficient software to make Sage ("highly") available
through it, and provide enough guaranteed resources for a class like
you have... for *substantially* less than $159.95/month, since the
monthly hosting and network fees I have to pay at UW are super-cheap.

Stay tuned...

 -- William

Longterm Programmng Goals (Features):

  * A very simple starter page where you can type a calculation in a
    box (no login required) with tab completion and interact support.
    If you login, then you can easily browse/search/share all past
    computations you've ever done.  Support for mobile browsers from
    the start.  This is all you will often need.

  * Users can have many projects.  A project is defined by a git
    repository, which salvus stores.  Many users can share the same
    project.  It can be linked to github (or google code, or
    bitbucket, etc.)  Thus a project provides a directory tree of
    files, and there is a revision history.

  * A command line -- alternative way to browse a project, and will
    be like the bash command line.   Can type "sage" to run Sage here. 

  * Editing documents -- Salvus will also provide browser-based
    editing of several file types, starting with .sws files (for
    compatibility with sage), but also supporting many other file
    types, and ways of working with documents and data.  Multiple
    users can simultaneously edit a document; changes are broadcast
    and updated on all clients.  Also, editors for .py files, .pyx
    files, csv files, presentations, etc.  Special support for the
    Sage library itself as a project.

  * Publishing -- this is like Heroku or Google App Engine; it will
    support making a persistent Sage-based application available to
    many users, either with a specific URL, or embedded in another
    web page (like with aleph.sagemath.org widgets).  Show example
    on Rob Beezer's website.   Paying users can allocate dedicated
    hosting resources for their app.

  * Graphics -- the entire approach to 2d and 3d graphics in the Sage
    notebook is terrible and will have to be re-done right using some
    combination of SVG, HTML5 Canvas and WebGL.  The eventual goal is
    no Java applets and no static png images.

  * Chat/cross-user messaging -- useful for teaching, online classes,
    tutorials, etc.

However... my entire approach to Salvus is the exact opposite of the
monolithic single-point-of-failure Sage notebook:

  * Taking my time and doing things right (and have thrown away huge
    amounts of code I wrote this summer).  With the sage notebook, I
    wrote something that *looks* and feels a lot like the current
    version in a sleep-deprived month.  

  * Architecture is very service-based.  Numerous independent
    components running on dozens of machines communicating securely
    via messages over a VPN.

  * Design goal -- absolutely no single points of failure anywhere in
    the design of salvus.  

(Note: The demo at http://salv.us is very unimpressive, because all the
real work is behind the scenes.)

Salvus Technology stack:
 
  * Physical Hardware and Remote VM's
  * Virtualization:  libvirt + kvm
  * VPN: tinc
  * Database: cassandra
  * SSL: stunnel   -- https://salv.us
  * Load balancing: haproxy
  * Static HTTP: nginx
  * Dynamic HTTP: tornado
  * Math/compute: forking tcp-based Sage server
  * Management: Paramiko + much new code

Client/server communication:

  * SockJS -- websocket connection between browser and tornado, when
    supported; otherwise, falls back to other long-polling approaches
  * TCP + SSL -- standard UNIX Socket connection for non-browser
    clients, e.g, command line.
  * HTTP -- old-fashioned POST for simple HTTP clients


Diagram (make it using mindmap):

   Client    Client    Client   Client   Client  Client
     /|\
      |
   https1.1 (websocket, etc.)
      |
      |   https://salv.us
     \|/ 
 HAProxy Load Balancers                      
 /|\       /|\      /|\      /|\
  |         |        |        |                                                
  |https1.1 |        |        |                                     
 \|/       \|/      \|/      \|/                                      
Tornado  Tornado  Tornado  Tornado    <--------------------------->   Cassandra   Cassandra    Cassandra ...
           /|\      /|\                                                 /|\
            |        |        -------------------------------------------|
   ---------|        |        |                                         
   |                 |        |
   |                          |
  \|/                        \|/
 SageServer   SageServer  SageServer   SageServer ...

   
Discussion of each component -- rest of talk.

 

Physical Hardware and Remote VM's:
----------------------------------- 
  * 4 3Ghz 16-core 64GB RAM, 1TB disk, 1U servers in Padelford basement
  * 1 8-core 32GB RAM machine in my office
  * 3 3Ghz 16-core 64GB RAM, 6TB disk, 1U servers currently being shiped to the UW tower
  * 2 servers at servedby.net (a cloud host)
  * Google Compute Engine: "8 single CPU instances with 100GB of persistent disk space"
  * Google App Engine: I received $60,000 via a research grant program -- not sure how to use it yet!

Current monthly fixed cost: $60/month (for UW tower hosting)

Amazon (in multiple ways) also told me they would give me free
credits, but didn't deliver.  They are big and confused/confusing.

This should be enough to support several thousand paying customers
(and many free customers), which will bring in enough money to expand.

Pricing will likely be around $10/month per user for the first tier of
non-free account, which competes very well with linnode + sagenb
($100/month?).   

Goal by end of March 2013: Have 1,500 worldwide paying users at
$10/month, which is what the above hardware should easily be able to
support, done right. That's also more than my entire full-time salary.

Virtualization:  libvirt + kvm:
-------------------------------

  * Virtualization is absolutely essential due to allowing users to
    remotely execute arbitrary code (ok, with sagenb.org nothing is
    virtualizaed, but that is just a ticking bomb, which keeps going
    off!)  

  * Public clouds: many companies now rent virtual machines.
 
  * Private clouds: startup many virtual machines based on a template
    with certain parameters, start services running on them.  When the
    machine shuts down, it leaves no trace (except persistent sparse
    disk images).

  * The *only* persistent data in all of Salvus is in the distributed
    database.  Plan is that this sits in a disk partition that is
    encrypted using the Linux ecryptfs filesystem and a passphrase
    that I type in (exactly once from my management interface) when
    starting the database.

  * OpenStack, CloudStack, OpenNebula, etc. -- war of the private
    cloud software distributions.  I spent quite a bit of time
    investigating all this, and its snake oil right now, at least for
    what I'm doing -- basically a lot of companies and people are
    making noise about implementing something open source kind of like
    what Amazon and VMware already did years ago, but with all the
    real work being doing by libvirt.  (This will likely be useful for
    something in a year or two, but now it is worse than useless.)
    The real work is libvirt, kvm (or xen).  I think Redhat put a ton
    of serious money/development into supporting the creation of high
    quality virtualization support -- KVM -- that is built into the
    Linux kernel now.  This is what I'm using directly... in the end,
    to do everything I want nicely, I wrote a few hundred lines of
    code (vm.py) and learned about how to get around subtle bugs.


VPN: tinc
-----------

  * http://www.tinc-vpn.org/

  * I'm NOT using http://openvpn.net/, even though it is enormously
    popular...  because all traffic goes through a master VPN server,
    which is a single point of failure.

  * tinc is "a Virtual Private Network (VPN) daemon that uses
    tunnelling and encryption to create a secure private network
    between hosts on the Internet."  

    Key features:
         - Every public facing machine on the vpn also acts as a
           server, so clients (virtual machines, public cloud
           instances) can connect as long as *any* public-facing
           maching is up.   
         - It's P2P -- not all traffic has to go through one 
           master node, which could be very inefficient. 
         - All traffic is encrypted.
         - Easy/sensible; oo add a machine to the VPN, you just
           generate a public and private key, then add the public key
           to at least one public facing machine.
         - A small C program that builds in seconds, and has been
           under development since 1998.
      
  * by using a VPN, I can easily combine cloud computers, my own
    computers, etc., and don't have to worry about ip addresses
    changing.  Also, I don't have to worry about securing
    communication between the different services or vm's in salvus.
    For a long time, I was worrying about things such as implementing
    SSL + TCP communication between the Tornado web server and backend
    Sage process; I threw all that code away, and instead just use a
    VPN and have a nice clean class A address space as a bonus.
 
Database: Cassandra
====================

The database is the only persistent component of Salvus.  It stores
everybody's data -- it's the memory of what the system is.  It's thus
an absolutely crucial design decision.

  * PostgreSQL is very nice, as is MySQL and even SQLite.  I really
    wanted to use all three, and tried very, very hard to do so (and
    wrote a lot of code that I threw away).  Then I started thinking
    seriously about the amount of data I will have to manage with the
    number of users I plan to have in the long run.  And I read books
    about how to scale website using mysql.  What they do is very
    difficult and ugly, e.g., sharding -- a bunch of different
    databases based on usernames.  (I also read a lot about
    SQLalchemy, used PostgreSQL in a big research project, etc.)

  * Then the Padelford server room blew up for the n-th time, and I
    firmly decided on no SPOF's as an *Axiom*.  From this perspective,
    relational databases like PostgreSQL become even more difficult to
    use. You have to have a master database, at least one other slave
    databases replicating it, and some sort of automatic failover that
    is activated when your software decides the master is not working
    (which is not an easy thing to decide!).

      "I'd like to welcome the github ops/dbas to the club of people
      who've learned the hard way that automated database failover
      usually causes more downtime than it prevents."

      http://news.ycombinator.com/item?id=4524340

    Basically, it's not easy.  I want easy!

  * One of the motivations for the recent wave of "NoSQL databases" is
    creation of databases that have no single point of failure.  They
    scale up by just adding more small machines, and one can configure
    how redundant they are.  I had learned a lot about MongoDB (a
    noSQL database) a few years ago, and after getting LMFDB.org to
    use it a lot (and using it for my own database work), I think it
    sucks.... for me at least. 

      "Does everyone hate MongoDB?"
      http://news.ycombinator.com/item?id=4570790

    I read about and tried out every noSQL database I could find.  (I
    even wrote one: http://code.google.com/p/nosqlite/) For my
    particular use case, by far the best one is Apache Cassandra
    (cassandra.apache.org), which is an open source database with
    copyright owned by Apache, and also commercial support from a
    company.  It is written in Java, which I'm no fan of, but still I
    really like Cassandra.  Unlike many options, Cassandra is pretty
    heavily used by serious companies.  E.g, Netflix makes enormous
    use of Cassandra, as does Disney.  Cassandra was originally
    written by Facebook, who open sourced it, hence it gained a lot of
    traction in the open source world (unlike say Google's internal
    databases, which they did not open source -- now I understand what
    Craig Citro was talking about last year...).

  * Cassandra seems to be the only database that satisfied my
    constraints (mature, distributed, scalable, support for multiple
    data centers); there wasn't any competition.  I've been using it
    for a while now, and I'm impressed.  I started with version 1.1
    using CQL3, which looks like a restricted subset of SQL, with NO
    JOINS or relations at all between tables!

   
SSL: stunnel   -- https://salv.us
=================================

  * One of the main motivations for rewriting the sage notebook "back
    in the day" to use twisted was to support SSL, i.e., encrypted
    communcation between the user's browser and sagenb.org.  This
    worked, but we never properly signed the cert, so users got a
    terrifying warning when they visited sagenb.org, and it was deamed
    easier to just remove the security than deal with the cert stuff.

  * It turns out that one can easily make *any* webapp using SSL
    encryption without having to touch the code that defines that
    website -- just use the program stunnel!

  * Getting a properly signed SSL certificate setup isn't impossible.
    (I even bought my cert for $12.99 by some trick... though UW actually
     offers free sigs if you're patient.)

  * https://salv.us works and doesn't give a security warning on any
    devices I've tested it on.
 
  * This component is a no-brainer. 

  * With salvus we run a couple of stunnel's and put them all in DNS, so
    the browser chooses one.  When an stunnel fails, the browser will just
    load a different one.   No SPOF.

Load balancing: haproxy
=======================

The stunnel process directs all requests to another program called
haproxy (which can be running on a different computer or the same
computer).  Haproxy is now the canonical open source load balancer
program.  Haproxy in monitors a bunch of web servers (for salvus, that
means nginx servers and tornado servers), makes sure they are working,
and balancing incoming connections between them.  The static content
requests go to nginx.  The dynamic websocket connections go to tornado
servers.

Static HTTP: nginx
==================

nginx seems to be the canonical choice for serving a huge number of
static webpages efficiently.  It was easy to setup and use.  It's
lightweight and easy to build from source.    

nginx is single threaded.  This is absolutely key to its success in
serving huge numbers of requests per second.  It's far more efficient
than the old apache approach, which would spawn a thread or process
per request.


Dynamic HTTP: tornado
=====================

Tornado is basically a clean self-contained nicely coded webserver
written in Python that works a lot like twisted (i.e., it is
asynchronous), but is much simpler and more targeted.  It was written
by some company that Facebook bought, and facebook then open sourced
it.  It's pretty much the only solution I could find for serving
websocket connections using a Python webserver -- I couldn't see a way
to do websockets with the WSGI servers, which most python webservers
are (e.g., flask).

Ipython also uses tornado.

Tornado does well in benchmarks.  It is *single threaded*!

One can read all the documenation for tornado in a few hours, and you
really know all you need to know, except what you'll learn from the
excellent source code.

One concern I have is that right now the way Tornado talks to
cassandra is not asynchronous -- I may have to write a better
driver. Database queries are typically super fast though, by the
nature of cassandra slow queries simply aren't allowed by the
language.


Math/compute: forking tcp-based Sage server
============================================

I wrote from scratch a TCP server that starts up as a Sage process
with various things pre-imported such as graphics, Maxima, etc.,
listens on a socket (running as root), accepts a connection, forks,
lowers privileges, executes blocks of sage code, and streams results
back over the connection.  

The design is much different than with the SAge notebook, where one
process watches another.  Here, as code executes, the running process
itself sends messages whenever it wants containing whatever it wants.
The tornado server receives these messages and routes them on (either
directly to a connected client, or to another tornado server to which
a client is connected).

All communication uses Google Protobuf2 binary format for speed and
clarity.  protobuf2 is a brilliant way of *defining* a message
protocol.  And it is very, very efficient and well supported.

Management: Paramiko + much new code
====================================

First I tried to do management by writing a Tornado-based management
web application that had a database backend.  This got painful and
ugly quickly, but looked cool and powerful.

I then wrote a big Python program to do component administration
(starting and stopping the many programs listed above), using mainly
Python Subprocess module, and only got it working on a single machine. 

Then -- just as with private clouds, I found that there are tons (and
tons!) of programs out there for managing installing and running
software on a bunch of machine.  I found one called Ansible that I
kind of liked at first, but found it to be too immature and not so
widely used.  There were some other solutions, but they seemed like
massive overkill.  I spent quite a bit of time investigating them. 
But for every actual application, there is a bunch of subtle logic
that just has to be figured out and coded, and I had a lot of that down
from my code mentioned above.

I took some of the good design ideas from Ansible, and the really,
really good underlying library Ansible uses -- Paramiko, which is a
nearly complete re-implementation of ssh in Python.  Regarding
Paramiko, it is very good code and it has excellent documentation.
The documentation is a bit odd, because it visibly looks ugly,
unorganized and bad, at first glance.  But when I actually started
*using* it to get work done, it is incredibly good.

So I adopted some of the code mentioned above, and have something that
really works now.  This is actually where *most* of the code I've
written and kept has gone.  

-------

Next time I talk, I'll give demos...
