Aug 10, 2012:
------------

I'm setting up VM's on my infrastructure.  Right now I have:

   * virtualbox machines running in spare cycles on geom.math.washington.edu (24-core, 128GB RAM, 128GB SSD for salvus):

configuring:

   * salvus1: 8GB disk, 8GB RAM, 8 cores: machine -- stunnel, haproxy, nginx, database (master), tornado
   * salvus4: 8GB disk + 8GB extra /tmp disk, 16 cores; 16GB RAM: sage server

   * virtualbox machines running in spare cycles on combinat.math.washington.edu (64-core, 192GB RAM, 64GB HD for salvus)

   * salvus5: 8GB disk, 8GB RAM: machine -- stunnel, haproxy, nginx, database (slave), tornado
   * salvus8: 8GB disk + 8GB extra /tmp disk, 16 cores; 16GB RAM: sage server

   * 1 virtual machine at servedby.net


!!!

combinat is massively faster than geom at running virtual machines.
Either there is something really weird with the disk (which I doubt, since
boxen had the same problem), or the instruction set support, or BIOS
configuration, or something. Very weird.


Aug 27, 2012:
------------

My primary goal for this morning is getting the basic demo to work
using Cassandra, having completely removed postgreSQL + memcached.
Once that is done, I'll work on deployment.  I'm going to investigate
partial other-people-wrote-them replacements for admin.py, just
because that script is complicated, and generic:
   * ansible
   * "open source configuration management software" (wikipedia)

  * As of 6:08am (in about 15-20 minutes of time) my primary goal is done -- it works.

I think I'll deal with this first:

  [x] restart of cassandra via my admin script fails, since maybe it takes too long to shut down

OK, that was easy...

Then onto serious stuff.

Testing out Ansible (http://ansible.github.com/)

   easy_install jinja2 pyyaml paramiko
   export ANSIBLE_LIBRARY=/Users/wstein/salvus/salvus/data/local/ansible
   export ANSIBLE_HOSTS=/Users/wstein/salvus/salvus/conf/ansible_hosts
   git clone git://github.com/ansible/ansible.git
   cd ansible && python setup.py install

Setting up a public git server on combinat1:

  http://git-scm.com/book/en/Git-on-the-Server-Public-Access


salvus@combinat1:~$ apt-get install apache2
salvus@combinat1:~$ git clone salvus salvus.git
salvus@combinat1:~$ cd salvus.git/.git/hooks;  cp post-update.sample post-update; cd
salvus@combinat1:~$ more /etc/apache2/httpd.conf
<VirtualHost *:80>
    DocumentRoot /home/salvus/salvus.git/.git
    <Directory /home/salvus/salvus.git/.git>
        Order allow,deny
        allow from all
    </Directory>
</VirtualHost>

Then:
  http://combinat1.salv.us/
gives the repo.

   export ANSIBLE_LIBRARY=/Users/wstein/salvus/salvus/data/local/ansible
   export ANSIBLE_HOSTS=/Users/wstein/salvus/salvus/conf/ansible_hosts

OK, done (in seconds), and now this works to automatically update
all nodes to latest version.

  ansible all -m shell -a "cd salvus; git pull http://combinat1.salv.us" -u salvus

Useful:

  ansible cassandra -a "df -h" -u salvus

Build cassandra on all nodes:

  ansible all -m shell -a "cd salvus/salvus; ./build.py --build_cassandra" -u salvus



I'm now working on diving up the nodes and deploying.

[cassandra]
servedby1.salv.us -- just asked them to give me 64GB disk space
bsd1.salv.us -- creating and easily adding a 64GB disk:

   VBoxManage createhd --filename cassandra.vdi --size 64000 --variant Fixed   # takes about 10 minutes, at least?
   ansible bsd1 -a "shutdown -h now" -u salvus -s -K  # shutdown bsd1...
   VBoxManage storageattach bsd1 --storagectl "SATA Controller" --medium cassandra.vdi --port 2 --type hdd
   nohup VBoxHeadless -s bsd1 --vrde off &

# format:
   ssh salvus@bsd1
   sudo su
   fdisk /dev/sdb   # make 1 new partition that is the whole thing
   mkfs.ext4 /dev/sdb1
# get uuid:
   ls -lh /dev/disk/by-uuid
   702faff3-0626-439f-94f9-c4174efd68de
# edit /etc/fstab:
   UUID=702faff3-0626-439f-94f9-c4174efd68de /cassandra               ext4    errors=remount-ro 0       1
# make path:
   mkdir /cassandra
   chown salvus. /cassandra

   mount -a




combinat4.salv.us --
   1. need to use lvm to expand my existing 64GB partition:
- stop all vm's:

ansible combinatvm -a "shutdown -h now" -u salvus -s -K
ps ax |grep VBox  # confirm it worked (on combinat)

- kill vpn server:

2134 ?        Ss     0:34 /home/salvus/salvus/salvus/data/local/sbin/tincd

- umount the volume

df -h
/dev/mapper/combinat-salvus    60G   50G  7.0G  88% /home/salvus


- lvextend -L+64G /dev/mapper/combinat-salvus
FAIL!
  Extending logical volume salvus to 123.60 GiB
  Insufficient free space: 16384 extents needed, but only 9968 available

I did math and got:

- lvextend -L+38.9G /dev/mapper/combinat-salvus
  Rounding up size to full physical extent 38.90 GiB
  Extending logical volume salvus to 98.50 GiB
  Logical volume salvus successfully resized

- e2fsck -f /dev/mapper/combinat-salvus
- resize2fs /dev/mapper/combinat-salvus
- mount /home/salvus
- df -h
/dev/mapper/combinat-salvus    99G   51G   44G  54% /home/salvus

   2. make a new 40GB lvm drive  -- that's all the space we have.  This is just an initial testing deployment!

# it took only about 2 minutes!


   3. attach it to combinat4

VBoxManage storageattach combinat4 --storagectl "SATA Controller" --medium cassandra.vdi --port 2 --type hdd

# oops -- already had users.vdi -- this was designed for running sage.  need to use a different vm!
# should use combinat2.salv.us

VBoxManage storageattach combinat4 --storagectl "SATA Controller" --medium none --port 2 --type hdd

VBoxManage closemedium disk cassandra.vdi
VBoxManage list hdds
mv cassandra.vdi ../combinat2

# fix what I broke:
VBoxManage storageattach combinat4 --storagectl "SATA Controller" --medium tmp.vdi --port 2 --type hdd

cd ../combinat2
VBoxManage storageattach combinat2 --storagectl "SATA Controller" --medium cassandra.vdi --port 2 --type hdd


# woah -- I got the hostnames wrong!
VBoxManage modifyvm combinat2 --name combinat4a
VBoxManage modifyvm combinat4 --name combinat2
VBoxManage modifyvm combinat4a --name combinat4

# repeat steps as for bsd1 above to format and mount disk as /cassandra
done



----

OK, now that I know how I'm going to setup things up for the initial
release, and have sufficient disk space, back to learning about
ansible!

Aug 29, 2012:
------------

It's 5:08pm.  I will now quickly check if there any obvious things
better than ansible, but if not, I'll start setting up my
infrastructure using it.

I'm just going to go with ansible. Why?  Because it is actually very
close to what I already wrote with my admin.py, at least in spirit.
Use ssh, etc.

I setup git@combinat1 as follows:

1. sudo adduser git

2. git@combinat1:~$ git --bare init
Initialized empty Git repository in /home/git/

3. On my laptop:
blastoff:salvus wstein$ git push git@combinat1:. master

4. make .ssh/authorized_keys be the file with keys for
   all the non-sage servers:

blastoff:ansible wstein$ scp authorized_keys git@combinat1:.ssh/

5. Change shell to git-shell for security purposes:
   chsh -s /usr/bin/git-shell git

6. test with ansible

I have to change this in /etc/ssh/ssh_config:

   StrictHostKeyChecking no

on every host, so it doesn't ask for the ssh key the first time.

  ansible combinat4.salv.us -m shell -a "echo 'StrictHostKeyChecking no' >> /etc/ssh/ssh_config" -u salvus -s -K


---

First thing to do on Aug 30:

   - finish adding strict host checking and authorized keys even for the sage servers; we want them
     to be up to date with the VPN, etc.


August 30, 2012:

Goal for today: nail down control of infrastructure.

More precisely, type password once and do all of following:

   * ability to update salvus git repo on all nodes easily

   * start sage servers

   * start nginx, haproxy,


   * start cassandra

   * monitor to see what is running or not

   *


---

OK, after using ansible more, I do not like it.  But there are things
I like about it.  However, those things are all really paramiko
things, so I'm getting rid of ansible from salvus, and just adding
paramiko, then finishing my admin.py script.


---

My tinc vpn is still not in great shape.  I really need to make the
configuration much more centralized and systematic, e.g., all the
tinc.conf files for all nodes need to be in the repo.

I've been having trouble on my laptop since I didn't have
   TCPonly = yes
set in tinc.conf.

I want to install paramiko everywhere....

-----

OK, I need to do this ... TODAY:

 [ ] start nginx running on several nodes automatically
... etc.

HOW.

 Problem 1: How to get status of nginx.  There is a pidfile.


Aug 31, 2012:
-------------
  cd salvus/salvus/data/secrets/
  scp -r wstein@salvus0.salv.us:/Users/wstein/salvus/salvus/data/secrets/salv.us .
  scp wstein@salvus0.salv.us:/Users/wstein/salvus/salvus/data/secrets/tornado.conf .

Sept 1, 2012:
-------------

I'm trying to deploy now!  It'll be a lot of debugging of every component...

Obvious todos for today:

 [x] change firewall rules for servedby1, so it can take http/https traffic
 [x] godaddy: point salv.us as combinat and servedby1
 [x] apt-get upgrade vm machines -- on general principal
 [x] http://salv.us is the normal website served by apache; start on servedby1 also

---

Went to breakfast with Tish and did work on the vert ramp for 2 hours and cleaned house.

---

until 5:30
 [x] get cassandra setup and running on cluster

------

5:30-8:30: went skateboarding and hanging out with kids at Jefferson

------

9:00pm:

 [ ] protect cassandra nodes as follows:
     use iptables to make a rule so all incoming packets from interfaces that are
     aimed at the cassandra ports are blocked, unless they come from one of a white-listed
     group of nodes.  These will be exactly the tornado nodes.
     This way we still only need one VPN!

Goal: block incoming packets to Cassandra default ports: 7000, 7001, 9160 except
from a very specific list of hosts, namely all hosts running tornado.

ufw deny 7000,7001,9160

Sept 2, 2012:
-------------

 [x] I'm very confused about cassandra, due to *nothing* in the salvus keyspace working -- I get
     "Unable to complete request: one or more nodes were unavailable."
     no matter what I do.  Why?

     - If I create a new columnfamily in the same keyspace, it fails:

      cqlsh:salvus> create columnfamily foo (key varchar primary key, value varchar);
      cqlsh:salvus> update foo set value='stein' where key='william';
      Unable to complete request: one or more nodes were unavailable.

     - if I create a new keyspace and table in it, things work fine:

cqlsh:salvus> CREATE KEYSPACE test2 WITH strategy_class = 'NetworkTopologyStrategy' AND strategy_options:DC0 = 1 AND strategy_options:DC1 = 1 and strategy_options:DC2 = 1;
cqlsh:salvus> use test2;
cqlsh:test2> create columnfamily foo (key varchar primary key, value varchar);
cqlsh:test2> update foo set value='stein' where key='william';
cqlsh:test2> select * from foo;
 KEY     | value
---------+-------
 william | stein

... And I made the whole schema and it seems to work fine?!

Maybe I mucked things up by accidentally using CQL2?

Delete everything and restart:

import admin, deploy; reload(admin); reload(deploy); h = deploy.hosts; s=deploy.services
h.ping('cassandra')
s.stop('cassandra')
s.status('cassandra')
h('cassandra', 'rm -rf /home/salvus/salvus/salvus/data/cassandra-0', timeout=10)
s.start('cassandra')
s.status('cassandra')

---

cassandra.init_cassandra_schema()
c = cassandra.cursor()
c.execute('select * from services')
# BOOM!
   OperationalError: Unable to complete request: one or more nodes were unavailable.


How to change keyspace options.

Use cassandra-cli (!):

$ cassandra-cli -h servedby1.salv.us -k salvus
[default@salvus] update keyspace salvus with strategy_options={DC0:3, DC1:3, DC2:3};
50c04637-08eb-38a7-94bc-c84eeb0b82ac
Waiting for schema agreement...
... schemas agree across the cluster
[default@salvus]

This does work.  This fixes the problem, strangely, and much to my surprise.

Moreover, this sucker is frightenlying DURABLE.  I just tried killing 2 out of 3 of the nodes,
and everything "just works". Amazing.


 [x] I see "Cluster: Test Cluster" when I do "describe cluster".  This should be "salvus"!

 [x] protect cassandra nodes as follows:
     use iptables to make a rule so all incoming packets from interfaces that are
     aimed at the cassandra ports are blocked, unless they come from one of a white-listed
     group of nodes.  These will be exactly the tornado nodes.
     This way we still only need one VPN!

Goal: block incoming packets to Cassandra default ports: 7000, 7001, 9160 except
from a very specific list of hosts, namely all hosts running tornado.

>>> import admin, deploy; reload(admin); reload(deploy); h = deploy.hosts; s=deploy.services
>>> ','.join(h.ip_addresses('tornado cassandra laptop'))
10.38.1.5,10.38.1.7,10.38.1.10,10.38.1.1,10.38.1.11

ufw reset
ufw allow
ufw allow proto tcp from 10.38.1.10 to any port 7000,7001,9160
ufw deny 9160
ufw deny 7000
ufw deny 7001
ufw enable
ufw status


So it's possible...

http://www.datastax.com/docs/1.1/references/firewall_ref

What I want for each cassandra node (which in some cases is also other services except sage):

0. Reset

  ufw --force reset

1. Allow ssh, nginx, http, stunnel, tornado access from anywhere, which means anywhere in VPN.

  ufw allow 22
  ufw allow 80
  ufw allow 443
  ufw allow 5000
  ufw allow 8000
  ufw allow 8080

2. Allow all access from salvus0 (master control), tornado, and cassandra nodes.

  ufw allow from 10.38.1.1; ufw allow from 10.38.1.5; ufw allow from 10.38.1.7; ufw allow from 10.38.1.10; ufw allow from 10.38.1.11

3. Deny everything else.

  ufw deny proto tcp to any port 1:65535; ufw deny proto udp to any port 1:65535

4. Start it

  ufw --force enable

[x] DAMN -- somehow I managed to firewall myself out of servedby1 anyways!  Time to learn how to get
a console on those boxes...  I will attempt with MS windows.

It turns out using Windows + Firefox *WORKS*.   Nothing else works, even IE....  To get the cursor out
of the VMRC console, hit "control+right_alt/option".


  [x] OK, make activating the above firewall automatic.

 [x] similar firewall for sage nodes:
      * accept 22 from everywhere
      * accept 6000 (the sage server port)  from all tornado nodes  and from salvus0
      * deny everything else
      * deny all outgoing

 [x] commit and gitpush to all nodes...

 [x] start stunnel's

 [x] make it so haproxy auto-determines nginx and tornado nodes, so they don't have to be passed in -- more precisely, the admin program finds this info at runtime.

 [x] get haproxy setup and running on cluster

 [x] get nginx setup and running on cluster

 [x] get tornado setup and running on cluster:

   -- I forgot about the second inter-tornado-server communications tcp port.  The firewall is blocking it.
      But also, even with that disabled, it isn't working.
      Oh, 7000 is the Cassandra port, isn't it?  Yep.
      So... I'll make an option to pass in the port to Tornado, and have the port be 5001.

 [x] database is not working again !
     I'm going to try getting rid of "data centers", since after all with only one node per, we get *no* advantage yet.

 [x] get sage setup and running on cluster




Sept 3, 2012:
-------------

I'm pretty excited to wake up and see that salvus is still actually
working, after I turned it on yesterday.

----------------------------

 [x] sage machines don't have DNS so they can't lookup tinc network.  for now, unblock dns.

 [x] major security issue: make sure Sage/tornado servers *only* bind to VPN address!

 [x] make a fully working local deployment: OS X only

 [x] fixed that very annoying sage_server bug where computation doesn't terminate, even though it does; disable monitor.py as pointless

Next up

 [x] https://salv.us doesn't work in safari or opera
 [x] fix the damned SSL cert!
 [ ] make a fully working local deployment: 2 linux vm's
 [ ] update sockjs-tornado: https://github.com/mrjoes/sockjs-tornado
 [ ] delete files created by running sage process:
print os.popen('hostname').read()
open('/tmp/foo','w').write('a'*1000000)
print os.popen('ls -lh /tmp/').read()
 [ ] quota on sage process
 [ ] when fail to connect to sage server (i.e., maybe it is down), should try another, unless there are none left!


Sept 4, 2012:
------------

For better or worse, I just totally redid the VPN right now that all ports are opened. :-)

git pull https://github.com/williamstein/salvus.git
williamstein

admin.tinc_conf('servedby1',...)

vi data/local/etc/tinc/tinc.conf
ConnectTo = bsd0
ConnectTo = combinat0
ConnectTo = servedby1

killall tincd
/home/salvus/salvus/salvus/data/local/sbin/tincd -D -d 3
----

starting everything again seemed flakie.  ick.

---

Now to try killing 2 out of 3 stunnel's and see what happens.

---

The non-local network speed is completely killing evaluation.
I need to make it so only the local cassandras are used, and
we don't make a new connection each time.
Likewise with sage compute.  Basically we need to implement the snitch,
since without that we're in pain.

[ ] Thought: i wonder if my websocket issues are related to
the haproxy load balancing alg?

...

[x] i just got obsessed with the ssl cert issue for a minute and found this site:

http://www.webhostingtalk.com/showthread.php?t=1038061

I think it means that I need to put the contents of gd_bundle.crt in
nopassphrase.pem, in the right order.  Wow that worked!  I just put
gd_bundle.crt at the end of nopassphrase.pem, and we're good to go!!


2012-09-04 at 10:37pm.

done * I'm going to move the Time: line in the UI to the top bar.
 * I might make the output expand in mobile mode, if I have the energy.  (nope!)
 * I might add a javascript spinner animation when the computation runs, if I have the energy.


Email to Jason Grout about architecture:
-----------------------------------------


BUG: sage server We should not allow incoming traffic to port 6000 even from the same machine!!!

salvus@bsd2:~/salvus/salvus$ sage -python sage_server.py -c -p 6000 --hostname bsd2.salv.us
PID = 17184
sage [0]: 2+3
5
sage [1]:
Exiting Sage client.

Or in salvus:
print os.popen('echo "2+2" | sage -python sage_server.py -c -p 6000 --hostname bsd2.salv.us').read()

Maybe?
import os, socket
print socket.gethostname()
os.system('echo "2+2" | sage -python /tmp/sage_server.py -c -p 6000 --hostname `hostname`.salv.us > /tmp/a')
print "out=", open('/tmp/a').read()


On Tue, Sep 4, 2012 at 9:00 AM, Jason Grout <jason.grout@drake.edu> wrote:
> On 9/3/12 8:39 PM, William Stein wrote:
>>
>> I did some design work and evaluation of infrastructure, then made choices
>> and implemented things. The technical aspects of design have changed
>> substantially from what I showed you in June, though its fundamentally
>> similar. Here are the main software components I'm using: * Tinc -- VPN *
>> Stunnel -- SSL termination * Haproxy -- load balancing and proxy server *
>> Cassandra -- database * Nginx -- static content * Tornado -- websocket
>> server; messaging; dynamic content * Sage -- math Also, I'm using *
>> Protobuf2 -- binary message format * Paramiko -- configuration management
>> (paramiko is an ssh implementation in Python)
>
>
> (yep; we're using paramiko to launch Sage processes and provide the HMAC
> initial key for messages.)
>
>
>>
>> Sage server processes with the library pre-imported runs as root in
>> completely separate virtual machines from anything else. When a server
>> receives a connection, it forks, and drops privileges to a *random*
>> user id (there is no need to actually create the user).   I haven't
>> setup automatic quotas in my current deployment yet, but I had that in
>> a previous version and will soon.          I had implemented the sage
>> server using SSL and passwords, but completely removed that.  Instead,
>> the server machine is on a VPN and has a firewall (setup using ufw)
>> that only allows packets in from a select white list of other machines
>> on the same VPN.
>
>
> That's interesting dropping to a user without creating an account. Do you
> clean up after that account after a computation session?

Yes. They can only create a limited number of files under /tmp and
those all get deleted when their session terminated.  The current
version of the code doesn't actually do the cleanup, though some
version I wrote this summer does.

> As for VPN: We had talked about having server machines talking directly with
> the client (e.g., websockets)---is that forwarded through the VPN, or are
> the websocket and dynamic data messages allowed to any computer from the
> server?

The processes running Sage *only* communicate with a separate computer
(on the same vpn) that is running a Tornado server.   The Tornado
server then talks
to the clients (albiet, through haproxy + stunnel).  So the architecture is:

   (Sage) <---> (Tornado) <---> (Haproxy)  <---> (Stunnel)  <---> (Client)
                                /|\
                                 |
                                \|/
                         (Cassandra)

I intend to support three types of clients:

       1. sockjs (so basically websocket or long polling)

       2. a classical JSON HTTP api, with polling, though I'll
severely restrict the amount of polling per second allowed.

       3. a standard SSL TCP socket .

Type 1 will be preferred for web browsers.    Type 2 will allow for
simplistic clients, easy use from urllib2 (say), and it should even be
possible to make it worth with something like lynx (no javascript),
though obviously that will require the user to click a link to see the
progress of their computation.   Type 3 will be for the command line
client, the desktop GUI application, and maybe a mobile app.


> I can imagine that the control messages (like start a new Sage
> instance) would only be allowed through the VPN.

When my script starts the Sage server process it also configures a
very restrictive firewall for that machine.  It blocks all incoming
traffic except for traffic on one port from the machines on the VPN
that are running tornado, and ssh so I can manage the machine.   It
blocks almost all outgoing traffic (some outgoing traffic is required
to stay on the VPN).

> What happens if a server
> is compromised? Can it compromise the rest of the network through the VPN
> easily?

It could disable the firewall, then send bogus messages to tornado
servers.  These would be annoying, but I don't think they would cause
very serious trouble or lead to information leakage.  The tornado
servers know that the machine is a
sage server, so it can't send messages as if it were not a sage
server.  It also can't change it's ip address or identification on the
VPN, because it would have to put new keys on the vpn server nodes.

> In other words, do you absolutely trust any message over the VPN is
> valid, without authentication?



>
>
>
>
>>
>> I had to switch from PostgreSQL to Cassandra because of my longterm
>> estimated storage requirements, and my design goals regarding salvus
>> being able withstand certain types of hardware and network failures.
>> For similar reasons, I switched from OpenVPN (which has a single point
>> of failure) to Tinc, which is a p2p vpn with no single points of
>> failure.  With my current *design*, you can just randomly kill quite a
>> few machines, and the system just stays up.
>>
>> Also, as I explained to you, I had planned to implement storage of
>> user data on nodes, replicated using git and coordinates using
>> PostgreSQL... but it turns out that Cassandra already does everything
>> I was planning to implement and much, much more regarding distributed
>> storage of data.  So that was another reason to use Cassandra instead
>> of PostgreSQL.  I'm also not using memcached anymore, since Cassandra
>> -- properly configured -- does much the same thing, with much less and
>> cleaner client code needed.
>
>
> Using git to synchronize things, we also had versioned data.  Are you still
> trying to version data in Cassandra?  Are you storing *all* user data/files
> in Cassandra?  I'm not familiar enough with Cassandra to really have a feel
> for how data needs to be structured, but it seems like you need to basically
> implement GridFS for Cassandra.

No.  I will simply be storing git bundles in Cassandra.  All data is
still versioned, and I do not have to implement 'GridFS'.


2012-09-05 at 12:36pm.
----------------------

I'm doing budget work for a grant right now.   We have:

  (10648*1.269 + 500)/.8 = 17515.3900000000 = amount I need to bring
  in to pay myself and for hosting costs

I will thus make getting $20K/month my goal for income from Salvus for this year.

At $99/year (or $10/month), that would require 2400 users.  That's my
target for paying customers for year 1.


Sept 18, 2012:
--------------
    [ ] implement a private cloud setup on my laptop with 3 vm's
        - setup rados cluster
        - setup ceph and test
        - try with ganeti, unless it is too hard; if so, switch to a java stack?

    [ ] implement private cloud setup on 01-04salvus machines.
    [ ] order four new machines for UW tower using DOD + Sage Foundation + combinat

    [ ] admin script: make it do the ssh stuff in parallel, somehow.
    [ ] expand deploy config to make full use of the new machines.
    [ ] upgrade tornado in salvus -- 2.4 is out!
    [ ] salvus programming:
         [ ] automated sage server disk quotas
         [ ] remove all files after session terminates
         [ ] make choice of sage server determined by speed/success with previous choice
         [ ] make choice of database determined by speed/success with previous choice
         [ ] update jquery
         [ ] desktop -- codemirror 2 editor
         [ ] update jquery-mobile
         [ ] mobile -- growing output
         [ ] optional user login/accounts: local/google/facebook -- terms of usage agreement
         [ ] nice way to browse/delete from/rank/publish past computations
         [ ] communication between tornado servers sending the proto messages to correct recipients
         [ ] way to send generic json message from sage process to browser; ensure fast
         [ ] persistent sessions (options appears on login)
         [ ] embed ability to compute via "mashup" in other webpages via a javascript snippet.


------------


----------------------------------------------------

Ceph is starting to finally do something useful...  This is a
nontrivial piece of software!

1. Install newest ceph/rados:

wget -q -O- https://raw.github.com/ceph/ceph/master/keys/release.asc | sudo apt-key add -
echo deb http://ceph.com/debian/ $(lsb_release -sc) main | sudo tee /etc/apt/sources.list.d/ceph.list
sudo apt-get update && sudo apt-get install ceph

2. Config:
   - make /dev/sdb1 with a btrfs formated filesystem on each node
   - do *NOT* put anything in /etc/fstab!

3. I made a file ceph1.conf:
   - on salvus1 in /var/lib/ceph.

Here is the one I used here:

#----------------------------------------

[mon]
    mon data = /var/lib/ceph/mon

[mon.1]
   host = salvus1
   mon addr = 192.168.56.150:6789

[mon.2]
   host = salvus2
   mon addr = 192.168.56.151:6789

[mon.3]
   host = salvus3
   mon addr = 192.168.56.152:6789


[mds]
  mds data = /var/lib/ceph/mds

[mds.1]
  host =salvus1

[mds.2]
  host =salvus2

[mds.3]
  host =salvus3

[osd]
    osd data = /var/lib/ceph/osd
    osd journal = /var/lib/ceph/osd/journal
    osd journal size=100; journal size, in megabytes

[osd.0]
    host = salvus1
    btrfs devs = /dev/sdb1

[osd.2]
    host = salvus2
    btrfs devs = /dev/sdb1

[osd.3]
    host = salvus3
    btrfs devs = /dev/sdb1

#----------------------------------------

4. Setup password-less login as root via ssh from salvus1 to salvus2 and salvus3.

 a. On all nodes -- Edit /etc/ssh/sshd_config to allow root login, then
        /etc/init.d/ssh restart
    Also, set root passwd.

   ssh-keygen -b 2048
   ssh-copy-id salvus2
   ssh-copy-id salvus3

5. Push out config:

    mkcephfs -a -c /var/lib/ceph/ceph1.conf --mkbtrfs

6. Try it:

    root@salvus1:~# mount -t ceph `hostname`:/ /ceph
    root@salvus1:/# df -h |grep mnt
    192.168.56.150:/   24G  2.8G   22G  12% /ceph

and similar from other machines.

7. WTF?  I made 100 files on salvus3 and have:

root@salvus2:/ceph/z# time ls
real	0m3.998s

repeatedly.  Why is it so incredibly slow?  The network connection is very fast -- 24MB/s.

I also tried copying a 100MB file to /ceph, and it takes
forever... basically hanging/killing the whole thing!  This is
ridiculous.

I tried turning of noatime, and that fixed nothing.
Transfer of a single 100MB file takes the time it should.
But ls of a bunch of small files is insanely slow.

The slowness problem completely vanishing by using ceph-fuse:

    mkdir /ceph
    ceph-fuse -m `hostname` /ceph

This is fast.

----

Yeah!  Now back to:

  http://blog.bob.sh/2012/02/basic-ceph-storage-kvm-virtualisation.html

  "A Basic Ceph Storage & KVM Virtualisation Tutorial"

Making snapshots works:

mkdir /ceph/test
cd /ceph/test
touch a b c
mkdir .snap/my_snapshot

Trying out RBD:

   modprobe rbd  # necessary!
   rbd create mydisk --size 150 # in megabytes
   rbd list
   echo "192.168.56.150 name=admin rbd mydisk" > /sys/bus/rbd/add
   ls -lh /dev/rbd/rbd/mydisk
        lrwxrwxrwx 1 root root 10 Sep 18 20:20 /dev/rbd/rbd/mydisk -> ../../rbd0
   mkfs -t ext4 /dev/rbd/rbd/mydisk
   mkdir /mnt/mydisk
    mount /dev/rbd/rbd/mydisk /mnt/mydisk

Benchmark:

  python -c "for i in range(1000): open(str(i),'w').write(str(i))"

Is fast!

  python -c "open('a','w').write('1'*(10**8))"

Now try to make a KVM VM:

   rados mkpool vm_disks
   qemu-img create -f rbd rbd:vm_disks/box1_disk1 1G

# disk.xml:

<disk type='network' device='disk'>
  <source protocol='rbd' name='vm_disks/box1_disk1'/>
  <target dev='vda' bus='virtio'/>
</disk>

###########################

Too slow network on my laptop because of network...

-----------------

Setup on 01-02salvus in server room with Rados and Ganeti.

1. Setup password-less root around

root@01salvus:~# ssh-keygen -b 2048
root@01salvus:~# ssh-copy-id 02salvus.math.washington.edu
root@01salvus:~# ssh-copy-id 03salvus.math.washington.edu
root@01salvus:~# ssh-copy-id 04salvus.math.washington.edu

Making the /etc/hosts file contain:

128.95.224.230 01salvus 01salvus.math.washington.edu
128.95.224.232 02salvus 02salvus.math.washington.edu
128.95.224.237 03salvus 03salvus.math.washington.edu
128.95.224.240 04salvus 04salvus.math.washington.edu


On salvus01 and salvus02:
-------------------------

Making /etc/hostname file contain:
   02salvus.math.washington.edu

Then type "service hostname restart"

2. Create an LVM logical volume to use for Rados:

  lvcreate --name ganeti_test --size 20G lvm
  lvdisplay
  lvscan

Do the same on 02salvus:

  lvcreate --name ganeti_test --size 20G 02salvus   # i named the lvm differently

3. Create ceph config file:

/var/lib/ceph/ceph0.conf

[mon]
    mon data = /var/lib/ceph/mon
[mon.1]
   host = 01salvus
   mon addr = 128.95.224.230:6789
[mon.2]
   host = 02salvus
   mon addr = 128.95.224.232:6789

[mds]
  mds data = /var/lib/ceph/mds
[mds.1]
  host = 01salvus
[mds.2]
  host = 02salvus

[osd]
    osd data = /var/lib/ceph/osd
    osd journal = /var/lib/ceph/osd/journal
    osd journal size=500; journal size, in megabytes
[osd.1]
    host = 01salvus
    btrfs devs = /dev/lvm/ganeti_test
[osd.2]
    host = 02salvus
    btrfs devs = /dev/02salvus/ganeti_test

4. Push out config and make filesystem:

   mkcephfs -a -c /var/lib/ceph/ceph0.conf --mkbtrfs

5. Test it out:
   # on each of 01salvus and 02salvus:

   /etc/init.d/ceph start
   mkdir /mnt/ceph; ceph-fuse -m `hostname` /mnt/ceph


root@01salvus:/var/lib/ceph# df -h |grep ceph
/dev/mapper/lvm-ganeti_test   20G  504M   18G   3% /var/lib/ceph/osd
ceph-fuse                     40G  5.1G   35G  13% /mnt/ceph

---------

Success.  Now moving on to ganeti.

GANETI test install:

1. Prereq:
  # apt-get install kvm python-paramiko  iputils-arping ndisc6 python-pyparsing  python-pyinotify python-pycurl socat qemu ganeti2 ganeti-htools ganeti-instance-debootstrap

   root@02salvus:/mnt/ceph# kvm --version
   QEMU emulator version 1.0 (qemu-kvm-1.0), Copyright (c) 2003-2008 Fabrice Bellard

2. Configuration:

   - fix the /etc/hostname file to the fqdn, etc.

   - I'm not doing any network config.  It seems that Ubuntu + kvm
     already does that and creates virbr0.

   - I'm going to start by trying the older ganeti-2.4.5 that comes with
     Ubuntu.  If that fails, I'll try ganeti-2.6.0 from source, which is here:
          http://code.google.com/p/ganeti/downloads/list
          wget http://ganeti.googlecode.com/files/ganeti-2.6.0.tar.gz

   - FAIL: it turns out that RADOS support is new; it's not in ubuntu's ganeti.
     So, uninstall that and build from source.

apt-get remove ganeti2 ganeti-htools
wget http://ganeti.googlecode.com/files/ganeti-2.6.0.tar.gz
cd ganeti-2.6.0/
./configure --localstatedir=/var --sysconfdir=/etc
make -j20
make install

Logout, then login, then:

root@01salvus:~# gnt-cluster  --version
gnt-cluster (ganeti v2.6.0) 2.6.0



3. Initialization:

Lots of crap, but see the line at the very, very end of this step.

   - I uncommented a line in /etc/default/ganeti-instance-debootstrap
       EXTRA_PKGS="acpi-support-base,console-tools,udev,linux-image-amd64"

   - CLUSTERNAME:   Huh?
      I added *this* to /etc/hosts on 01salvus and 02salvus,
      and made the CLUSTERNAME "ganeti".

# this ip is something I don't have allocated to me, but it is in my range, so
# I'm "taking" it.

128.208.160.190 ganeti

root@01salvus:/etc/default# gnt-cluster init ganeti

Oops:
Failure: prerequisites not met for this operation:
error type: wrong_input, error details:
Error: volume group 'xenvg' missing
specify --no-lvm-storage if you are not using lvm

gnt-cluster init --enabled-hypervisors=kvm --vg-name=lvm --no-drbd-storage --master-netdev=virbr0 ganeti


gnt-cluster destroy --yes-do-it
# fail, so

rm -rf /var/lib/ganeti/; mkdir /var/lib/ganeti


   gnt-cluster init --enabled-hypervisors=kvm  --no-drbd-storage --no-lvm-storage --master-netdev=virbr0 --ipolicy-disk-templates=rbd ganeti

<ARGH>

No dice.  This Ganeti is seeming really frustrating.  Given how broken the demo
in the video is, etc., I'm not enthuised.  Also, given that I will at some point
want a hybrid cloud with Amazon, I'm even less enthuised.

The main other options are:

  * CloudStack.
  * OpenNebula
  * Eucalyptus

CloudStack does work with RADOS/Ceph:

   http://ceph.com/docs/master/rbd/rbd-cloudstack/
   "You can use RBD to run instances on in Apache CloudStack.
   This can be done by adding a RBD pool as Primary Storage."
   Then many caveats.
</ARGH>

---------------

Back to trying ganeti...  I had hit this:

  http://theengguy.blogspot.com/2012/06/ganeti-not-working-in-ubuntu-1204.html

as I found looking in:

   /var/log/ganeti/commands.log

I did this as they recommend:

###########################################
aptitude install build-essential dpkg-dev
apt-get source python-pycurl
aptitude build-dep python-pycurl
aptitude install libcurl4-openssl-dev
cd pycurl-7.19.0
perl -p -i -e "s/gnutls/openssl/" debian/control
dpkg-buildpackage -rfakeroot -b
cd ..
dpkg -i python-pycurl_7.19.0-4ubuntu3_amd64.deb
###########################################


Now I hit:

     "Error 60: SSL certificate problem"

I note that I completely misunderstood the "--master-netdev" parameter, and this could be the problem.

   gnt-cluster init --master-netdev=eth0 --enabled-hypervisors=kvm  --no-drbd-storage --no-lvm-storage  --ipolicy-disk-templates=rbd ganeti


-----------

I'm going to try my own python install from scratch in /root, since this is just
messed up.

 cd
 mkdir local
 wget http://python.org/ftp/python/2.7.3/Python-2.7.3.tar.bz2
 tar xf Python-2.7.3.tar.bz2
 cd Python-2.7.3/
 ./configure --prefix=/root/local
 make install

# Modify /root/.bash_profile so /root/local/bin is in front of path
# and logout/login/confirm.

   export PATH=$HOME/local/bin:$HOME/local/sbin:$PATH

# Install easy_install:

 wget http://pypi.python.org/packages/source/s/setuptools/setuptools-0.6c11.tar.gz
 tar xf setuptools-0.6c11.tar.gz
 cd setuptools-0.6c11/
 python setup.py install

# Now install deps:

 easy_install pyopenssl paramiko pycurl pyparsing pyinotify  simplejson

# Install ganeti into this Python

wget http://ganeti.googlecode.com/files/ganeti-2.6.0.tar.gz
tar xf ganeti-2.6.0.tar.gz
cd ganeti-2.6.0/
./configure --localstatedir=/var --sysconfdir=/etc --prefix=/root/local
make
make install


gnt-cluster init --master-netdev=virbr0 --enabled-hypervisors=kvm  --no-drbd-storage --no-lvm-storage  --ipolicy-disk-templates=rbd ganeti

To get it to actually use my new python, I have to install my replace command and do:

root@01salvus:~/local/sbin# replace /usr/bin/python "/usr/bin/env python" *

# same in /root/local/lib/ganeti

... and we have exactly the same problem still.

Report of the same symptom:

  https://groups.google.com/forum/?fromgroups=#!topic/ganeti/P7hWx_GLAbY

And... indeed, that was the problem!!!
The problem is that there is an old ganeti process running from the last attempt.
Killing that, cleaning everything up, etc., and everything is fine.

I ran using the system-wide version, and all is good, so I will not be
using the version in /root/.

Hmm.  The master device really is on the web and using "128.208.160.190".

THIS WORKED:

gnt-cluster init --master-netdev=virbr0 --enabled-hypervisors=kvm  --no-drbd-storage --no-lvm-storage  --ipolicy-disk-templates=rbd ganeti

--
4. Join other node to cluster:

 gnt-node add 02salvus.math.washington.edu

I had *problems* with this until I removed the 02salvus name from the /etc/hosts file!
I.e., I *had* to remove the short version!!! Crazy.

root@01salvus:/usr/local/lib/python2.7/dist-packages/ganeti# gnt-node list
Node                         DTotal DFree MTotal MNode MFree Pinst Sinst
01salvus.math.washington.edu      -     -  62.9G  1.0G 61.7G     0     0
02salvus.math.washington.edu      -     -  62.9G  1.0G 61.7G     0     0

--

5. Actually use it?

root@01salvus:/usr/local/lib/python2.7/dist-packages/ganeti# gnt-os list
Can't get the OS list

This is because I didn't do this:

  http://docs.ganeti.org/ganeti/current/html/install.html#installing-the-operating-system-support-packages

which is explained wrong!!!!!  What the FUCK.

./configure --prefix=/usr --localstatedir=/var --sysconfdir=/etc --with-os-dir=/srv/ganeti/os
make && make install

This must be done on all nodes.

 cd /tmp
 wget http://ganeti.googlecode.com/files/ganeti-instance-debootstrap-0.9.tar.gz
 tar xzf ganeti-instance-debootstrap-0.9.tar.gz
 cd ganeti-instance-debootstrap-0.9
 ./configure --prefix=/usr --localstatedir=/var --sysconfdir=/etc --with-os-dir=/srv/ganeti/os
  make && make install

---

Now it actually works to do "gnt-os list". !!


Add this to /etc/hosts:
   192.168.122.10 test1

Now try to create an instance:
   gnt-instance add -n 01salvus:02salvus -o debootstrap -t rbd -s 10G test1

This fails with:

  Hypervisor parameter validation failed on node 01salvus.math.washington.edu: Parameter 'kernel_path' fails validation: not found or not a file (current value: '/boot/vmlinuz-3-kvmU')

So we do this (as suggested at http://geekfun.com/2009/07/14/tips-on-using-ganeti-to-manage-a-kvm-based-virtual-machine-cluster-on-ubunty-jaunty-jackelope-9-04/):

  cd /boot
  ln -s vmlinuz-3.2.0-29-generic vmlinuz-3-kvmU

Next error on trying again with the same input:

  error type: wrong_input, error details:
  OS name must include a variant

# from https://groups.google.com/forum/?fromgroups=#!topic/ganeti/YVk1MBJzR-E


   gnt-instance add -n 01salvus -o debootstrap+default -t rbd -s 4G test1

Next error -- checking bridges on destination node '01salvus.math.washington.edu': Missing bridges xen-br0

   gnt-instance add -n 01salvus -o debootstrap+default --net 0:link=virbr0 -t rbd -s 4G test1

# the docs say "bridge=", but they are wrong.

Fails due to lack of /sys/bus/rbd/devices, so I do

    modprobe rbd

and try again:

It got further, but failed with:

This may result in very poor performance, (re)-partitioning suggested.
I: Retrieving Release
E: Failed getting release file http://ftp.us.debian.org/debian/dists/lenny/Release,

which indeed does not exist.
...


The debootstrap instance is too old, so try with a newer one, on ALL NODES:

 cd /tmp
 wget http://ganeti.googlecode.com/files/ganeti-instance-debootstrap-0.11.tar.gz
 tar xzf ganeti-instance-debootstrap-0.11.tar.gz
 cd ganeti-instance-debootstrap-0.11
 ./configure --prefix=/usr --localstatedir=/var --sysconfdir=/etc --with-os-dir=/srv/ganeti/os
  make && make install


Try again:

  gnt-instance remove test1
  gnt-instance add -n 01salvus -o debootstrap+default --net 0:link=virbr0 -t rbd -s 4G test1

This file shows the install is working:

  root@01salvus:/var/log/ganeti/os# tail -f  add-debootstrap+default-test1-2012-09-19_17_41_42.log


After installing (about 20 minutes?), the instance fails to start:


  Failure: command execution error:
  Could not start instance: Error while executing backend function: 'pool'

This is because it's completely broken for everybody.  Gees.

  https://groups.google.com/forum/?fromgroups=#!topic/ganeti/tV4MVBJyS-Q

I tried applying the one-line patch there and no change...

   gnt-instance startup test1

I'm going to try the latest git repo of ganeti...

If that doesn't work, it's time to go for DRBD, which is clearly much
more well supported.

   git clone git://git.ganeti.org/ganeti.git
   apt-get install autogen automake pandoc python-docutils python-sphinx graphviz
   ./autogen.sh
   ./configure --localstatedir=/var --sysconfdir=/etc
   make && make install

This did not work.  Hmm.

----

What to do.

I should try a local simple storage first before even trying drbd.

Try again with plain with a 1-node cluster:

  gnt-instance remove test1

gnt-node remove 02salvus.math.washington.edu
gnt-cluster destroy --yes-do-it
gnt-cluster init --master-netdev=virbr0 --enabled-hypervisors=kvm  --no-drbd-storage --vg-name=lvm ganeti

  gnt-instance add -n 01salvus -o debootstrap+default --net 0:link=virbr0 -t plain -s 4G test1

This did fully work.

I tried the Ganeti Web Manager and it really sucks.
I'm officially rejecting ganeti as a possible component of salvus.  Oh well.

Options:
  * list here: http://www.linux-kvm.org/page/Management_Tools
  * CloudStack
  * OpenNebula
  * Eucalyptus
  * OpenStack


* the openstack distributed storage: http://www.openstack.org/software/openstack-storage/

Cloudstack on ubuntu 12.04:
  http://www.cloudstack.org/forum/6-general-discussion/11158-302-on-ubuntu-1204.html

---------------

OpenNebula:

Looks like the only big-usage academic/truly free cloud option.

Packages:

  http://dev.opennebula.org/packages/opennebula-3.6.0/Ubuntu-12.04/

wget http://dev.opennebula.org/packages/opennebula-3.6.0/Ubuntu-12.04/Ubuntu-12.04-opennebula_3.6.0-1_amd64.deb
dpkg -i Ubuntu-12.04-opennebula_3.6.0-1_amd64.deb
apt-get install -f -y
----

I'm trying out OpenNebula via a VM manager as explained here: http://opennebula.org/cloud:sandbox:kvm

  ./installer.sh frontend 128.208.160.190

Domain frontend created from /root/frontend.xml

Appliance started, called frontend. The IP is 128.208.160.190

A copy of the frontend VM description file is located at
~/frontend.xml. You can use it to start the frontend again executing:
 $ virsh create ~/frontend.xml


ICK.
--

CloudStack... doesn't seem to support Ubuntu 12.04 and seems weird.

===================

Eucalyptus: looks interesting. Try it.

Following http://www.eucalyptus.com/download/eucalyptus/ubuntu-12


  wget http://www.eucalyptus.com/sites/all/modules/pubdlcnt/pubdlcnt.php?file=/sites/all/files/c1240596-eucalyptus-release-key.pub&nid=878
  apt-key add *eucalyptus-release-key.pub

  echo "deb http://downloads.eucalyptus.com/software/eucalyptus/3.1/ubuntu precise main" > /etc/apt/sources.list.d/eucalyptus.list

  echo "deb http://downloads.eucalyptus.com/software/euca2ools/2.1/ubuntu precise main" > /etc/apt/sources.list.d/euca2ools.list

  apt-get update

  apt-get install eucalyptus-cloud eucalyptus-cc eucalyptus-sc eucalyptus-walrus
  apt-get install  eucalyptus-nc


  /usr/sbin/euca_conf --initialize

￼service eucalyptus-cloud start

 service eucalyptus-cc start
￼service eucalyptus-nc start

root@01salvus:~# netstat -a |grep 8773
tcp        0      0 *:8773                  *:*                     LISTEN
udp        0      0 228.7.7.3:8773          *:*
udp        0      0 01salvus.math.wash:8773 *:*
root@01salvus:~# netstat -a |grep 8774
tcp        0      0 *:8774                  *:*                     LISTEN
root@01salvus:~# netstat -a |grep 8775
tcp        0      0 *:8775                  *:*                     LISTEN

Registering COMPONENTS:

  /usr/sbin/euca_conf --register-walrus --partition walrus --host 01salvus --component walrus-01salvus

￼/usr/sbin/euca_conf --register-cluster --partition cluster01 --host 01salvus --component cc-01salvus

  /usr/sbin/euca_conf --register-sc --partition cluster01 --host 01salvus --component sc-01salvus

￼/usr/sbin/euca_conf --register-nodes "01salvus"

CREDENTIALS:

  /usr/sbin/euca_conf --get-credentials admin.zip
  unzip admin.zip
  source eucarc

WEB INTERFACE:

  ssh -L 8443:localhost:8443 salvus@01salvus

Then visit:

  https://localhost:8443/

Login and password are *both* "admin".


Eucalyptus seems to have thought through security much more clearly
than the other systems.  Bravo!!

The admin interface mostly works, though search is broken.

   root@01salvus:~/eucalyptus# eustore-describe-images
   1107385945 centos      x86_64  2012.07.05     CentOS 5 1.3GB root, Hypervisor-Specific Kernels
   root@01salvus:~/eucalyptus# eustore-install-image -i 1107385945  -b walrus -k "Hypervisor-Specific Kernels"

That actually does download it.  But fails because kernel specified wrong...

I have no idea how to specify k... and yet my second guess works:


   root@01salvus:~/eucalyptus# eustore-install-image -i 1107385945  -b walrus -k kvm

--------------------------------------

My god, this shit is just so heavy.   What about plain libvirt?

apt-get install virtinst virt-viewer

virt-install -n test1 -r 512 -f ./test1.img -s 5 -c ./ubuntu-12.04-server-amd64.iso -w network:virbr0 -m 54:52:00:00:10:01 -v --os-type=linux --noautoconsole

FAIL.

As root:

virt-install -n test1 -r 512 -f ./test1.img -s 5 -c ./ubuntu-12.04-server-amd64.iso -m 54:52:00:00:10:01 -v --os-type=linux --noautoconsole


Hmm, learn about http://virt-tools.org/

This article looks good:  http://www.sysadminworld.com/2012/ubuntu-12-04-kvm-virtualization/

chmod a+rw ubuntu-12.04-server-amd64.iso

virt-install -v --graphics vnc,port=8123 --name test2 --ram 1024 --disk path=/tmp/test2.img,size=4  -c ubuntu-12.04-server-amd64.iso


Try as normal user (salvus) and fail.
Found this page: http://forums.gentoo.org/viewtopic-t-925976-start-0.html
Says to do:

  gpasswd -a qemu kvm

ie add "qemu" to kvm group, due to privilage issues.
That does not help.

What did fix the problem is doing:

root@01salvus:/dev# ls -lh kvm
crw-rw---- 1 root kvm 10, 232 Sep 19 21:57 kvm
root@01salvus:/dev# chmod a+rw kvm
root@01salvus:/dev# ls -lh kvm
crw-rw-rw- 1 root kvm 10, 232 Sep 19 21:57 kvm

which is pretty dangerous/evil, at least if this were a big multi-user system, which it isn't.


#  apt-get remove eucalyptus-cloud eucalyptus-cc eucalyptus-sc eucalyptus-walrus  eucalyptus-nc



---

Have to spoof mac for wifi router to let me back!

sh-3.2# ifconfig en0
en0: flags=8863<UP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST> mtu 1500
	ether 10:40:f3:91:6b:26
	inet6 fe80::1240:f3ff:fe91:6b26%en0 prefixlen 64 scopeid 0x4
	inet 169.254.76.216 netmask 0xffff0000 broadcast 169.254.255.255
	media: autoselect
	status: active



...

OK, after a very long day of this, I've learned one thing:

   USE libvert directly!

There is no significant value added for me in using any of the private cloud systems.

By using libvirt I can even using Ceph/RDB if I want and it is fast enough...

Now I need to properly learn kvm and libvirt.

---------------

Sept 20, 2012:
-------------

virt-install + rados block device?

Trying stuff from here:  http://blog.bob.sh/2012/02/basic-ceph-storage-kvm-virtualisation.html

  modprobe rbd
  rados mkpool vm_disks
  qemu-img create -f rbd rbd:vm_disks/test1 4G

# get around lack of functionality in virt-install:
root@01salvus:/tmp/virt# pwd
/tmp/virt
root@01salvus:/tmp/virt# more disk.xml
<disk type='network' device='disk'>
  <source protocol='rbd' name='vm_disks/test1'/>
  <target dev='vda' bus='virtio'/>
</disk>




Also, make sure to enable caching or it will be dog slow!
   See http://ceph.com/wiki/QEMU-RBD

Too frustrating with shitty internet!




Wait:

   http://www.osrg.net/sheepdog/

   "Sheepdog is a distributed storage system for QEMU/KVM. It provides
   highly available block level storage volumes that can be attached
   to QEMU/KVM virtual machines. Sheepdog scales to several hundreds
   nodes, and supports advanced volume management features such as
   snapshot, cloning, and thin provisioning."

   Heh, that's what I *need*!!!


Try it out:

   sudo su
   aptitude install corosync libcorosync-dev liburcu-dev
   git clone git://github.com/collie/sheepdog.git
   cd sheepdog
   ./autogen.sh
   ./configure
   make
   make install

   lvcreate --name sheep_test --size 20G lvm
   lvscan
   mkfs.btrfs /dev/lvm/sheep_test
   mkdir /mnt/sheep
   mount /dev/lvm/sheep_test /mnt/sheep
   df -h
   sheep /mnt/sheep

It just doesn't work at all.  aRgh.

#

I will now plan out the functionality I want with VM's, exactly.
----------------------------------------------------------------

VM types:
=========
Salvus:
   * sage server -- small, but LOTS of ephemeral machines
   * web: tornado, etc. -- small; ephemeral
   * cassandra server -- lots of *disk* that must not be redundant or replicated
                      -- several on each node
                      -- might as well use raw lvm

Sage build machines:
   * not my problem

Sage web services:
   * sagemath.org, etc.  very stable

Sage compute services:
   * ?


For each salvus machine, at least, I need to:

    - decide on machine name and ip address ---> put in cassandra db
    - generating private and public keys --> publish public key (in cassandra db)
    - adding public keys to all tinc servers --> (tell them to update from db)
    - making sure the machine knows its private keys --> create a tiny img with the key and any other config info
    - decide on which node to start the machine --> put in cassandra db
    - copy/update any files to that machine's local storage --> paramiko
    - start machine --> paramiko

Also we will have to monitor machines:

    - for each machine listed in cassandra db:
        - ping it and record time (or timeout)
        - use paramiko to login and check on top status

Stopping machines:

    - change cassandra db

-------------------

At the core, I need to write something like this:

class VirtualMachine(object):
    def __init__(self, host, ip_address, machine_type):
    def start(self):
    def stop(self):
    def restart(self):
    def status(self):

-----

I just looked at the openstack website again, and watched a video, and it looks
fantastic featurewise.  It's written mostly in python.  Maybe give it a try:

http://www.hastexo.com/resources/docs/installing-openstack-essex-20121-ubuntu-1204-precise-pangolin

No, it is soooo complicated!  And probably not mature.

---------



Sept 22, 2012:
==============

This is a session of playing with libvirt/kvm, just to get comfortable.  It's nice!

   [ ] setup a template libvirt VM for running other services
   [ ] setup a template libvirt ubuntu VM for running Sage

Started these installs at 6:45am

# base salvus image
virt-install --os-variant=ubuntuprecise -v --graphics vnc,port=8122 --name salvus --vcpus=2  --ram 4096 --disk path=salvus.img,size=4 -c ubuntu-12.04-server-amd64.iso

 virsh -c qemu:///session undefine salvus





# salvus_sage
virt-install -v --graphics vnc,port=8124 --name salvus_sage --vcpus=16 --ram 8192 --disk path=salvus_sage.img,size=12 -c ubuntu-12.04-server-amd64.iso

ssh -L 8124:localhost:8124 salvus@01salvus.math.washington.edu

XML definitions of machines are here:

$HOME/.libvirt/qemu/

virsh -c qemu:///session start salvus_web
virsh -c qemu:///session start salvus_sage
virsh -c qemu:///session list

To ssh into a machine:

virsh -c qemu:///session qemu-monitor-command --hmp salvus_sage 'hostfwd_add ::2222-:22'   # NOT persistent?!
ssh localhost -p 2222

I'm going to try to install sage from source on there -- see how long it takes, etc.

I'm also going to try a build test on geom -- I hope it works!


Sept 23, 2012
============

I tried benchmarking factorial(10^7) in a new virt-install'ed vm, and
it was 14 seconds, versus around 15 seconds not in the VM.

The other main issue is how incredibly slow the network is.  Maybe
that is easily fixed by re-installing the OS, since I seriously tested
a lot on here.  On geom -- using kvm -- the network is fine.

Just curious if this was because of running using qemu but without kvm?

It's kvm.  Weird.

My next guess is it is because limited hardware of the CPU is made available to the
machine, so optimizations aren't done.

I'll start a test of this.

In the vm, we have:

salvus@salvus-sage:~$ cat /proc/cpuinfo |grep flags|tail -1
flags		: fpu de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 syscall nx lm nopl pni cx16 popcnt hypervisor lahf_lm svm abm sse4a

On the host we have:
salvus@01salvus:~$ cat /proc/cpuinfo |grep flags|tail -1
flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc extd_apicid aperfmperf pni pclmulqdq monitor ssse3 cx16 sse4_1 sse4_2 popcnt aes xsave avx lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 nodeid_msr topoext perfctr_core arat cpb npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold


The flags are restricted for live migration.   But we will only migrate between opterons, so...


virsh capabilities

<capabilities>

  <host>
    <uuid>44454c4c-4c00-1047-8030-b1c04f395631</uuid>
    <cpu>
      <arch>x86_64</arch>
      <model>Opteron_G3</model>
      <vendor>AMD</vendor>
      <topology sockets='1' cores='4' threads='2'/>
      <feature name='wdt'/>
      <feature name='skinit'/>
      <feature name='osvw'/>
      <feature name='3dnowprefetch'/>
      <feature name='cr8legacy'/>
      <feature name='extapic'/>
      <feature name='cmp_legacy'/>
      <feature name='pdpe1gb'/>
      <feature name='fxsr_opt'/>
      <feature name='mmxext'/>
      <feature name='aes'/>
      <feature name='sse4.2'/>
      <feature name='sse4.1'/>
      <feature name='ssse3'/>
      <feature name='ht'/>
      <feature name='vme'/>
    </cpu>

Putting this in the machine's XML had no effect at all.


From the man virt-install page:

 --cpu host
           Expose the host CPUs configuration to the guest. This enables the guest to take advantage of many of the host
           CPUs features (better performance), but may cause issues if migrating the guest to a host without an identical
           CPU.

I'll try a new install with "--cpu host" and see what happens.   I'm going to do this on 03salvus too, to see if the networking issues
is resolved.

virt-install -v --cpu host --graphics vnc,port=8124 --name salvus_sage --vcpus=16 --ram 8192 --disk path=salvus_sage.img,size=12 -c ubuntu-12.04-server-amd64.iso

NOTE: The physical network on all the salvus nodes is way too slow. It's 10MB/s, but should be 100?!
The card is: Broadcom NetXtreme II BCM5716 1000Base-T
Not a problem with combinat using *NOT* the same card.

Exactly this problem: http://www.linuxquestions.org/questions/linux-networking-3/broadcom-nic-slow-854656/

Compiling the driver on 03salvus.

Neat trick here:  http://ubuntuforums.org/showthread.php?t=1870780&page=2

   apt-get install apt-file
   apt-file search linkage.h

But we need:

  apt-get install linux-source

  Reboot!


Trying to build driver, but still hitting the error:

  ln -s /usr/src/linux-headers-3.2.0-31/include/asm-generic /usr/src/linux-headers-3.2.0-31/include/asm
  rm /usr/src/linux-headers-3.2.0-31/include/asm

This got nowhere:
  ln -s /usr/src/linux-headers-3.2.0-31-generic/arch/x86/include/asm /usr/include/asm

---

According to http://manpages.ubuntu.com/manpages/lucid/man4/bce.4freebsd.html
"1000baseSX   Sets 1000Mbps operation.  Only full-duplex mode is supported
                  at this speed."

So the question is how to use the builtin driver, but set to the above speed.

Try:

  ifconfig eth0 media 1000baseSX

  apt-get install ethtool

  ethtool eth0

   root@03salvus:/etc# /sbin/mii-tool
   eth0: negotiated 100baseTx-FD, link ok
  ...


 ifconfig eth0 media 1000baseTx

What I want: [   31.797673] bnx2 0000:01:00.0: eth0: NIC Copper Link is Up, 1000 Mbps full duplex
root@combinat:/home/wstein# /sbin/mii-tool
eth0: negotiated 1000baseT-FD flow-control, link ok


What I got:  [   25.759307] bnx2 0000:01:00.0: eth0: NIC Copper Link is Up, 100 Mbps full duplex
root@03salvus:/etc# /sbin/mii-tool
eth0: negotiated 100baseTx-FD, link ok

----

I think it is the CABLE!  That's what this page says:  http://ubuntuforums.org/archive/index.php/t-1283713.html

Moreover, 02salvus and 04salvus do connect fine at gigabit.

Solution: swap out the cables on 01salvus and 03salvus!!!!


---

I should test virtualbox on ...


In the meantime, on 01salvus, I'll try this:

   virt-install -v --cpu host --graphics vnc,port=8125 --name salvus_sage_cpuhost --vcpus=16 --ram 8192 --disk sparse=true,path=salvus_sage_cpuhost.img,size=14 -c ubuntu-12.04-server-amd64.iso

   ssh -L 8125:localhost:8125 salvus@01salvus.math.washington.edu

   virsh -c qemu:///session start salvus_sage_cpuhost

   virsh -c qemu:///session qemu-monitor-command --hmp salvus_sage_cpuhost 'hostfwd_add ::2223-:22'   # NOT persistent?!

   scp -P 2223 sage-5.3.tar localhost:

-----------

So finally, I'll do this whole test on 02salvus, which has a solid network, and is pretty clean.


   scp 01salvus:.screenrc .
   virt-install -v --cpu host --graphics vnc,port=8125 --name salvus_sage_cpuhost --vcpus=16 --ram 8192 --disk path=salvus_sage_cpuhost.img,size=14 -c ubuntu-12.04-server-amd64.iso
   ssh -L 8125:localhost:8125 salvus@02salvus.math.washington.edu

   virsh -c qemu:///session start salvus_sage_cpuhost
   virsh -c qemu:///session qemu-monitor-command --hmp salvus_sage_cpuhost 'hostfwd_add ::2222-:22'
   ssh localhost -p 2222
   wget http://sage.math.washington.edu/home/release/sage-5.3/sage-5.3.tar
   sudo apt-get install g++ gfortran m4 make dpkg-dev libssl-dev
   sleep 6000; export MAKE="make -j16"; tar xvf sage-5.3.tar; cd sage-5.3; make


I solved the networking slowdown issue!
(Thanks to https://help.ubuntu.com/community/KVM/Networking)

    <interface type='user'>
      <mac address='52:54:00:be:75:c2'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
      <model type='virtio'/>  <!-- ADD THIS LINE to ~/.libvirt/qemu/salvus_sage_cpuhost.xml -->
    </interface>

This should be something I can do via the virt-install line with:

   -net nic,model=virtio -net user

so, this should be it:

   virt-install -v --cpu host -net nic,model=virtio -net user --graphics vnc,port=8125 --name salvus_sage_cpuhost --vcpus=16 --ram 8192 --disk path=salvus_sage_cpuhost.img,size=14 -c ubuntu-12.04-server-amd64.iso

---

Sept 24, 2012
=============

I had no real time to work today at all, because of teaching, shoping,
the vert ramp paperwork and banking stuff, etc.  Until right now.  I
did get to fix the network and do a clean re-install of 01salvus.

Great discussion of the qcow2 image format: http://people.gnome.org/~markmc/qcow-image-format.html

I'm going to take another stab at making template virtual machines for:

 (1) sage, (2) web, (3) cassandra

Step 1: make a base template


   virt-install -v --cpu host --network user,model=virtio --graphics vnc,port=8125 --name salvus_base --vcpus=2 --ram 4096 --disk path=salvus_base.img,size=8 -c ubuntu-12.04-server-amd64.iso

   ssh -L 8125:localhost:8125 salvus@01salvus.math.washington.edu

Setup LVM with one big root partition and no swap (swap will be on another image).

   virsh -c qemu:///session start salvus_base
   virsh -c qemu:///session qemu-monitor-command --hmp salvus_base 'hostfwd_add ::2222-:22'

Sept 25, 2012
=============

  [ ] setup 01salvus, 02salvus, 03salvus, 04salvus to each be tinc VPN servers
      * for that, should install salvus on each, which requires a bootstrap
            git clone https://github.com/williamstein/salvus.git   # and type login: williamstein, password: ....
      * I fixed the /etc/hosts files back to stock, plus put in shortcuts for the cluster.
      * on each node:  git clone 01salvus:salvus/
      * fixed build system
            git push https://github.com/williamstein/salvus.git

* woops:
    WARNING: IPython History requires SQLite, your history will not be saved
     WARNING: Readline services not available or not loaded.WARNING: The auto-indent feature requires the readline libraryPython 2.7

Fix:
    . salvus-env
    sudo apt-get install libncurses-dev
    easy_install readline
Ick:
    WARNING: IPython History requires SQLite, your history will not be saved

Fix:
    . salvus-env
    sudo apt-get install libsqlite3-dev
    ./build.py --build_python

sudo apt-get install libncurses-dev libsqlite-dev
cd salvus/salvus; . salvus-env; easy_install readline ; ./build.py --build_python

Finally:

   >>> import admin
   >>> admin.tinc_conf('01salvus', ['combinat0', 'bsd0', 'servedby1'])
.... argh

First... should I run "repair" on a cassandra node? I haven't in week(s).  Here's how:
1. Look in conf/deploy/services for the cassandra nodes.
2. ssh salvus@bsd1.salv.us "cd salvus/salvus/; . salvus-env; nodetool repair"

  [ ] working with the vpn is too difficult right now, because list of
      hosts isn't all stored in a database.  fix this.

Options:

  - right now the vpn info *is* in a database, namely the git repo.
    But pushing and pulling to/from that every time I modify the vpn
    seem bad.

  - use Cassandra to store the vpn info:
       - advantage: no single point of failure
       - disadvantage: only a few nodes can connect to cassandra, so we'll have to
         run another service just to deal with this.
       - service is called tinc_directory.py
       - the *only* machines that need access to the directory database are the
         virtual machine hosts, since they will be configuring the tinc directory
         when they provision a machine.
       - actually, this service could just be rolled into the tornado
         server itself, since it can talk to cassandra, and any host
         on the vpn can talk to the tornado server.
       - bootstrap?!  there is a catch 22, since when no vm's are
         started, obviously nothing can work!  How about the program
         that starts up a node reconfigures tinc with the latest host
         files, etc., as soon as it can contact a tornado node and
         that node works... but until then it uses the last
         potentially stale default hosts info.
       - problem: if I do this via tornado, I have to somehow deal with authentication,
         since it is necessary to write to the database when provisioning new nodes.
         i don't have to worry about ssl, since all traffic is encrypted.

  - use a sqlite file on the "master control node" --> SPOF

  - put a postgresql server back into salvus just for this --> SPOF

  - get better at just using the git repo for this (or is this is a
    non-option if we start provisioning hundreds of machines?)

  - use control node filesystem:
      - admin can only be done from vm host nodes

  - for automatically provisioning new vm's there is already a SPOF, namely the
    machine on which the vm is running.  So we really only need to put the public
    key for the vm in the hosts/ directory for *that* one machine.  There is no
    point in putting it anywhere else.   This could be dynamic: when we start a VM,
    we generate the keys and put them in the hosts/ directory; when we stop the VM,
    we remove them.  That's it.       Since we have far fewer VM hosts, we just continue
    as is with them.   This would all be pretty easy.

Let's get 01salvus on our VPN:

salvus@01salvus:~$ cd salvus/salvus/
salvus@01salvus:~/salvus/salvus$ . salvus-env
salvus@01salvus:~/salvus/salvus$ ipython

   >>> import admin; admin.tinc_conf('01salvus', ['combinat0', 'bsd0'])

sudo apt-get install liblzo2-dev

NOTE: VPN thoughts for later...: According to
http://www.tinc-vpn.org/pipermail/tinc/2010-March/002276.html the
algorithm to determine path is very naive -- one of the shortest
paths.  However, in tinc 1.1 (which I would have to upgrade to) they
use a much better algorithm.  To build 1.1 on OSX, do this first
export CFLAGS="-fnested-functions"

TODO: should add "servedby1" to list of servers above, but then have
to change the firewall rules for that machine to allow connections
from salvus nodes.

For other nodes:

       cd salvus/salvus/; . salvus-env; git pull 01salvus:salvus/; sudo apt-get install liblzo2-dev; ./build.py --build_tinc; ipython
       import admin, os; admin.tinc_conf(os.popen('hostname').read().strip(), ['combinat0', 'bsd0'])

       git add .; git commit -a -v

Back on 01salvus:

git pull 02salvus:salvus/

  [x] just installed updates on bsd.math and restarted those virtual
      machines.  some of the combinat vm's were mysteriously crashed!

  [ ] test setup salvus-base to be on vpn but not public.

  [ ] test out copy on write with my template vm above:

qemu-img create -f qcow2 -b winxp.img test01.img

  [x] *** System restart required ***  -- wonder about --
Answer: http://askubuntu.com/questions/28530/how-can-i-tell-what-package-requires-a-reboot-of-my-system
salvus@01salvus:~$ more  /var/run/reboot-required.pkgs
linux-image-3.2.0-31-generic
linux-base
dbus

  [ ] 01salvus needs -- apt-get update; apt-get upgrade


-------

Sept 26, 2012

I'm still learning how to make template base virtual machines, etc.

This looks relevant:

  http://www.linux-kvm.com/content/how-you-can-use-qemukvm-base-images-be-more-productive-part-1

$ qemu-img create -b  salvus_base.img -f  qcow2    salvus_base-clone.img
$ ls -lh salvu*.img
-rw-r--r-- 1 salvus salvus 193K Sep 26 20:20 salvus_base-clone.img
-rwxrwxr-x 1 salvus salvus 8.0G Sep 26 20:19 salvus_base.img


$ virt-install -v --cpu host --network user,model=virtio --graphics vnc,port=8126 --name salvus_base_clone --vcpus=2 --ram 4096 --disk path=salvus_base-clone.img  --boot hd

FAIL

$ virsh --connect qemu:///session destroy salvus_base_clone
$ virsh --connect qemu:///session undefine salvus_base_clone

I want to start a "transient" domain.

"Rebasing base images" (i.e., merging):

  http://www.linux-kvm.com/content/be-more-productive-base-images-part-2

  qemu-img convert windows-clone.qcow2 –O qcow2 windows-marketing.qcow2



$ virt-install -v --cpu host --network user,model=virtio --graphics vnc,port=8126 --name salvus_base_clone --vcpus=2 --ram 4096 --import --disk salvus_base-clone.img --noautoconsole

FAIL

Test the image


# apt-get install guestmount
# add salvus user to fuse group !!!
mkdir x
guestmount -a salvus_base-clone.img --rw x -i


From a random place I found this:

virt-install --vcpus $CPUS \
  --ram $MEM --import \
  --name $TARGET_NAME \
  --disk $TARGET_IMG,device=disk,bus=virtio,format=qcow2 $DISK_SWAP \
  --vnc --noautoconsole --force \
  --network=$NETWORK,mac=$MAC \
  $NETWORK2

#And made this, which worked!!

$ virt-install --cpu host --network user,model=virtio --graphics vnc,port=8126 --name salvus_base_clone --vcpus=2 --ram 4096 --import --disk salvus_base-clone.img,device=disk,bus=virtio,format=qcow2 --noautoconsole

# are macs of 2 machines made this way different?

qemu-img create -b  salvus_base.img -f  qcow2    salvus_base-clone1.img  # 0.070s
qemu-img create -b  salvus_base.img -f  qcow2    salvus_base-clone2.img  # 0.080s

virt-install --cpu host --network user,model=virtio --graphics vnc,port=8120 --name salvus_base_clone1 --vcpus=2 --ram 4096 --import --disk salvus_base-clone1.img,device=disk,bus=virtio,format=qcow2 --noautoconsole  # 5.829s

virt-install --cpu host --network user,model=virtio --graphics vnc,port=8121 --name salvus_base_clone2 --vcpus=2 --ram 4096 --import --disk salvus_base-clone2.img,device=disk,bus=virtio,format=qcow2 --noautoconsole  #

  ssh -L 8120:localhost:8120 -L 8121:localhost:8121 salvus@01salvus

MACs are different.  Good.

Hmm. but the network sucks.  need to retry that parameter and fix this.

  virsh -c qemu:///session list
  virsh -c qemu:///session qemu-monitor-command --hmp salvus_base_clone2 'hostfwd_add ::2222-:22'


I wonder about recompacting .img, but don't really need to worry about that...

Network performance testing:

  sudo apt-get install iperf

iperf -s # on server
iperf -c servername # on client

Hit control-c

From this we see that the connection 01salvus <--> virtual machine: is
very fast, but the additional NAT is very slow.  Maybe with tinc this
doesn't matter.

Through tinc, between combinat0 and 01salvus (physical machines), we get:

   [  3]  0.0-10.0 sec   270 MBytes   226 Mbits/sec

which is I guess OK, given that it is encrypted, etc.

From combinat1 -- a *VirtualBox* VM -- to 01salvus over tinc:

   salvus@combinat1:~$ iperf -c 01salvus.salv.us
   ------------------------------------------------------------
   Client connecting to 01salvus.salv.us, TCP port 5001
   TCP window size: 16.4 KByte (default)
   ------------------------------------------------------------
   [  3] local 10.38.1.7 port 39425 connected with 10.38.1.14 port 5001
   [ ID] Interval       Transfer     Bandwidth
   [  3]  0.0-10.0 sec  43.6 MBytes  36.4 Mbits/sec

To combinat.math.washington.edu from combinat1:

   [  3]  0.0- 3.0 sec   187 MBytes   522 Mbits/sec

Over usual network:

salvus@combinat1:~$ iperf -t 3 -c 01salvus.math.washington.edu
------------------------------------------------------------
Client connecting to 01salvus.math.washington.edu, TCP port 5001
TCP window size: 23.5 KByte (default)
------------------------------------------------------------
[  3] local 10.0.2.15 port 38370 connected with 128.95.224.230 port 5001
[ ID] Interval       Transfer     Bandwidth
[  3]  0.0- 3.0 sec   168 MBytes   469 Mbits/secx

So tinc is *killing* us here!

Change to *only* use combinat0 as tinc server.... and the result is just acceptable (though it varies):

salvus@combinat1:~$ iperf  -c 01salvus.math.washington.edu
------------------------------------------------------------
Client connecting to 01salvus.math.washington.edu, TCP port 5001
TCP window size: 23.5 KByte (default)
------------------------------------------------------------
[  3] local 10.0.2.15 port 38374 connected with 128.95.224.230 port 5001
[ ID] Interval       Transfer     Bandwidth
[  3]  0.0-10.1 sec   124 MBytes   104 Mbits/sec


I reniced to -19 the tinc process on combinat0 and I got:

[  3]  0.0-10.1 sec   342 MBytes   285 Mbits/sec

W00t

Not every time, but it is something.  Check this out:

salvus@combinat1:~$ iperf  -c 01salvus.math.washington.edu
------------------------------------------------------------
Client connecting to 01salvus.math.washington.edu, TCP port 5001
TCP window size: 23.5 KByte (default)
------------------------------------------------------------
[  3] local 10.0.2.15 port 38385 connected with 128.95.224.230 port 5001
[ ID] Interval       Transfer     Bandwidth
[  3]  0.0-10.0 sec   577 MBytes   484 Mbits/sec

Moral: always "nice -19" the tincd.

  nice --19 /home/salvus/salvus/salvus/data/local/sbin/tincd

I did that on 01salvus, combinat1, and combinat0, and now I get
consistently 340+ Mbits/sec, which is consistent with compression.  So
Virtualbox networking is just fine, and tincd is not evil.

Need to install salvus in the base image.

1. Delete the machines I made:

virsh --connect qemu:///session undefine salvus_base_clone1
virsh --connect qemu:///session undefine salvus_base_clone2

virsh --connect qemu:///session destroy salvus_base_clone2


virsh --connect qemu:///session list --all

2. Start base machine:

virsh --connect qemu:///session start salvus_base
virsh -c qemu:///session qemu-monitor-command --hmp salvus_base 'hostfwd_add ::2222-:22'
ssh localhost -p 2222

3. Make it better:

sudo apt-get install git iperf
sudo apt-get autoremove

sudo apt-get install liblzo2-dev libssl-dev libreadline-dev  libsqlite3-dev libncurses5-dev emacs23


I just did

   wget http://sage.math.washington.edu/home/release/sage-5.3/sage-5.3.tar

when logged in via "ssh localhost -p 2222" and that ssh network thing died, as did the whole
NETWORK!  But not the vm. Restarting the network on the host fixed the problem temporarily.
This is of course very worrisome.

----

I'm putting the vm base on the vpn, since that's what really matters.

DONE.

I get this consistently:

salvus@salvus-base:~/salvus/salvus$ iperf -P 3 -t 5 -c combinat0.salv.us
------------------------------------------------------------
Client connecting to combinat0.salv.us, TCP port 5001
TCP window size: 22.9 KByte (default)
------------------------------------------------------------
[  3] local 10.38.1.254 port 33181 connected with 10.38.1.2 port 5001
[ ID] Interval       Transfer     Bandwidth
[  3]  0.0- 5.0 sec   121 MBytes   202 Mbits/sec

No hangs.

salvus@salvus-base:~/salvus/salvus$ iperf -c 01salvus.salv.us
------------------------------------------------------------
Client connecting to 01salvus.salv.us, TCP port 5001
TCP window size: 22.9 KByte (default)
------------------------------------------------------------
[  3] local 10.38.1.254 port 47411 connected with 10.38.1.14 port 5001
[ ID] Interval       Transfer     Bandwidth
[  3]  0.0-10.0 sec   256 MBytes   215 Mbits/sec

See tinc taking a full core in top:

 9163 root       1 -19 15116 1540 1168 R   98  0.0   1:13.61 tincd

---

1. Now add more cores and RAM and build Sage on base VM.

  emacs /home/salvus/.libvirt/qemu/salvus_base.xml

and change vcpu from 2 to 16.
    change ram from 4194304 to 16777216

2. start:

    virsh --connect qemu:///session start salvus_base

3. Notice that it's on the vpn!

   mosh salvus@10.38.1.254  # works fine

4. Consistent scp speed from 01salvus to vm:     26.0MB/s

salvus@01salvus:~$ scp ubuntu-12.04-server-amd64.iso 10.38.1.254:
ubuntu-12.04-server-amd64.iso                                                                                         14%  101MB  26.0MB/s   00:22 ETA

5. start some other useful stuff installing:

     sudo apt-get install texlive

6. start sage build:

    export MAKE="make -j8"; make ptestlong


 ssh -L 8125:localhost:8125  salvus@01salvus

----

Idea to fix network problem:

This is the problem, exactly.
   http://serverfault.com/questions/362038/qemu-kvm-virtual-machine-virtio-network-freeze-under-load

Will it be as good?

     "Preliminary tests suggest that I don't have the problem if I
     substitute e1000 for virtio in the first -net flag to qemu-kvm."

I'm going to try a speed test on this, by cloning the other img (live?!), then
making a new machine using that.


   qemu-img convert salvus_base.img nettest.img


   virt-install -d --cpu host --network user,model=e1000 --graphics vnc,port=8122 --name nettest --vcpus=2 --ram 4096 --import --disk nettest.img,device=disk,bus=virtio,format=qcow2 --noautoconsole

This fails with "Unable to read from monitor: Connection reset by peer", no matter what I do . Weird.
I think I'll just wait and setup a copy-on-write image as before.

  qemu-img create -b  salvus_base.img -f  qcow2    salvus_base-clone1.img

  virt-install --cpu host --network user,model=e1000 --graphics vnc,port=8120 --name salvus_base_clone1 --vcpus=2 --ram 4096 --import --disk salvus_base-clone1.img,device=disk,bus=virtio,format=qcow2 --noautoconsole

Make another clone right now with virtio

  qemu-img create -b  salvus_base.img -f  qcow2    salvus_base-virtio.img

  virt-install --cpu host --network user,model=virtio --graphics vnc,port=8121 --name salvus_base_virtio --vcpus=2 --ram 4096 --import --disk salvus_base-virtio.img,device=disk,bus=virtio,format=qcow2 --noautoconsole

It really is necessary to generate new keys, etc.

THIS IS REALLY THE BUG:

  https://bugs.launchpad.net/ubuntu/+source/qemu-kvm/+bug/997978

IMPORTANT: **************************
Apply fix there:

   sudo apt-get install python-software-properties; sudo add-apt-repository ppa:ubuntu-virt/kvm-network-hang; sudo apt-get update; sudo apt-get install qemu-kvm

 virsh --connect qemu:///session start salvus_base_virtio

... and this seems to be working so far!


Problems left to solve before I can just code this:

[ ] How to mount an img as salvus user (not root)

     virsh -c qemu:///session list
     sudo apt-get install libguestfs-tools
     virt-filesystems -a salvus_base-virtio.img

Fix was easy - this is a slight security risk though (see http://bugs.debian.org/cgi-bin/bugreport.cgi?bug=615029).

     sudo chmod a+r /boot/vmlinuz-* # used by guestmount
     mkdir mnt; guestmount -i -a salvus_base-virtio.img --rw mnt
     ls mnt
     fusermount -u mnt

[ ] get pid of kvm process

     virsh --connect qemu:///session start salvus_base_virtio

This yields 52227, so we have that to check with:

     ps ax |grep kvm|grep salvus_base_virtio

Consider using python bindings for some of this?

     apt-get install ipython

ipython

In [1]: import libvirt
In [2]: import sys
In [3]: conn = libvirt.openReadOnly(None)
In [4]: conn
Out[4]: <libvirt.virConnect instance at 0x208c128>
In [5]: dom0 = conn.lookupByName("salvus_base_virtio")
In [6]: dom0
Out[6]: <libvirt.virDomain instance at 0x1e8fb90>
In [7]: dom0.info()
Out[7]: [1, 4194304L, 4194304L, 2, 8270000000L]
In [12]: dom0.isActive()
Out[12]: 1
In [17]: %timeit dom0.isActive()
1000 loops, best of 3: 389 us per loop

So instead of watching for the pid, I can call isActive periodically.
This means that my vm.py script/daemon must use the system-wide python.
That should be OK.

Fix autoboot thing.


   sudo apt-get install iotop


This makes it so I don't have to prefix virsh with that --connect stuff:

   export VIRSH_DEFAULT_CONNECT_URI="qemu:///session"

Put in .bash_profile on all four machines...

Renaming from salvus_base to template?  Actually salvus_base is a good name.

virsh start salvus_base

root@salvus-base:/boot/grub# vi /etc/grub.d/00_header
root@salvus-base:/boot/grub# update-grub2

OK, done.

Make a "backup" of my image:

   rsync -axvH images/ 02salvus:images/

--RESIZING MY ROOT DISK IMG-----------------------------
My / disk is almost full, which could be annoying and complicating for
now good reason down the line.  Let's deal with this now, following:

   http://serverfault.com/questions/324281/how-do-you-increase-a-kvm-guests-disk-space

First:

  time qemu-img resize images/salvus_base.img +8G

  virsh start salvus_base


root@salvus-base:/home/salvus# fdisk /dev/sda

Command (m for help): p

Disk /dev/sda: 17.2 GB, 17179869184 bytes
255 heads, 63 sectors/track, 2088 cylinders, total 33554432 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0x000d72a0

   Device Boot      Start         End      Blocks   Id  System
/dev/sda1   *        2048      499711      248832   83  Linux
/dev/sda2          501758    16775167     8136705    5  Extended
/dev/sda5          501760    16775167     8136704   8e  Linux LVM


* delete partitions 2 and 5

New: extended partition:

Command (m for help): n
Partition type:
   p   primary (1 primary, 0 extended, 3 free)
   e   extended
Select (default p): e
Partition number (1-4, default 2):
Using default value 2
First sector (499712-33554431, default 499712):
Using default value 499712
Last sector, +sectors or +size{K,M,G} (499712-33554431, default 33554431):
Using default value 33554431

Command (m for help): p

Disk /dev/sda: 17.2 GB, 17179869184 bytes
255 heads, 63 sectors/track, 2088 cylinders, total 33554432 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0x000d72a0

   Device Boot      Start         End      Blocks   Id  System
/dev/sda1   *        2048      499711      248832   83  Linux
/dev/sda2          499712    33554431    16527360    5  Extended

New logical partition:

Command (m for help): n
Partition type:
   p   primary (1 primary, 1 extended, 2 free)
   l   logical (numbered from 5)
Select (default p): l
Adding logical partition 5
First sector (501760-33554431, default 501760):
Using default value 501760
Last sector, +sectors or +size{K,M,G} (501760-33554431, default 33554431):
Using default value 33554431

Fix type


   Device Boot      Start         End      Blocks   Id  System
/dev/sda1   *        2048      499711      248832   83  Linux
/dev/sda2          499712    33554431    16527360    5  Extended
/dev/sda5          501760    33554431    16526336   8e  Linux LVM

"write" out and reboot.


pvdisplay
pvresize /dev/sda5
pvdisplay
lvresize /dev/salvus-base/root -l +2048
resize2fs /dev/salvus-base/root

It worked:

root@salvus-base:/home/salvus# resize2fs /dev/salvus-base/root
resize2fs 1.42 (29-Nov-2011)
Filesystem at /dev/salvus-base/root is mounted on /; on-line resizing required
old_desc_blocks = 1, new_desc_blocks = 1
Performing an on-line resize of /dev/salvus-base/root to 4130816 (4k) blocks.
The filesystem on /dev/salvus-base/root is now 4130816 blocks long.

root@salvus-base:/home/salvus# df -h
Filesystem                     Size  Used Avail Use% Mounted on
/dev/mapper/salvus--base-root   16G  6.5G  8.3G  44% /
udev                           7.9G  4.0K  7.9G   1% /dev
tmpfs                          3.2G  248K  3.2G   1% /run
none                           5.0M     0  5.0M   0% /run/lock
none                           7.9G     0  7.9G   0% /run/shm
/dev/sda1                      228M   47M  169M  22% /boot
root@salvus-base:/home/salvus# df -h /
Filesystem                     Size  Used Avail Use% Mounted on
/dev/mapper/salvus--base-root   16G  6.5G  8.3G  44% /

---------------------------

  cd vm
  qemu-img create -b salvus_base.img -f qcow2 10.38.1.20.img
  mkdir mnt; guestmount -i -a 10.38.1.20.img --rw mnt

Sept 28:
========
In base vm with 16 cores, the time to doctest all sage (after doing it once already):

salvus@salvus-base:~/sage-current$ time sage -tp 20 devel/sage/sage/

real    7m23.826s
user    98m13.080s
sys     13m17.094s

I'm curious how long on the non-VM (bare metal hardware).
Also, I'm curious how long while building on the bare metal hardware at the same time...

MAIN Goal now is getting vm.py to actually work.

On bare metal, repeatedly:
salvus@01salvus:~/sage-5.3$ time ./sage -tp 20 devel/sage/sage
real    6m2.230s
user    82m51.347s
sys     11m13.054s

That's pretty cool -- kvm isn't costing us much at all -- it's 81% of
bare metal overall.
---

Working on loging now that I can start vm's, etc.

salvus@01salvus:~/salvus/salvus$ . salvus-env
salvus@01salvus:~/salvus/salvus$ ./vm.py --ip_address=10.38.1.38 --hostname=01salvus1 --logfile=log -d


# need to do this in the base vm:
sage -sh
easy_install protobuf

On the host machines:

   sudo apt-get install guestmount
   sudo chmod a+r /boot/vmlinuz-* # used by guestmount

Trying on 02salvus:

./vm.py --ip_address 2.1
virsh -c qemu:///session qemu-monitor-command --hmp 10.38.2.1 'hostfwd_add ::2222-:22'
ssh localhost -p 2222


On 04salvus:

sudo apt-get install virtinst

---

7:10am Sept 30:
===============

I just changed the /etc/rc.local's to

   nice --19 /home/salvus/salvus/salvus/data/local/sbin/tincd

I can autostart/stop/etc. VM's on all *salvus machines.  However, the
tinc vpn thing doesn't work on 03salvus and 04salvus.  I have no idea
why.  Here is my plan to find out:

[] start a VM on 03salvus
       salvus@03salvus:~$ cd salvus/salvus/; . salvus-env; ./vm.py --ip_address=3.1

[] see if it works magically; if so, done -- try 04salvus
   DOESN'T WORK

[] start a VM on 01salvus, and verify that vpn does work.
       salvus@01salvus:~$ cd salvus/salvus/; . salvus-env; ./vm.py --ip_address=2.1

[] swap in various ways the two vm credentials exactly and see what happens:

   1. add host public key for 3.1 vm to 01salvus server and try to connect manually:
       salvus@03salvus:~$ virsh -c qemu:///session qemu-monitor-command --hmp 10.38.3.1 'hostfwd_add ::2222-:22'
       salvus@03salvus:~$ ssh localhost -p 2222
       salvus@10dot38dot3dot1:~$ sudo scp /home/salvus/salvus/salvus/data/local/etc/tinc/hosts/10dot38dot3dot1 salvus.math.washington.edu@01salvus:salvus/salvus/data/local/
       salvus@10dot38dot3dot1:~$ vi /home/salvus/salvus/salvus/data/local/etc/tinc/tinc.conf  # change 03salvus to 01salvus
       salvus@10dot38dot3dot1:~$ sudo su
       root@10dot38dot3dot1:/home/salvus# killall tincd
       root@10dot38dot3dot1:/home/salvus# /home/salvus/salvus/salvus/data/local/sbin/tincd -D -d3
    ## this actually works!
  2. OK, test that network is solid:
salvus@10dot38dot2dot1:~$ iperf -s
...
salvus@10dot38dot3dot1:~$ iperf -c 10.38.2.1
------------------------------------------------------------
Client connecting to 10.38.2.1, TCP port 5001
TCP window size: 22.9 KByte (default)
------------------------------------------------------------
[  3] local 10.38.3.1 port 48455 connected with 10.38.2.1 port 5001
[ ID] Interval       Transfer     Bandwidth
[  3]  0.0-10.0 sec   213 MBytes   179 Mbits/sec

  3. Inspect how tinc is configured on 03salvus versus 01salvus.

salvus@01salvus:~$ ls -lh salvus/salvus/data/local/etc/tinc/
lrwxrwxrwx 1 salvus salvus   27 Sep 27 09:06 hosts -> ../../../../conf/tinc_hosts
-rw------- 1 salvus salvus 1.7K Sep 27 09:06 rsa_key.priv
-rw-rw-r-- 1 salvus salvus   55 Sep 27 09:06 tinc.conf
-rwx------ 1 salvus salvus   61 Sep 27 09:06 tinc-up

salvus@03salvus:~$ ls -lh salvus/salvus/data/local/etc/tinc/
total 12K
lrwxrwxrwx 1 salvus salvus   27 Sep 28 22:51 hosts -> ../../../../conf/tinc_hosts
-rw------- 1 salvus salvus 1.7K Sep 28 22:51 rsa_key.priv
-rw-rw-r-- 1 salvus salvus   55 Sep 28 22:51 tinc.conf
-rwx------ 1 salvus salvus   61 Sep 28 22:51 tinc-up

   4. Try to connect to 03salvus from 02salvus:

salvus@02salvus:~$ vi /home/salvus/salvus/salvus/data/local/etc/tinc/tinc.conf  # comment out others -- put only 03salvus
salvus@02salvus:~$ more /home/salvus/salvus/salvus/data/local/etc/tinc/tinc.conf
Name = 02salvus
ConnectTo = 03salvus
#ConnectTo = combinat0
#ConnectTo = bsd0
salvus@02salvus:~$ sudo su
[sudo] password for salvus:
root@02salvus:/home/salvus# killall tincd
root@02salvus:/home/salvus# /home/salvus/salvus/salvus/data/local/sbin/tincd -D -d3

    THIS FAILS.

    Conclusion: Something wrong with 03salvus's config -- not the VM, but the server.  Inspect further.

    Ahhh.  03salvus and 04salvus got new public keys.  I probably
    never updated those in the base template vm.  So the problem is
    that clients are not trusting the server (not the other way
    around).  This is straightforward to fix, and really obvious.  I
    must have been tired/stupid/rushed to not instantly realize this
    before.

  (done)   * make pub keys correct in repo on 01salvus (and github)
  (done)   * shutdown my vm's
     * start the base template vm.
salvus@01salvus:~$ virsh start salvus_base

     * pull repo

salvus@01salvus:~$ ssh base.salv.us
salvus@salvus-base:~$ cd salvus/; git pull

test that can connect to 03 and 04

root@salvus-base:/home/salvus/salvus/salvus/data/local/etc/tinc# scp hosts/salvus_base salvus@03salvus.math.washington.edu:salvus/salvus/data/local/etc/tinc/hosts/

works!

salvus@01salvus:~$ virsh -c qemu:///session qemu-monitor-command --hmp salvus_base 'hostfwd_add ::2222-:22'
salvus@01salvus:~$ ssh localhost -p 2222


     * apt-get update; apt-get upgrade
re-edit /etc/grub.d/00_header
     * reboot, reboot  -- works, works
     * shutdown

     * (done) test on 01salvus
     * push out new vm template to (rsync):

salvus@01salvus:~$ time rsync --sparse vm/images/salvus_base.img 03salvus:vm/images/salvus_base.img
salvus@03salvus's password:

real    2m59.049s
user    1m42.946s
sys     0m8.365s

This --sparse option does work:

salvus@03salvus:~/vm/images$ du -sch *
7.8G    salvus_base.img
7.8G    total

     * test

salvus@03salvus:~$ cd salvus/salvus/; . salvus-env; ./vm.py --ip_address=3.1

WORKS!!

[x] also test on 02 and 04 after rsync --sparse...

works.


[x] I also need to make the tinc.conf on all salvus nodes like this:

ConnectTo = 01salvus
ConnectTo = 03salvus
ConnectTo = 04salvus
ConnectTo = servedby1
ConnectTo = combinat0
ConnectTo = bsd0

vi /home/salvus/salvus/salvus/data/local/etc/tinc/tinc.conf
sudo su
killall tincd && sleep 1 && nice --19 /home/salvus/salvus/salvus/data/local/sbin/tincd

   - 02salvus -- done
   - 01salvus
   - 03salvus
   - 04salvus

[x] I need to pull in the latest repo (with new public keys) to all salvus nodes, and to combinat0, bsd0, servedby1:

salvus@servedby1:~$ mv salvus salvus0
salvus@servedby1:~$ git clone https://github.com/williamstein/salvus.git
salvus@servedby1:~$ mv salvus0/salvus/data salvus/salvus/

   [x] servedby1
   [x] combinat0
   [x] bsd0


[x] I need to un-firewall 01-04salvus for servedby1



---

It looks like I get another 20 minutes before this Tish adventure on a boat today...

What to do?

[x] debug the VirtualMachine process object at a low level:

In [1]: import admin
In [2]: v = admin.VirtualMachine('2.1', vcpus=3, ram=3)

[ ] have some fun:

  >>> v = [admin.VirtualMachine('2.%s'%i, vcpus=2, ram=2)  for i in range(1,9)]
  >>> [x.start() for x in v]

TOP:

64739 salvus    20   0 3973m 257m 6092 S   91  0.4   0:07.10 kvm                                                       64814 salvus    20   0 2412m  26m 6036 S   75  0.0   0:02.31 kvm                                                       64827 salvus    20   0 2412m  26m 6036 S   51  0.0   0:01.64 kvm                                                       64685 salvus    20   0 4045m 260m 6092 S   31  0.4   0:07.64 kvm                                                       64711 salvus    20   0 3973m 256m 6092 S   25  0.4   0:07.56 kvm                                                       64726 salvus    20   0 3757m 261m 6092 S   24  0.4   0:07.17 kvm                                                       64672 salvus    20   0 4109m 253m 6092 S   24  0.4   0:07.47 kvm                                                       64701 salvus    20   0 4050m 250m 6092 S   15  0.4   0:07.47 kvm

salvus@01salvus:~/salvus/salvus$ df -h
Filesystem                 Size  Used Avail Use% Mounted on
/dev/mapper/01salvus-root   55G   24G   29G  46% /


salvus@01salvus:~$ virsh list --all
 Id Name                 State
----------------------------------
  1 10.38.2.1            running
  2 10.38.2.5            running
  3 10.38.2.2            running
  4 10.38.2.3            running
  5 10.38.2.7            running
  6 10.38.2.4            running
  7 10.38.2.6            running

salvus@01salvus:~/salvus/salvus$ free
             total       used       free     shared    buffers     cached
Mem:      65948776   44262736   21686040          0     254824   39749772

In a VM:
Cpu0  :  0.0%us,  0.0%sy,  0.0%ni,100.0%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st
Cpu1  :  0.8%us,  0.8%sy,  0.0%ni, 98.3%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st
Mem:   2051596k total,   282656k used,  1768940k free,    23988k buffers

VPN Network seems rock solid.

Interestingly, 2 iperf's at once don't slow things down much, if at all.  good.

Stopping. good.

Cleanup. good.

TODO: make status output different, e.g., maybe it could give status for the kvm process too or something?
      as it is, then %mem, etc., are useless....

In [8]: [str(x.status()) for x in v]
Out[8]:
["{'%mem': '0.0', 'pid': '4842', '%cpu': '0.0', 'cputime': '00:00:00', 'etime': '01:33', 'start': '09:32:00', 'vsize': '80592', 'rss': '8244'}",
 "{'%mem': '0.0', 'pid': '4847', '%cpu': '0.0', 'cputime': '00:00:00', 'etime': '01:33', 'start': '09:32:00', 'vsize': '80592', 'rss': '8244'}",
 "{'%mem': '0.0', 'pid': '4864', '%cpu': '0.0', 'cputime': '00:00:00', 'etime': '01:33', 'start': '09:32:00', 'vsize': '80592', 'rss': '8244'}",
 "{'%mem': '0.0', 'pid': '4882', '%cpu': '0.0', 'cputime': '00:00:00', 'etime': '01:33', 'start': '09:32:00', 'vsize': '80592', 'rss': '8244'}",
 "{'%mem': '0.0', 'pid': '4900', '%cpu': '0.0', 'cputime': '00:00:00', 'etime': '01:33', 'start': '09:32:00', 'vsize': '80592', 'rss': '8240'}",
 "{'%mem': '0.0', 'pid': '4915', '%cpu': '0.0', 'cputime': '00:00:00', 'etime': '01:33', 'start': '09:32:00', 'vsize': '80592', 'rss': '8244'}",
 "{'%mem': '0.0', 'pid': '4935', '%cpu': '0.0', 'cputime': '00:00:00', 'etime': '01:33', 'start': '09:32:00', 'vsize': '80592', 'rss': '8244'}"]

Time to stop 8 vms:

In [9]: time [x.stop() for x in v]
CPU times: user 0.01 s, sys: 0.03 s, total: 0.04 s
Wall time: 5.72 s


4:17pm Sept 30:
===============

I need to have three files in my deploy list:

  - hosts
  - services
  - infrastructure

At some point, these could become three tables in a sqlite database
that is distributed (with a table that holds every query that changes
the table, and gets played eventually to all nodes).

I really need some database-like thing -- with no single point of failure, etc., --
to keep track of this configuration info, in the long run.

   http://zookeeper.apache.org/   -- not so impressed.

Another option would be to use git.  Just store config info in these
text files in a git repo, and pull/push it around as needed.  This
might work?  It's what I'm doing now, basically.

Right now I'm going to figure out the file-based infrastructure implementation and get
it implemented.

git pull wstein@salvus0.salv.us:/Users/wstein/salvus/

 [x] Define format via sample config file.  Will be much like Service class.
 [x] Test/debug

 [x] Starting Sage service: test/debug as good example:

$ cqlsh -3 servedby1.salv.us
cqlsh> select * from salvus.sage_servers ;

Sage client test:

blastoff:salvus wstein$ time echo "os.popen('hostname').read()" | ./sage_server.py -c --hostname 10.38.2.1 -p 6000
PID = 2723
sage [0]: '10dot38dot2dot1\n'
sage [1]:
Exiting Sage client.
real	0m0.329s
user	0m0.116s
sys	0m0.069s


   [x]      - turning on the firewall kills tinc unless I ssh in via 2222 then it comes back and works.

ufw --force reset && ufw allow 22 && ufw allow out 22 && ufw allow out 53 && ufw allow 655 && ufw allow out 655 && ufw allow proto tcp from 10.38.1.5 to any port 6000 && ufw allow proto tcp from 10.38.1.7 to any port 6000 && ufw allow proto tcp from 10.38.1.1 to any port 6000 && ufw allow proto tcp from 10.38.1.11 to any port 6000 && ufw deny proto tcp to any port 1:65535 && ufw deny proto udp to any port 1:65535 && ufw default deny outgoing && ufw --force enable

 [x] when restarting a vm, the admin Service object caches an ssh session to the old machine, which fails.   need to delete from cache if it fails.

 [x] not necessary -- we just background doing the setting and it is very fast. Maybe when starting sage process, make setting the firewall part of the process not the Service. ?

??? [ ] Dependencies.  When stopping services, should stop other services
     first?  E.g., if we're stopping a vm, we should stop the services
     on the vm.  This would be good for the sage services, since their
     status is in the cassandra db, and will be wrong if they are not
     properly stopped.


----

Disk benchmarking:

  time dd bs=1M count=512 if=/dev/zero of=test conv=fdatasync; rm test

sage.math: 64.9 MB/s

sage.math over NFS: 82.5 MB/s

sage.math ramdisk (tmpfs):  933 MB/s

disk.math locally:  39.5 MB/s  (my home dir)
                    226 MB/s   in /lvm/data/home/, which has the RAID

combinat locally:  73.7 MB/s

04salvus: 135 MB/s

kvm on 02salvus with default cache option (whatever that is):  40.9 MB/s
kvm on 02salvus with cache=writeback:  around 104MB/s


virtualbox on combinat: 63.4 MB/s

virtualbox on bsd: absurd/inconsistent numbers -- must be cached in ram

---
Another benchmark:

salvus@10dot38dot2dot1:~$ wget http://sage.math.washington.edu/home/release/sage-5.4.rc0/sage-5.4.rc0-sage.math.washington.edu-x86_64-Linux.tar.gz
2012-09-30 19:49:14 (49.7 MB/s) - `sage-5.4.rc0-sage.math.washington.edu-x86_64-Linux.tar.gz' saved [738098205/738098205]
salvus@10dot38dot2dot1:~$ time tar xf sage-5.4.rc0-sage.math.washington.edu-x86_64-Linux.tar.gz
real	0m46.797s
user	0m18.109s
sys	0m7.236s
salvus@10dot38dot2dot1:~$ time sync
real	0m0.319s
user	0m0.000s
sys	0m0.004s

Versus native disk on 01salvus:
salvus@01salvus:~/tmp$ wget http://sage.math.washington.edu/home/release/sage-5.4.rc0/sage-5.4.rc0-sage.math.washington.edu-x86_64-Linux.tar.gz
2012-09-30 19:50:02 (82.0 MB/s) - `sage-5.4.rc0-sage.math.washington.edu-x86_64-Linux.tar.gz' saved [738098205/738098205]
salvus@01salvus:~/tmp$ time tar xf sage-5.4.rc0-sage.math.washington.edu-x86_64-Linux.tar.gz
real	0m22.142s
user	0m17.841s
sys	0m6.420s
salvus@01salvus:~/tmp$ time sync
real	0m22.804s
user	0m0.000s
sys	0m0.048s


----> Combining with the sync time, it is almost the same...  Note that the vm in the test had very little ram.
Try again with a VM with lots (32gb) of RAM:

salvus@10dot38dot2dot1:~$ free
             total       used       free     shared    buffers     cached
Mem:      32951292     438400   32512892          0      14248      43396
-/+ buffers/cache:     380756   32570536
Swap:            0          0          0
salvus@10dot38dot2dot1:~$ wget http://sage.math.washington.edu/home/release/sage-5.4.rc0/sage-5.4.rc0-sage.math.washington.edu-x86_64-Linux.tar.gz
100%[===============================================================================================================================================>] 738,098,205 86.4M/s   in 8.5s
salvus@10dot38dot2dot1:~$ time tar xf sage-5.4.rc0-sage.math.washington.edu-x86_64-Linux.tar.gz
real	0m32.921s
user	0m18.241s
sys	0m9.941s
salvus@10dot38dot2dot1:~$ time sync
real	0m8.841s
user	0m0.000s
sys	0m0.020s
---------------

Seems great.

====================
[ ] Implement support for attaching persistent images to VM

   [ ] semantics

I don't see I'm going to get any significant advantage by using LVM's
directly over sparse images.  And sparse images will be way, way
easier to manage, right?  Which is easier to manage?

   - lvm volume: - always there as a separate volume
                 - lvm supports snapshots
                 - will be clear when starting VM what to do regarding persistent disk
                 - trivial for host to also mount
                 - much hard to move from one machine to another
   - qcow2 image: - sparse
                  - easy to move to another machine
                  - easy to make a complete backup of it.
                  - supports 128-bit AES encryption.  This would be
                    *perfect* to have -- then even if an image is
                    copied/taken/etc, nothing is lost.
                    Obviously, this will entail typing a password
                    when the vm is started, but that's ok, since
                    I can automate that through admin.py.
                  - of course, one can just encrypt an lvm filesystem.

ecryptfs: https://help.ubuntu.com/12.04/serverguide/ecryptfs.html
apt-get install ecryptfs-utils


root@10dot38dot3dot1:/home/salvus# mount -t ecryptfs secret secret
time dd bs=1M count=512 if=/dev/zero of=test conv=fdatasync; rm test
root@10dot38dot3dot1:/home/salvus#  time dd bs=1M count=512 if=/dev/zero of=test conv=fdatasync; rm test
536870912 bytes (537 MB) copied, 5.77535 s, 93.0 MB/s
root@10dot38dot3dot1:/home/salvus/secret# time dd bs=1M count=512 if=/dev/zero of=test conv=fdatasync; rm test
536870912 bytes (537 MB) copied, 9.3332 s, 57.5 MB/s
real	0m9.336s

*So, there is definitely a performance hit.*

Backups:

I do not *need* to make complete backups of these for my application.
I do need to keep (and test, etc.) the specially made incremental
backups that Cassandra (the DB) produces.  But that is *it*.

     ./vm.py ... --disk=cassandra1.img,64,backup.img,10


[ ] create img file, format ext4:

salvus@02salvus:~/tmp$ qemu-img create -f qcow2 ~/vm/images/10.38.2.1-cassandra1.img 64G
Formatting '/home/salvus/vm/images/10.38.2.1-cassandra1.img', fmt=qcow2 size=68719476736 encryption=off cluster_size=65536

salvus@02salvus:~/tmp$ ls -lh ~/vm/images/10.38.2.1-cassandra1.img
-rw-r--r-- 1 salvus salvus 193K Sep 30 22:22 /home/salvus/vm/images/10.38.2.1-cassandra1.img

# FAIL!!!
# mkfs.ext4 -f ~/vm/images/10.38.2.1-cassandra1.img
# mount -t ext4 -o loop ~/vm/images/10.38.2.1-cassandra1.img  mnt

Try this:
sudo apt-get install libguestfs-tools
FAIL.

LAME: I have an idea -- put special code to do *all* the provisioning logic
in salvus itself, and just call it in /etc/rc.local.

Bad idea.

!! this works !! Try this:

   sudo apt-get install guestfish

No way, guestfish has exactly the functionality I want in something
called the "Prepared Disk Images" section!

  guestfish -N fs:ext4:1G quit   # this works

# the above always creates test1.img in the current directory.

[x] just to prove this worked, mount it

  mkdir mnt
  guestmount -a test1.img -m/dev/vda1 --rw mnt

salvus@02salvus:~/tmp$ df -h mnt
Filesystem      Size  Used Avail Use% Mounted on
guestmount      7.9G  146M  7.4G   2% /home/salvus/tmp/mnt

[ ] tell VM to mount it under /mnt/

Do a test run to see how things come up:

   qemu-img create -b ~/vm/images/salvus_base.img -f qcow2 root.img
   fusermount -u mnt

   virt-install --cpu host --network user,model=virtio --name test1 --vcpus=8 --ram 4096 --import --disk root.img,device=disk,bus=virtio,format=qcow2 --disk test1.img,device=disk,bus=virtio,format=qcow2 --noautoconsole

virt-install --cpu host --network user,model=virtio --name test1 --vcpus=8 --ram 4096 --import --disk root.img,device=disk,bus=virtio,format=qcow2 --filesystem /home/salvus/tmp/mnt/,/mnt/foo  --noautoconsole
virsh -c qemu:///session qemu-monitor-command --hmp test1 'hostfwd_add ::2222-:22'


   virt-install --cpu host --network user,model=virtio --name test1 --vcpus=8 --ram 4096 --import --disk root.img,device=disk,bus=virtio,format=qcow2 --disk test1.img --noautoconsole

virsh --connect qemu:///session undefine test1

   virt-install --cpu host --network user,model=virtio --name test1 --vcpus=8 --ram 4096 --import --disk root.img,device=disk,bus=virtio,format=qcow2,cache=writeback --disk test1.img,bus=virtio,cache=writeback --noautoconsole

# disk gets big when I do:  dd bs=1M count=512 if=/dev/zero of=test conv=fdatasync; rm test
# but speed is fine.

time qemu-img convert test1.img test2.img  # this results in disk that is small again!

 virt-install --cpu host --network user,model=virtio --name test1 --vcpus=8 --ram 4096 --import --disk root.img,device=disk,bus=virtio,format=qcow2,cache=writeback --disk test1.img,bus=virtio,cache=writeback --disk test2.img,bus=virtio,cache=writeback --disk test3.img,bus=virtio,cache=writeback  --noautoconsole

# the devices are mounted in order as /dev/vda1 /dev/vdb1  /dev/vdc1 /dev/vdd1... in order of --disk on command line.

So this should now be dead easy to finish.


-----------------

[ ] Database Encryption plan: Replace the script "salvus/salvus/data/local/bin/start-cassandra"
by making

       ./cassandra.py

do what start-cassandra does, but also have option to encrypt its path using ecryptfs.
It will have to interactively ask for the passphrase.  Then make admin.py support this.
NEVER pass the passphrase on the command line or as a saved file.

This is the right level at which to implement encryption for salvus.
It will work on all of virtualbox/vmware/in the cloud/kvm, native hosts, etc.
It greatly simplifies implementing my own private cloud functionality.
Also, the way ecryptfs works, evidently as long as you know the passphrase,
you can read the files. Full stop.   If somebody steals in .img file or
physical computer, nothing is compromised.

Even better -- I should be able to set things up so when the salvus
user logs in via ssh (paramiko) to start cassandra, the encrypted img
is mounted.  That would be nice.

This article looks potentially useful:
   http://nwrickert2.wordpress.com/2012/01/17/experimenting-with-ecryptfs/

-----------------

7:34pm Oct 1:
===============

I get about an hour work on salvus this morning, and that's most
of what I get today.  Focus super hard and get the persistent image
working?


todo -- Bug to fix later -- status when machine is down, should reflect that instead of going boom!
In [21]: s.status('sage','10.')
INFO:root:python_c: cd "$HOME/salvus/salvus" && . salvus-env && python -c "import admin; print admin.Sage(id=0,**{'address': '10.38.1.100'}).status()"
INFO:root:10.38.1.100 -->

Back to work on persistent disks:
-------------------
# /etc/fs

/dev/vdb1 /mnt/b ext4 defaults 0 1
/dev/vdc1 /mnt/c ext4 defaults 0 1

permission issues...

guestfish -N fs:ext4:1G

guestmount -a test1.img -m/dev/vda1 --rw mnt

then change owner.... works.


OK, everything works.  The only thing left for later is automatically increasing the size.

I'm my office at 9:54am.  Should prep for class but I want to do one more important thing:
   - make it so the base templates are in a different subdir, and that one can
     select the one to use.  This will make it much, much easier to

Snuck it in -- done by 10:14am.

Now back to teaching prep.

NEXT thing I must address -- I think caching to the cassandra db is *really* slow every once in a while.
It should *NOT* block getting answers back!  Weird.  Anyways, _something_ is really slow every once
and a while for no reason.

The problem was that I needed to run repair on the cassandra node on
bsd1.  It was jacked.  Giving inconsistent results, etc.  I also
turned off the sage servers on combinat, since they seemed slow,
maybe, possibly due to heavy load by other users....

Done teaching.  I'm going to see if I can understand this speed issue with salvus right now,
then go skate jefferson.

Plan:  [x] I set USE_EXECUTION_CACHE to False and the problem persists.  So it is *not* caused by that!
       [x] put in more debugging code... I'm guessing cassandra choosing sage server is the problem...?

[x] turn off use of cassandra as much as possible in one particular tornado instance and test.

[x] new plan

In [1]: import cassandra
In [2]: cassandra.set_nodes(['combinat4.salv.us','bsd1.salv.us','servedby1.salv.us'])
In [3]: cassandra.get_sage_servers()
Out[3]:
[(u'combinat3.salv.us', 6000),
 (u'combinat2.salv.us', 6000),
 (u'bsd2.salv.us', 6000),
 (u'servedby2.salv.us', 6000)]


OK, the speed problem is *entirely* that opening a new connection to
cassandra is stupid and time consuming.

I ended up working for nearly 2 hours, but now the use of cassandra is massively better.
It automatically fails over only if there is a connection error or it takes more than 1
second to do something.  In that case, screw that server -- try another.

Now we solidly get about 0.09 on fresh comps.

Oct 1, 2012 at 8:05pm:
======================

I'm tired, but could work on salvus for an hour if I can muster the strength.

Upcoming projects:

   - roll out salvus running on new 4-machine private cloud, including databases

   - change combinat to use kvm instead of vbox

   - polish admin.py some more.

   - start working on GUI and functionality now that I have a solid backend

   - support virtualbox VM's in my vm.py script, so I can manage all
     infrastructure via admin.py, instead of kvm only.

I'm going to look into making vm.py work with VirtualBox.  It would be handy.

I restructured the ~/vm/images path on 02salvus and in my code.  I need to update
code and make that change on 01,03,04salvus.  Doing that now.


   cd salvus; git pull wstein@salvus0.salv.us:/Users/wstein/salvus/; cd ~/vm/images; mkdir base; mv salvus_base.img base/salvus.img; emacs /home/salvus/vm/qemu/salvus_base.xml

DONE.

I also just updated the base VM and added ecryptfs-utils.

Re-shrink base image on 01salvus:

Before:
salvus@01salvus:~/vm/images/base$ du -sch salvus.img
8.0G	salvus.img
8.0G	total
salvus@01salvus:~/vm/images/base$ du -sc salvus.img
8359236	salvus.img
8359236	total

time qemu-img convert salvus.img tmp.img

After (surprisingly, not much better):
salvus@01salvus:~/vm/images/base$ du -sch tmp.img
7.8G	tmp.img
7.8G	total
salvus@01salvus:~/vm/images/base$ du -sc tmp.img
8098872	tmp.img
8098872	total


Test, then rsync out to other nodes.

cp salvus.img backups/salvus-2012-10-02.img
mv tmp.img salvus.img
./push

[x] enable passwordless login between 0k-salvus machines

  ssh-keygen -b 2048
  cd .ssh
  cp id_rsa.pub authorized_keys
  cd
  rsync -axvH  .ssh/ 02salvus:.ssh/
  rsync -axvH  .ssh/ 03salvus:.ssh/
  rsync -axvH  .ssh/ 04salvus:.ssh/



[ ] This site looks good for how to convert a qemu image to a vbox image.

  http://en.wikibooks.org/wiki/QEMU/Images#Exchanging_images_with_VirtualBox

  [ ] Convert salvus_base


  [ ] How to make a VirtualBox "differencing image".


[ ] I seriously need to code in admin.py to answer questions:
     - "what would be deployed on a given node?"
     - what is actually deployed on a given node

[x] add base and disk support to admin.py for Vm class.

[x] disk space -- allocate 512GB lvm on 0ksalvus for persistent disk.

      lvcreate --name salvus_persistent --size 512G 01salvus
      mkfs.ext4 /dev/mapper/01salvus-salvus_persistent

Add to /etc/fstab:

        /dev/mapper/01salvus-salvus_persistent  /home/salvus/vm/images/persistent ext4 defaults 1 1

Then:

       mount -a; chown salvus. /home/salvus/vm/images/persistent/; rm -rf /home/salvus/vm/images/persistent/lost+found/

Then we can make a snapshot of persistent disks and back them up using rsync (or mirror) later...

Do this on 02,03,04salvus now:

mkdir -p /home/salvus/vm/images/persistent; chown salvus. /home/salvus/vm/images/persistent; lvcreate --name salvus_persistent --size 512G `hostname`; mkfs.ext4 /dev/mapper/`hostname`-salvus_persistent; echo "/dev/mapper/`hostname`-salvus_persistent  /home/salvus/vm/images/persistent ext4 defaults 1 1" >> /etc/fstab; mount -a; chown salvus. /home/salvus/vm/images/persistent/; rm -rf /home/salvus/vm/images/persistent/lost+found/

DONE.

  [X] no -- learn rsnapshot and setup nightly backups where 0ksalvus is backed up to (0k+1)salvus?

Discussion about backing up sparse files:

  http://www.backupcentral.com/phpBB2/two-way-mirrors-of-external-mailing-lists-3/rsnapshot-24/rsnapshot-questions-96266/

NO -- Potential plan:
   [ ] Make a 400G (or whatever is left) logical volume on each 0ksalvus node and mount it as /rsnapshot

   [ ] 01salvus backs up to 02salvus:/rsnapshot/01salvus/
       02salvus backs up to 03salvus:/rsnapshot/02salvus/
       03salvus backs up to 04salvus:/rsnapshot/03salvus/
       04salvus backs up to 01salvus:/rsnapshot/04salvus/

... I am not sure I agree with this plan. I should only backup what I
*need* to backup and no more.  And that's the cassandra data.

Instead, I should focus on how to do that safely, and archive those
snapshots.  Absolutely nothing else needs to be backed up.

I should:

   (1) make sure all infrastructure configuration, setup, etc., is as
       *automated as possible* and stored in the github salvus repo.
   (2) become very good at backing up cassandra's DB, and have this backup
       be something that is done from within the cassandra nodes, controlled
       by my admin scripts, so it works no matter how they are deployed.
   (3) ensure that *all* state relevant to the salvus app is stored in
       cassandra or github.


[ ] try exporting cassandra to json:

salvus@bsd1:~/salvus/salvus/data/cassandra-0/conf$ export CASSANDRA_CONF=`pwd`

FAIL -- confused by weird snapshot behavior...


[ ] clearing all old snapshots:

   h.nodetool('clearsnapshot')

[ ] making a new snapshot:

   h.nodetool('snapshot')

[ ] Given the simplicity of my data model(s) I could also write my own dump/restore...?

[ ] EVIDENTLY, It is *critical* that JNA is configured to use cassandra properly:

  sudo apt-get install libjna-java

I did not have this installed.

Fix on existing nodes:

   import admin, deploy; reload(admin); reload(deploy); h = deploy.hosts; s=deploy.services
   h('all','ufw disable; killall apt-get; apt-get -y --force-yes install libjna-java; ufw --force enable', sudo=True)
   # BETTER:
         h.apt_install('all', 'libjna-java')

Fix on my base salvus image:

  virsh start salvus_base
  virsh -c qemu:///session qemu-monitor-command --hmp salvus_base 'hostfwd_add ::2222-:22'
  ssh localhost -p 2222
  login and "sudo apt-get install libjna-java", then shutdown.
  # push out the updated base image

Netflix's Priam does all the backup stuff I would want and much, much more: https://github.com/Netflix/Priam
Unfortunately, it is undocumented and only works on AWS with their API's so is useless to me.  However,
it is important to note that they:

    1. do use the incremental backup feature
    2. regularly use their backups to create test clusters for testing things out based on live data.

2. would be good for me to do.

I just tried taking the stateless_exec snapshot directory from one of the cassandra nodes, and following
the directions here to restore to my laptop cassandra instance:

    http://www.datastax.com/docs/1.1/operations/backup_restore

It worked perfectly.  Very cool.

I'll just have to mull over cassandra backup.
In the meantime, doing:

  h.nodetool('snapshot')

and

  h.nodetool('repair')

regularly is a good idea.

-----

I'm at home.  Some over-users have pseudo-crashed combinat again.
I need to move salvus to dedicated infrastructure asap.
Time to move to the salvus nodes.  I don't think there is anything
more I need to do in order to do this (except with cassandra).

Move to salvus:

(done)  1. Replace combinat.math.washington.edu in godaddy dns with ip of 01salvus.
(done)  2. Add ip for 02salvus also, for redundancy.

PLAN:

  [ ] fix vpn broadcast everywhere and restart those tinc's

  [ ] make a completely *new* minimal deployment to 01salvus cluster
      with these rules/properties; except it will still use two old
      cassandras

  [ ] turn off old deployment

  [ ] fix network stuff on all old machines by hand (since vbox isn't automatic).

  [ ] start old machines as part of new deployment.

DETAILS tinc:

     [ ] change all tinc conf's so I have a class A vpn -- just change
         tinc-up generated by admin, by vm, and on every single node;
         I think this just means changing the netmask in tinc-up to
         255.0.0.0 and that is it.

try this on

   - my laptop
   - 01salvus
   - my vm template system


emacs salvus/salvus/data/local/etc/tinc/tinc-up; sudo killall tincd; sudo salvus/salvus/data/local/sbin/tincd


and test.

     [ ] remove combinat0 from hosts/ path on git repo
     [ ] change to using ip addresses everywhere (no more names)

ARGH -- weirdly all the new dell machines are inaccessible over the
network right now, so I can't do anything anyways.  i'll get up very
early tomorrow and work on this after I am doing preping to teach.

LET THIS BE A WARNING TO ME!

Now salvus is running on servedby, combinat (for the sage sessions --
it isn't dead?), and bsd. But it is running!

---------

Oct 3, 2012:

  - i just tested switching to a class A network, and it works fine.  Yes.
  - i fixed the salvus machines and combinat -- indeed, it was ufw gone crazy!

--

I just got approved via wstein@uw.edu for 8 Google Compute Engine
instances for testing.   I will mention this in my talk tomorrow,
and start planning to set them up!


I'm going to try to make it so I can run vm's on geom just as easily as 0ksalvus.

Packages (and important fix):

   sudo apt-get install python-software-properties; sudo add-apt-repository ppa:ubuntu-virt/kvm-network-hang; sudo apt-get update; sudo apt-get install qemu-kvm  guestfish guestmount virtinst; sudo apt-get remove unattended-upgrades; sudo apt-get autoremove

Base image:

   Copy/paste over ssh public key from 01salvus to authorized keys on salvus@geom

   salvus@geom: mkdir vm; mkdir vm/images/; mkdir vm/images/base

# Remove old .libvirt on geom and replace by one from 01salvus, and symlink to

   salvus@geom: rm -rf ~/.libvirt; cd vm; ln -s ~/.libvirt/qemu .; scp -r 01salvus:.libvirt ~/

   salvus@01salvus:~/vm/images/base$ rsync --sparse -axvH ./ geom.math.washington.edu:vm/images/base/

# Making it so virsh works

   echo 'export VIRSH_DEFAULT_CONNECT_URI="qemu:///session"' >> ~/.bashrc; source ~/.bashrc
   virsh list --all

# Fix permissions so that guestmount works (run this *and* put in /etc/rc.local)

   sudo chmod a+r /boot/vmlinuz-* # used by guestmount

Add salvus to the kvm, libvirt and fuse groups in etc/group


Build salvus:

   git clone https://github.com/williamstein/salvus.git; cd salvus/salvus; . salvus-env ; ./build.py --build_all

Setup vpn:

   admin.tinc_conf('geom0',['combinat0','bsd0','01salvus','02salvus','03salvus','04salvus','servedby1'])


Oct 3, 2012
===========

Working on setting up vpn and kvm for other hosts.

I've change the tinc names to match hostnames.  This means I need to
update salvus on all tinc vpn servers, which are:

   '01salvus', '02salvus', '03salvus', '04salvus', 'geom', 'combinat', '

and also salvus_base, which I will want to push out.  So:

 # on 01salvus (say), but could do this on any 0ksalvus or geom or combinat:

    cd salvus; git pull
    virsh start salvus_base
    ssh base.salv.us
    cd salvus/salvus/
    git pull
    sudo shutdown -h now

# launch pushing out new vm:
(This should be in the git repo!)

salvus@01salvus:~$ more vm/images/base/push
#!/usr/bin/env python

import os

BASE = 'vm/images/base/'

for hostname in ['%02dsalvus.math.washington.edu'%k for k in [1,2,3,4]] + \
                ['%s.math.washington.edu'%h for h in ['geom','combinat']]:
    cmd = "rsync --sparse -uaxvH %s %s:vm/images/base/ &"%(BASE, hostname)
    print cmd
    os.system(cmd)

---

Setup to use 01salvus as base git repo:

  cd salvus; git remote rm origin; git remote add origin 01salvus.math.washington.edu:salvus/; git pull origin master


# on 01salvus:

   cd salvus; git pull
   ssh 01salvus "cd salvus; git pull origin master"
   ssh 02salvus "cd salvus; git pull origin master"
   ssh 03salvus "cd salvus; git pull origin master"
   ssh 04salvus "cd salvus; git pull origin master"
   ssh geom.math.washington.edu "cd salvus; git pull origin master"
   ssh combinat.math.washington.edu "cd salvus; git pull origin master"


OK, the above is now automated.  W00t!

Need a script to *restart* tinc.

---

Trying to get rid of last virtualbox vm on combinat.

This is: combinat4.salv.us = 10.38.1.10

I'm first just trying to copy off the cassandra-0 directory, which is actually quite small

Making a test machine on geom:

  salvus@geom:~/salvus/salvus$ ./vm.py --ip_address=10.37.1.10 --disk=cassandra:64

However, strangely the persistent disk on there is *not* sparse.  WEIRD.  So I'll
try on 01salvus.

Actually, this is just something I didn't implement correctly!

I'll see if I can figure this out really quickly.

I think I fixed this problem -- it was due to moving a sparse img file
across filesystems using Python.  At least that was why there was a
problem on salvus.


NEXT, after I completely finish preping my talk, or at least want to work on salvus again,
I want to:

   - lay out my new deployment
   - implement features I need to make it work (especially using
     cassandra on persistent disk nodes where one gets /mnt/cassandra to start with)
   - start up a whole new thing just on 0ksalvus
   - test
   - stop old one
   - expand new one to encompass old one
   - turn off combinat4
   - uninstall virtualbox on combinat
   - test that kvm works on combinat


===

I'm going to give a talk tomorrow on Salvus.  My goals in giving this talk are:

  * clarify for myself what I've done and where I'm going with this project
  * clarify for myself what hardware and organizations are involved
  * clarify for myself what is left to do
  * get useful feedback, and maybe key help

Outline of talk.


Title slide


  Salvus - Adj. meaning well, unharmed, sound; alive; safe, saved

  Pictures


Project goals:

  * project to make Sage (and other math software) very robustly
    available over the internet to a large number of users.

  * generate revenue to support core sage development: working with UW
    C4C on license agreement, payment system, etc., they get 20% of
    revenue and the other 80% goes in the sage foundation account

  * if successful, spin off as a separate company in a few years.

Similar Projects:

  Programming, but no math:
     * http://c9.io -- "Your code anywhere, anytime... Write, run, and debug your code with our powerful and flexible cloud IDE."
     * https://www.appsoma.com/ -- "Code & run in the cloud"
     * http://www.heroku.com/ -- "cloud application platform"

  Math, but no programming or scalable computation:
     * http://www.wolframalpha.com/

  Sage:
     * http://sagenb.org -- online notebooks
     * http://aleph.sagemath.org -- single embeddable sage snippet

  Cloud hosting -- can do everything, but expensive and complicated...
     * http://aws.amazon.com/ -- Amazon Web Services
     * http://www.linode.com/ -- Deploy and Manage Linux Virtual Servers in the Linode Cloud.

Emails:

On Wed, Sep 26, 2012 at 9:54 AM, Chris Seberino <cseberino@gmail.com> wrote:
> I have a Xen Ubuntu Linux 1 Gb RAM virtual machine  from a provider called
> Linode.
>

Linode charges $39.95/month for your 1GB machine.  For a 4GB machine,
which would be more likely to work well, you would have to pay
$159.95/month.

My plan with http://salv.us/ -- which I'm working hard on right now --
is to buy a bunch of hardware (mostly done), host it at UW, write much
better more efficient software to make Sage ("highly") available
through it, and provide enough guaranteed resources for a class like
you have... for *substantially* less than $159.95/month, since the
monthly hosting and network fees I have to pay at UW are super-cheap.

Stay tuned...

 -- William

Longterm Programmng Goals (Features):

  * A very simple starter page where you can type a calculation in a
    box (no login required) with tab completion and interact support.
    If you login, then you can easily browse/search/share all past
    computations you've ever done.  Support for mobile browsers from
    the start.  This is all you will often need.

  * Users can have many projects.  A project is defined by a git
    repository, which salvus stores.  Many users can share the same
    project.  It can be linked to github (or google code, or
    bitbucket, etc.)  Thus a project provides a directory tree of
    files, and there is a revision history.

  * A command line -- alternative way to browse a project, and will
    be like the bash command line.   Can type "sage" to run Sage here.

  * Editing documents -- Salvus will also provide browser-based
    editing of several file types, starting with .sws files (for
    compatibility with sage), but also supporting many other file
    types, and ways of working with documents and data.  Multiple
    users can simultaneously edit a document; changes are broadcast
    and updated on all clients.  Also, editors for .py files, .pyx
    files, csv files, presentations, etc.  Special support for the
    Sage library itself as a project.

  * Publishing -- this is like Heroku or Google App Engine; it will
    support making a persistent Sage-based application available to
    many users, either with a specific URL, or embedded in another
    web page (like with aleph.sagemath.org widgets).  Show example
    on Rob Beezer's website.   Paying users can allocate dedicated
    hosting resources for their app.

  * Graphics -- the entire approach to 2d and 3d graphics in the Sage
    notebook is terrible and will have to be re-done right using some
    combination of SVG, HTML5 Canvas and WebGL.  The eventual goal is
    no Java applets and no static png images.

  * Chat/cross-user messaging -- useful for teaching, online classes,
    tutorials, etc.

However... my entire approach to Salvus is the exact opposite of the
monolithic single-point-of-failure Sage notebook:

  * Taking my time and doing things right (and have thrown away huge
    amounts of code I wrote this summer).  With the sage notebook, I
    wrote something that *looks* and feels a lot like the current
    version in a sleep-deprived month.

  * Architecture is very service-based.  Numerous independent
    components running on dozens of machines communicating securely
    via messages over a VPN.

  * Design goal -- absolutely no single points of failure anywhere in
    the design of salvus.

(Note: The demo at http://salv.us is very unimpressive, because all the
real work is behind the scenes.)

Salvus Technology stack:

  * Physical Hardware and Remote VM's
  * Virtualization:  libvirt + kvm
  * VPN: tinc
  * Database: cassandra
  * SSL: stunnel   -- https://salv.us
  * Load balancing: haproxy
  * Static HTTP: nginx
  * Dynamic HTTP: tornado
  * Math/compute: forking tcp-based Sage server
  * Management: Paramiko + much new code

Client/server communication:

  * SockJS -- websocket connection between browser and tornado, when
    supported; otherwise, falls back to other long-polling approaches
  * TCP + SSL -- standard UNIX Socket connection for non-browser
    clients, e.g, command line.
  * HTTP -- old-fashioned POST for simple HTTP clients


Diagram (make it using mindmap):

   Client    Client    Client   Client   Client  Client
     /|\
      |
   https1.1 (websocket, etc.)
      |
      |   https://salv.us
     \|/
 HAProxy Load Balancers
 /|\       /|\      /|\      /|\
  |         |        |        |
  |https1.1 |        |        |
 \|/       \|/      \|/      \|/
Tornado  Tornado  Tornado  Tornado    <--------------------------->   Cassandra   Cassandra    Cassandra ...
           /|\      /|\                                                 /|\
            |        |        -------------------------------------------|
   ---------|        |        |
   |                 |        |
   |                          |
  \|/                        \|/
 SageServer   SageServer  SageServer   SageServer ...


Discussion of each component -- rest of talk.



Physical Hardware and Remote VM's:
-----------------------------------
  * 4 3Ghz 16-core 64GB RAM, 1TB disk, 1U servers in Padelford basement
  * 1 8-core 32GB RAM machine in my office
  * 3 3Ghz 16-core 64GB RAM, 6TB disk, 1U servers currently being shiped to the UW tower
  * 2 servers at servedby.net (a cloud host)
  * Google Compute Engine: "8 single CPU instances with 100GB of persistent disk space"
  * Google App Engine: I received $60,000 via a research grant program -- not sure how to use it yet!

Current monthly fixed cost: $60/month (for UW tower hosting)

Amazon (in multiple ways) also told me they would give me free
credits, but didn't deliver.  They are big and confused/confusing.

This should be enough to support several thousand paying customers
(and many free customers), which will bring in enough money to expand.

Pricing will likely be around $10/month per user for the first tier of
non-free account, which competes very well with linnode + sagenb
($100/month?).

Goal by end of March 2013: Have 1,500 worldwide paying users at
$10/month, which is what the above hardware should easily be able to
support, done right. That's also more than my entire full-time salary.

Virtualization:  libvirt + kvm:
-------------------------------

  * Virtualization is absolutely essential due to allowing users to
    remotely execute arbitrary code (ok, with sagenb.org nothing is
    virtualizaed, but that is just a ticking bomb, which keeps going
    off!)

  * Public clouds: many companies now rent virtual machines.

  * Private clouds: startup many virtual machines based on a template
    with certain parameters, start services running on them.  When the
    machine shuts down, it leaves no trace (except persistent sparse
    disk images).

  * The *only* persistent data in all of Salvus is in the distributed
    database.  Plan is that this sits in a disk partition that is
    encrypted using the Linux ecryptfs filesystem and a passphrase
    that I type in (exactly once from my management interface) when
    starting the database.

  * OpenStack, CloudStack, OpenNebula, etc. -- war of the private
    cloud software distributions.  I spent quite a bit of time
    investigating all this, and its snake oil right now, at least for
    what I'm doing -- basically a lot of companies and people are
    making noise about implementing something open source kind of like
    what Amazon and VMware already did years ago, but with all the
    real work being doing by libvirt.  (This will likely be useful for
    something in a year or two, but now it is worse than useless.)
    The real work is libvirt, kvm (or xen).  I think Redhat put a ton
    of serious money/development into supporting the creation of high
    quality virtualization support -- KVM -- that is built into the
    Linux kernel now.  This is what I'm using directly... in the end,
    to do everything I want nicely, I wrote a few hundred lines of
    code (vm.py) and learned about how to get around subtle bugs.


VPN: tinc
-----------

  * http://www.tinc-vpn.org/

  * I'm NOT using http://openvpn.net/, even though it is enormously
    popular...  because all traffic goes through a master VPN server,
    which is a single point of failure.

  * tinc is "a Virtual Private Network (VPN) daemon that uses
    tunnelling and encryption to create a secure private network
    between hosts on the Internet."

    Key features:
         - Every public facing machine on the vpn also acts as a
           server, so clients (virtual machines, public cloud
           instances) can connect as long as *any* public-facing
           maching is up.
         - It's P2P -- not all traffic has to go through one
           master node, which could be very inefficient.
         - All traffic is encrypted.
         - Easy/sensible; oo add a machine to the VPN, you just
           generate a public and private key, then add the public key
           to at least one public facing machine.
         - A small C program that builds in seconds, and has been
           under development since 1998.

  * by using a VPN, I can easily combine cloud computers, my own
    computers, etc., and don't have to worry about ip addresses
    changing.  Also, I don't have to worry about securing
    communication between the different services or vm's in salvus.
    For a long time, I was worrying about things such as implementing
    SSL + TCP communication between the Tornado web server and backend
    Sage process; I threw all that code away, and instead just use a
    VPN and have a nice clean class A address space as a bonus.

Database: Cassandra
====================

The database is the only persistent component of Salvus.  It stores
everybody's data -- it's the memory of what the system is.  It's thus
an absolutely crucial design decision.

  * PostgreSQL is very nice, as is MySQL and even SQLite.  I really
    wanted to use all three, and tried very, very hard to do so (and
    wrote a lot of code that I threw away).  Then I started thinking
    seriously about the amount of data I will have to manage with the
    number of users I plan to have in the long run.  And I read books
    about how to scale website using mysql.  What they do is very
    difficult and ugly, e.g., sharding -- a bunch of different
    databases based on usernames.  (I also read a lot about
    SQLalchemy, used PostgreSQL in a big research project, etc.)

  * Then the Padelford server room blew up for the n-th time, and I
    firmly decided on no SPOF's as an *Axiom*.  From this perspective,
    relational databases like PostgreSQL become even more difficult to
    use. You have to have a master database, at least one other slave
    databases replicating it, and some sort of automatic failover that
    is activated when your software decides the master is not working
    (which is not an easy thing to decide!).

      "I'd like to welcome the github ops/dbas to the club of people
      who've learned the hard way that automated database failover
      usually causes more downtime than it prevents."

      http://news.ycombinator.com/item?id=4524340

    Basically, it's not easy.  I want easy!

  * One of the motivations for the recent wave of "NoSQL databases" is
    creation of databases that have no single point of failure.  They
    scale up by just adding more small machines, and one can configure
    how redundant they are.  I had learned a lot about MongoDB (a
    noSQL database) a few years ago, and after getting LMFDB.org to
    use it a lot (and using it for my own database work), I think it
    sucks.... for me at least.

      "Does everyone hate MongoDB?"
      http://news.ycombinator.com/item?id=4570790

    I read about and tried out every noSQL database I could find.  (I
    even wrote one: http://code.google.com/p/nosqlite/) For my
    particular use case, by far the best one is Apache Cassandra
    (cassandra.apache.org), which is an open source database with
    copyright owned by Apache, and also commercial support from a
    company.  It is written in Java, which I'm no fan of, but still I
    really like Cassandra.  Unlike many options, Cassandra is pretty
    heavily used by serious companies.  E.g, Netflix makes enormous
    use of Cassandra, as does Disney.  Cassandra was originally
    written by Facebook, who open sourced it, hence it gained a lot of
    traction in the open source world (unlike say Google's internal
    databases, which they did not open source -- now I understand what
    Craig Citro was talking about last year...).

  * Cassandra seems to be the only database that satisfied my
    constraints (mature, distributed, scalable, support for multiple
    data centers); there wasn't any competition.  I've been using it
    for a while now, and I'm impressed.  I started with version 1.1
    using CQL3, which looks like a restricted subset of SQL, with NO
    JOINS or relations at all between tables!


SSL: stunnel   -- https://salv.us
=================================

  * One of the main motivations for rewriting the sage notebook "back
    in the day" to use twisted was to support SSL, i.e., encrypted
    communcation between the user's browser and sagenb.org.  This
    worked, but we never properly signed the cert, so users got a
    terrifying warning when they visited sagenb.org, and it was deamed
    easier to just remove the security than deal with the cert stuff.

  * It turns out that one can easily make *any* webapp using SSL
    encryption without having to touch the code that defines that
    website -- just use the program stunnel!

  * Getting a properly signed SSL certificate setup isn't impossible.
    (I even bought my cert for $12.99 by some trick... though UW actually
     offers free sigs if you're patient.)

  * https://salv.us works and doesn't give a security warning on any
    devices I've tested it on.

  * This component is a no-brainer.

  * With salvus we run a couple of stunnel's and put them all in DNS, so
    the browser chooses one.  When an stunnel fails, the browser will just
    load a different one.   No SPOF.

Load balancing: haproxy
=======================

The stunnel process directs all requests to another program called
haproxy (which can be running on a different computer or the same
computer).  Haproxy is now the canonical open source load balancer
program.  Haproxy in monitors a bunch of web servers (for salvus, that
means nginx servers and tornado servers), makes sure they are working,
and balancing incoming connections between them.  The static content
requests go to nginx.  The dynamic websocket connections go to tornado
servers.

Static HTTP: nginx
==================

nginx seems to be the canonical choice for serving a huge number of
static webpages efficiently.  It was easy to setup and use.  It's
lightweight and easy to build from source.

nginx is single threaded.  This is absolutely key to its success in
serving huge numbers of requests per second.  It's far more efficient
than the old apache approach, which would spawn a thread or process
per request.


Dynamic HTTP: tornado
=====================

Tornado is basically a clean self-contained nicely coded webserver
written in Python that works a lot like twisted (i.e., it is
asynchronous), but is much simpler and more targeted.  It was written
by some company that Facebook bought, and facebook then open sourced
it.  It's pretty much the only solution I could find for serving
websocket connections using a Python webserver -- I couldn't see a way
to do websockets with the WSGI servers, which most python webservers
are (e.g., flask).

Ipython also uses tornado.

Tornado does well in benchmarks.  It is *single threaded*!

One can read all the documenation for tornado in a few hours, and you
really know all you need to know, except what you'll learn from the
excellent source code.

One concern I have is that right now the way Tornado talks to
cassandra is not asynchronous -- I may have to write a better
driver. Database queries are typically super fast though, by the
nature of cassandra slow queries simply aren't allowed by the
language.


Math/compute: forking tcp-based Sage server
============================================

I wrote from scratch a TCP server that starts up as a Sage process
with various things pre-imported such as graphics, Maxima, etc.,
listens on a socket (running as root), accepts a connection, forks,
lowers privileges, executes blocks of sage code, and streams results
back over the connection.

The design is much different than with the SAge notebook, where one
process watches another.  Here, as code executes, the running process
itself sends messages whenever it wants containing whatever it wants.
The tornado server receives these messages and routes them on (either
directly to a connected client, or to another tornado server to which
a client is connected).

All communication uses Google Protobuf2 binary format for speed and
clarity.  protobuf2 is a brilliant way of *defining* a message
protocol.  And it is very, very efficient and well supported.

Management: Paramiko + much new code
====================================

First I tried to do management by writing a Tornado-based management
web application that had a database backend.  This got painful and
ugly quickly, but looked cool and powerful.

I then wrote a big Python program to do component administration
(starting and stopping the many programs listed above), using mainly
Python Subprocess module, and only got it working on a single machine.

Then -- just as with private clouds, I found that there are tons (and
tons!) of programs out there for managing installing and running
software on a bunch of machine.  I found one called Ansible that I
kind of liked at first, but found it to be too immature and not so
widely used.  There were some other solutions, but they seemed like
massive overkill.  I spent quite a bit of time investigating them.
But for every actual application, there is a bunch of subtle logic
that just has to be figured out and coded, and I had a lot of that down
from my code mentioned above.

I took some of the good design ideas from Ansible, and the really,
really good underlying library Ansible uses -- Paramiko, which is a
nearly complete re-implementation of ssh in Python.  Regarding
Paramiko, it is very good code and it has excellent documentation.
The documentation is a bit odd, because it visibly looks ugly,
unorganized and bad, at first glance.  But when I actually started
*using* it to get work done, it is incredibly good.

So I adopted some of the code mentioned above, and have something that
really works now.  This is actually where *most* of the code I've
written and kept has gone.

-------

Next time I talk, I'll give demos...

Oct 5, 2012:
------------

  I now want to make it so the sage notebook is usable through
  salv.us, but made highly scalable and available! -- why not?

  Here's the overall plan:

    * create a new kind of service designed much like the existing
      sage_server.py (maybe as part of it)
    * it is a forking server that runs as root on a "secure vm"
    * service:
        - create a new connection
        - required timeout parameter -- kill self if nothing happens for t seconds
        - send a collection of .sws files
        - it drops privilegs and starts running as a sage notebook (as
          admin say) with no authentication or login required with
          that collection as sws files; this serves on some port on
          that machine.  the port number is sent back as a message.
        - when a worksheet is saved or is created, it creates and
          sends (over the tcp connection -- via protobuf), the new sws
          file.
        - when connection closed, it destroys all traces of self.
    * once that service exists, I can:
        - have user accounts in salv.us
        - have a link to connect to *their* sage notebook server.
        - have tornado proxy all connections between that server and
          the authenticated user.
        - modify sage notebook server html look and to have a link
          back to salvus.
        - store user .sws's in cassandra.

Things to do right now:

  [ ] implement that hosts thing:

      [ ] make it so the hostname -- if given -- can be used elsewhere.


Oct 6, 2012:
===========

I have around 4-5 hours to do focused work on salvus.  I'm well rested
and have an espresso.  Let's see what happens.

First goal: deal with the new hosts file format.  I'm going to consider
getting rid of the groups and just putting the group info in the hostname.

I just did a test and apparently having a name that resolves to
multiple ip addresses is valid even in the /etc/hosts file.  So I'll
simply make my hosts file a valid /etc/hosts file, which will avoid
lots of potential confusion by other devs.

4:30-7:30: redid the hosts stuff.

[x] when starting a vm service (from admin) do the hostname --> ip_address mapping before ssh'ing.

[ ] to try starting vm's, i need to change the ip addresses of salvus
cluster on vpn to match my new standard.

blastoff:salvus wstein$ ssh salvus@01salvus.math.washington.edu
salvus@01salvus:~$ emacs salvus/salvus/data/local/etc/tinc/hosts/`hostname`

Wait maybe I can just change them on my laptop; git commit, then pull and restart tincd's?

blastoff:~ wstein$ cd salvus/salvus/conf/tinc_hosts/
blastoff:tinc_hosts wstein$

I have to also modify tinc-up on each host.

 cd salvus/salvus; . salvus-env; git pull; emacs data/local/etc/tinc/tinc.conf; emacs data/local/etc/tinc/tinc-up ; restart_tincd

#!/bin/sh
ifconfig $INTERFACE 10.1.1.1 netmask 255.0.0.0

Also useful:

ssh 02salvus.math.washington.edu -t emacs salvus/salvus/data/local/etc/tinc/tinc.conf


---

NOTE to self -- I should provide both the sage notebook *and* ipython
notebook as something available via salvus.  Why not?


Oct 7, 2012:
============

I get about 2 hours or so this morning to work on salvus.  The goal
should just be to get the components of deploy2 to actually work.

I sure would like to make this fast using threads:

 import admin, deploy2; reload(admin); reload(deploy2); h = deploy2.hosts; s=deploy2.services
 s.status('vm')

Right now it takes quite a while, since there are so many vm's.  I've
been meaning to make this optimization for a while.  Let's give it a
shot.

 time len(s.status('vm'))
 # 16
 CPU times: user 0.14 s, sys: 0.04 s, total: 0.19 s
 Wall time: 14.54 s

....

and it worked!  Now after connections made:

time len(s.status('vm'))
CPU times: user 0.08 s, sys: 0.04 s, total: 0.12 s
Wall time: 0.25 s
Out[10]: 16

Versus (after connections made):

time len(s.status('vm',parallel=False))
CPU times: user 0.13 s, sys: 0.04 s, total: 0.17 s
Wall time: 2.81 s
Out[11]: 16



---

Fixing some tincs:

Name = bsd
ConnectTo = 01salvus
ConnectTo = 02salvus
ConnectTo = 03salvus
ConnectTo = 04salvus
ConnectTo = servedby1
ConnectTo = combinat
ConnectTo = bsd
ConnectTo = geom
Device = /dev/tap0


Done.

I just fixed an issue with stunnel control too.


---

The kvm deployment is not rock solid.


"Connected to 10.1.2.2
Internal application error"

Proper shutdown:

    import admin, deploy2; reload(admin); reload(deploy2); s=deploy2.services; h = s._hosts
    s.stop('cassandra', wait=True, parallel=False)

    s.status('cassandra', wait=True, parallel=False)

    s.stop('vm')
    s.status('vm')

WORKFLOW/OPS: It seems critical that I come up with a good way to do upgrades, staging, etc.,

In particular, I need to be able to regularly update the base image without having to restart
all guests.   Or do it in a sensible rolling way...


Oct 8:
========

s.stop('cassandra', parallel=True)
s.status('cassandra', parallel=True)
s.stop('vm', parallel=True)

# quit the session -- due to cassandra connection being broken temporarily!!

s.start('vm', parallel=True)

import time
while True:
    v = h.ping('all')[1]
    if not v: break
    print "Waiting for ", v


s.start('cassandra', wait=True, parallel=True)

s.start('all', wait=True, parallel=True)

[s.start(x, parallel=False, wait=True) for x in 'sage','nginx', 'haproxy','cassandra', 'tornado']

TODO:
  [ ] any timeouts in code that use signals won't work in parallel! -- so find better way to do timeouts...
  [ ] add swap to base machine
  [ ] start running kvm's on geom and combinat

---

So, the web03 vm just "crashed" again.  What is causing this?
Must find out!



virsh -c qemu:///session qemu-monitor-command --hmp web03 'hostfwd_add ::2222-:22'
ssh localhost -p 2222

... nothing, so only a console could help to debug this...

It could be a firewall issue.
It could be lack of swap.
It could be config on 03salvus and 02salvus?

Question, can I attach a console somehow.

With virt-install, it would be:  "--graphics vnc,port=8121"

---

Enable vnc support throughout.   Done and easy and done right :-)

---

DONE Add some vm's on geom/combinat


Oct 8 at 7:09pm.
================

The network on web04 machines failed.

 ssh -L 9003:localhost:9003  salvus@04salvus


Now trying to update/upgrade and reboot 04salvus, etc.   Maybe I didn't do that before properly...?

/etc/grub.d/00_header

---

I reconfigured things, including upgrading, and *still* have the problem with the network...
i.e., web03's network crashed.  :-(  That should not happen.

I will try iperf back and forth between web03 10.1.3.3 and web04 10.1.4.3 to see what happens.


salvus@web04:~$ iperf -p 7000 -s

Woah, web03 died before I could even try it!

I will try restarting web03, *not* starting any services/firewalls running on it, then use iperf:

Then try again with.

web03:~$ iperf -c -p 7000 10.1.4.3

First test:
 175 Mbits/sec


I just tried using a kvm on geom and easily crashed it.  I will stop
using geom/combinat in salvus for now, since they need to be rebooted
after installing the kvm fix.?

I'm really stumped....  Running lots of iperf's yielded no problems
at all with web03 or web04.

Hmmmm.

The problem has returned again now with web04. The next obvious thing
to do is to change vm.py to not use virtio networking and see
if the problem vanishes completely.  That will at least prove that
virtio networking is stil broken (despite the patches).

    user,model=e1000

But first, I want to ensure that I am really running the patched kvm
stuff.  Hmm...

root@04salvus:/etc/apt# dpkg -l |grep virt

is same as on 01salvus.

Looking at

   https://bugs.launchpad.net/ubuntu/+source/qemu-kvm/+bug/997978

it says that a better fix is committed to "proposed", which one enables by adding this
to /etc/apt/sources.list:

    deb http://archive.ubuntu.com/ubuntu/ precise-proposed restricted main multiverse universe

I'm trying this now on 03salvus and 04salvus....

... and it seems to have worked -- no crashes all night long.

I'm going to push the changes to 01salvus, 02salvus, geom, and combinat right now,
reboot 01salvus, 02salvus, and restart salvus.

It seems that geom and combinat now work too (at least iperf passes).


--------------


TODO someday: upgrade to ubuntu 12.10...   http://releases.ubuntu.com/12.10/

Time to think more about SACNAS.  My plan:

  1. Setup a sagenb on each salvus machine customized to allow trivial
     account creation, and call them 01salvus.sagenb.org,
     02salvus.sagenb.org, etc.  Use them.

  2. Create content in shared worksheets.  Make sure students download
     their worksheets at the end instead of depending on the servers
     staying.

  3. If I have time left, try to implement a scalable worksheet server
     and salvus accounts and *use it* this Wednesday.

Plan for 1:
  [ ] Make a sagenb process class in admin.py.
  [ ] Make a deploy directory with hosts and services:
# hosts
10.1.1.1 padelford kvm-host 01salvus
10.1.1.2 padelford kvm sagenb1

10.1.2.1 padelford kvm-host 02salvus
10.1.2.2 padelford kvm sagenb2

10.1.3.1 padelford kvm-host 03salvus
10.1.3.2 padelford kvm sagenb3

10.1.4.1 padelford kvm-host 04salvus
10.1.4.2 padelford kvm sagenb4

# services
[vm]
01salvus   {'hostname':'sagenb1', 'vcpus':16, 'ram':24, 'vnc':9100, 'disk':'sagenb:64'}
02salvus   {'hostname':'sagenb2', 'vcpus':16, 'ram':24, 'vnc':9100, 'disk':'sagenb:64'}
03salvus   {'hostname':'sagenb3', 'vcpus':16, 'ram':24, 'vnc':9100, 'disk':'sagenb:64'}
04salvus   {'hostname':'sagenb4', 'vcpus':16, 'ram':24, 'vnc':9100, 'disk':'sagenb:64'}


  [ ] Edit dns to make salvus1.sagenb.org, salvus2.sagenb.org, etc.

  [ ] setup apache proxy on each host.

---> I just can't do it.  It is so ugly.

####################
# Sage Notebook Server
####################

class Sagenb(Process):
    def __init__(self, address, path, port, monitor_database=None, debug=True, id=0):
        self._address = address  # to listen on
        self._path = path
        self._port = port
        pidfile = os.path.join(PIDS, 'sage-%s.pid'%id)
        logfile = os.path.join(LOGS, 'sage-%s.log'%id)
        Process.__init__(self, id, name='sage', port=port,
                         pidfile    = pidfile,
                         logfile = logfile, monitor_database=monitor_database,
                         start_cmd  = ['sage', '--python', 'sage_server.py',
                                       '-p', port, '--address', address,
                                       '--pidfile', pidfile, '--logfile', logfile, '2>/dev/null', '1>/dev/null', '&'],
                         start_using_system = True,  # since daemon mode currently broken
                         service = ('sage', port))


    def port(self):
        return self._port



Plan for 3 (version 1):

  [ ] User account table.
  [ ] User account creation page.
  [ ] Table that contains columns:
          user_id   complete_tarball_of_a_sagenb
  [ ] Extend sage_server.py to accept a message that is:
           "Create file with given name and contents in current directory."
  [ ] Write a little bit of code that does this:
         "Start a sage notebook server using this tarball on this port."
  [ ] Modify tornado server so that when an authenticated user visits
           http://salv.us/sagenb
      they get their sagenb.

There is no way I can do this in time in a way that I will like.

-----------------

Oct 9, 2012 at 12:28pm

Setting up computers at UW tower data center.

 * Ubuntu 12.04.1 LTS

Subnet: 128.95.242.128/26 (& 172.25.242.128/26)
vlan: 1763
Mask: 255.255.255.192
Gateway: 128.95.242.129
IP numbers available for your use: 128.95.242.133 through 128.95.242.190

(private)
and 172.25.242.133 through 172.25.242.190

(I have Broadcom Corp NetXtreme II BCM57711 10-gigabit PCIe ports...)

Use 128GB for automatic LVM partitioning.

128.95.242.133 05salvus
128.95.242.135 06salvus
128.95.242.137 07salvus
128.95.242.139 08salvus

128.95.242.134 05salvus-admin
128.95.242.136 06salvus-admin
128.95.242.138 07salvus-admin
128.95.242.140 08salvus-admin

[x] openssh server
[x] virtual machine host


# /etc/network/interfaces
# The primary network interface
auto eth0
iface eth0 inet static
address 128.95.242.135
netmask 255.255.255.192
gateway 128.95.242.129
dns-nameservers 128.95.120.1 128.95.112.1 8.8.8.8 8.8.4.4
---



Then on first boot:

* Turn off memtest

* Control-E

Configure iDrac6 ip, user (root), passwd; Enable IPMI over Lan -- (maybe I can eventuall;y figure out how to use it)

Network:

netmask 255.255.255.192
gateway 128.95.242.129
dns-nameservers 128.95.120.1 128.95.112.1 8.8.8.8 8.8.4.4


Add this to /etc/apt/sources.list:

      deb http://archive.ubuntu.com/ubuntu/ precise-proposed restricted main multiverse universe


sudo su
echo "deb http://archive.ubuntu.com/ubuntu/ precise-proposed restricted main multiverse universe" >> /etc/apt/sources.list
apt-get update
apt-get upgrade
apt-get install iperf dpkg-dev texlive make m4 g++ gfortran liblzo2-dev libssl-dev libreadline-dev  libsqlite3-dev libncurses5-dev emacs23 apache2 qemu-kvm  guestfish guestmount virtinst libguestfs-tools git lynx mosh

apt-get remove unattended-upgrades
apt-get autoremove

reboot -h now

# Install salvus

git clone 01salvus.math.washington.edu:salvus/; cd salvus/salvus; . salvus-env; ./build.py --build_all

mkdir ~/vm; mkdir ~/vm/images; mkdir ~/vm/images/base; rsync --sparse -uaxvH 01salvus.math.washington.edu:vm/images/base/ ~/vm/images/base/

# Install and configure tincd:

10.4.1.1 05salvus
10.4.2.1 06salvus
10.4.3.1 07salvus
10.4.4.1 08salvus

# Edit /etc/rc.local

sudo emacs /etc/rc.local
   sudo chmod a+r /boot/vmlinuz-* # used by guestmount
   nice --19 /home/salvus/salvus/salvus/data/local/sbin/tincd







echo "deb http://archive.ubuntu.com/ubuntu/ precise-proposed restricted main multiverse universe" >> /etc/apt/sources.list
apt-get update
apt-get -y --force-yes upgrade
apt-get -y --force-yes install iperf dpkg-dev texlive make m4 g++ gfortran liblzo2-dev libssl-dev libreadline-dev  libsqlite3-dev libncurses5-dev emacs23 apache2 qemu-kvm  guestfish guestmount virtinst libguestfs-tools git

apt-get -y --force-yes remove unattended-upgrades
apt-get -y --force-yes autoremove

reboot -h now


 tar xvf sage-5.4.rc1.tar ; export MAKE="make -j20"; cd sage-5.4.rc1; make ptestlong

DONE at 5:42pm!


----------

I'm doing the following, so that I can make *.sagenb.org servers pointing at them.

128.95.242.133 SALVUSMATH.INFO  (05salvus)
128.95.242.135 SAGEWS.ORG       (06salvus)
128.95.242.137 SAGEWS.COM       (07salvus)
128.95.242.139 MTHYME.COM       (08salvus)


Steps to create salvus*.sagenb.org servers:

  (done) 1. setup dns

(done) -- finish vm host config on 05-08salvus:

 usermod -a -G fuse,kvm,libvirtd,netdev salvus;  chmod a+wr /dev/kvm

-- setup tincd:


cd /home/salvus/salvus/salvus/data/local/etc/
mkdir tinc; cd tinc
echo '#!/bin/sh' >tinc-up
echo "ifconfig $INTERFACE 10.4.1.1 netmask 255.0.0.0" >> tinc-up
chmod +x tinc-up

echo "Name = `hostname`" > tinc.conf
echo "ConnectTo = 01salvus" >> tinc.conf
echo "ConnectTo = 02salvus" >> tinc.conf
echo "ConnectTo = 03salvus" >> tinc.conf
echo "ConnectTo = 04salvus" >> tinc.conf
echo "ConnectTo = 05salvus" >> tinc.conf
echo "ConnectTo = 06salvus" >> tinc.conf
echo "ConnectTo = 07salvus" >> tinc.conf
echo "ConnectTo = 08salvus" >> tinc.conf
echo "ConnectTo = servedby1" >> tinc.conf
echo "ConnectTo = geom" >> tinc.conf
echo "ConnectTo = combinat" >> tinc.conf
echo "ConnectTo = bsd" >> tinc.conf


/home/salvus/salvus/salvus/data/local/sbin/tincd --config /home/salvus/salvus/salvus/data/local/etc/tinc -K


-- do a test of it all on 01salvus


----

virsh -c qemu:///session qemu-monitor-command --hmp web03 'hostfwd_add ::2222-:22'
ssh localhost -p 2222


-------------

MAKING A NEW BASE: (needs to be put in scripts/)

     export PREV=salvus1
     export NAME=salvus2
     qemu-img create -b ~/vm/images/base/$PREV.img -f qcow2 ~/vm/images/base/$NAME.img

     virt-install --cpu host --network user,model=virtio --name $NAME --vcpus=8 --ram 4096 --import --disk ~/vm/images/base/$NAME.img,device=disk,bus=virtio,format=qcow2,cache=writeback --noautoconsole  --graphics vnc,port=8121
     virsh -c qemu:///session qemu-monitor-command --hmp $NAME 'hostfwd_add ::2222-:22'; ssh localhost -p 2222

        cd salvus; git pull https://github.com/williamstein/salvus.git

     virsh --connect qemu:///session undefine $NAME
     virsh --connect qemu:///session destroy $NAME
     virsh --connect qemu:///session list --all

     cd ~/salvus/salvus; . salvus-env;  push_vm_images_base.py


import admin; admin.tinc_conf('10.4.3.1')



v = h.ping('all')[0]; v.sort(lambda x, y: cmp(x[1],y[1])); v

-----------

OK, the VM machine farm is working.




------------


  2. create sagenb.py script that can safely run sagenb (starting as root and dropping priv) on my current vm's

(done)



  4. configure apache2 on each machine to proxy to sagenb.


 cd /etc/apache2/mods-enabled; ln -s ../mods-available/proxy* .; ln -s ../mods-available/rewrite.load .


<VirtualHost *>
  RewriteEngine On
  ServerName salvus5.sagenb.org
  ProxyPreserveHost On
  ProxyPass    / http://10.123.1.5:8080/
  ProxyPassReverse / http://10.123.1.5:8080/
  ProxyTimeout 30
  DocumentRoot /
        <Location />
           DefaultType text/html
        </Location>
</VirtualHost>



-------

Create Sagenb process object:

  python ./sagenb.py --daemon --pidfile=pid --logfile=log --address=10.123.1.1 --port=8080 --path=/mnt/sagenb


--
  3. deploy with a vm on each machine

-- need to make new base salvus2.img.  done.


# send out persistent sagenb image:

 rsync --sparse -uaxvH /home/salvus/vm/images/persistent/sagenb1-sagenb.img 05salvus:vm/images/persistent/sagenb5-sagenb.img


5. Push out /mnt/sagenb/notebook.sagenb from sagenb1 to sagenb2, 5,6 to avoid password typing and having to copy persistent images via rsync:

#   sudo rsync -axvH /mnt/sagenb/ salvus@10.123.1.5:/mnt/sagenb/

sudo rsync -axvH /mnt/sagenb/ root@10.123.1.2:/mnt/sagenb/

NEED TO ENABLE ROOT LOGIN on salvus-base...

Fix httpd.conf's:

NameVirtualHost *:80


<VirtualHost *:80>
  RewriteEngine On
  ServerName salv.us
  DocumentRoot /var/www/
</VirtualHost>


<VirtualHost *:80>
  RewriteEngine On
  ServerName salvus1.sagenb.org
  ProxyPreserveHost On
  ProxyPass    / http://10.123.1.1:8080/
  ProxyPassReverse / http://10.123.1.1:8080/
  ProxyTimeout 30
  DocumentRoot /
        <Location />
           DefaultType text/html
        </Location>
</VirtualHost>

----

Oct 11, 2012:
---------------
5:47pm.

  [x] get salvus working on my laptop again:

import admin; reload(admin); s = admin.Services('conf/deploy_local/', username='salvus'); h=s._hosts

Fail -- I need to update the config format.

Ooops... I think starting it locally just accidentally deleted key files!  Tread with care!

    707             # clever idea from john a. meinel: map the error codes to errno
--> 708             raise IOError(errno.ENOENT, text)
    709         elif code == SFTP_PERMISSION_DENIED:
    710             raise IOError(errno.EACCES, text)

IOError: [Errno 2] No such file

--

  [x] make https://salv.us/about have the static content (e.g., my talk, etc.), and get rid of link to "preliminary demo"

  [x] make https://salv.us/ have the current compute demo, but have an about link.

However, there is a problem.  When I click on the /about (or "about") link from https://localhost/
I end up at http://localhost:8080/about/.  Also, just typing

   https://localhost/about

redirects immediately to http://localhost:8080.  This is surprising.
I wonder if this is happening for all the subdirectories?  Could it be
why IE gives those security warnings about part of the page being
insecure?!

Strange: If I go to this URL (with the /), things work fine

   https://localhost/about/

[x] do the same as above for the mobile site!


--

Incidentally, this is in an interview with Linus I read today: " The
only way that problems get solved in real life is with a lot of hard
work on getting the details right. Not by some over-arching ideology
that somehow magically makes things work."  I think with Salvus I'm at
the stage where I've chosen basically all the technology I need to
choose (maybe I need more bootstrap.js), and it is now all about
polishing and getting the details right.



-----

  [x] salvus: push out this code + test.

Plan:

[x]  - make a new .img version with the latest code:

 -- snippet below
 -- cd salvus; git pull https://github.com/williamstein/salvus.git
 -- apt-get update; apt-get upgrade
 -- sudo reboot # test it starts

[x]   - change the base image definition for the nginx vm's
(success)  -- restart just one and test by directly connecting to it.
  -- restart all.

[s.restart('vm', hostname='web0%s'%i,wait=False) for i in [1,2,3,4]]; s.wait_until_up('web04'); [s.start('all', 'web0%s'%i,wait=False) for i in [1,2,3,4]]



[x]   - try restarting all of the vm machines and any services that run on them.

Then try changing config so haproxy binds to port 80 and redirects traffic.

s.restart('haproxy', '02salvus')

 ssh 03salvus -t "sudo apt-get -y --force-yes remove apache2; sudo apt-get  -y --force-yes autoremove"


----

     virsh list  --all # check that no base vm is running
     export PREV=salvus5
     export NAME=salvus5b
     qemu-img create -b ~/vm/images/base/$PREV.img -f qcow2 ~/vm/images/base/$NAME.img

      virt-install --cpu host --network user,model=virtio --name $NAME --vcpus=16 --ram 32768 --import --disk ~/vm/images/base/$NAME.img,device=disk,bus=virtio,format=qcow2,cache=writeback --noautoconsole  --graphics vnc,port=8121
     virsh -c qemu:///session qemu-monitor-command --hmp $NAME 'hostfwd_add ::2222-:22'; ssh localhost -p 2222

       sudo apt-get update; sudo apt-get upgrade;
       sudo reboot -h now
       cd salvus/salvus; git pull https://github.com/williamstein/salvus.git
       . salvus-env

# for example:
       ./build.py --build_stunnel --build_nodejs --build_nodejs_packages --build_haproxy --build_nginx --build_cassandra --build_python_packages

     virsh --connect qemu:///session undefine $NAME
     virsh --connect qemu:///session destroy $NAME
     virsh --connect qemu:///session list --all

     cd ~/salvus/salvus; . salvus-env;  push_vm_images_base.py
------------

[ ] BIG TASK: deploy more robustly -- I'm going to extend my salvus deployment to 2 more nodes in the UW tower.

   phase 1:
    [x] add 05salvus, 06salvus to ip list for salv.us
    [x] shutdown testn VM's and remove from hosts/services
    [x] configure hosts file with more vm's
    [x] configure services file with more services on 05salvus and 06salvus
    [x] start vm's
    [x] start web services on vm's
    [x] test that is working

   phase 2:
    [ ] configure 4 more cassandra nodes to tower center (for a total of 8)

 - reading http://www.datastax.com/docs/1.1/operations/cluster_management#adding-capacity
 - for now I'll add 4 new nodes, hence doubling the size.
 - run this code:
N=8;
for i in range(N):
    print  i * (2**127 / N)
 - for now, I'm evidently not yet using the NetworkTopologyStrategy,
   so the topology option is trivial.  I will have to switch when I
   get serious...

- start first node:

   import admin; reload(admin); s = admin.Services('conf/deploy2/', username='salvus'); h = s._hosts
   s.status('vm', hostname='cassandra05')
   s.start('cassandra', 'cassandra05')

Nothing happened.    Maybe I forgot to set a property:

    http://www.datastax.com/docs/1.1/configuration/node_configuration#init-properties

Nope, evidently the problem was that I had set new nodes and this node
to be the seed.  I changed that so *only* old nodes were seeds and now
it is "joining".... but it still failed completely again.  Hmm.

I know!  It's the firewall.  Cassandra nodes have a whiteless firewall, and it's now wrong
for the first 4.

So... I will:

 [x] set seeds back on.
 [x] restart the first 4 cassandra procs (really just to fix their firewalls):

[s.restart('cassandra', 'cassandra0%s'%i) for i in range(1, 5)]


- suddenly the node I started above has joined the cluster -- but it has an incomplete view of the data.
- I'm trying "repair" on it... and now it is fine.

 [x] then add new nodes. -- this is working perfectly.

node = 'cassandra08'; s.start('vm', hostname=node); s.wait_until_up(node);  s.start('cassandra', node, wait=False)
h.nodetool('netstats', node, wait=True)
h.nodetool('repair', node, wait=True)
h.nodetool('ring', node, wait=True)


[x] SERIOUS BUG:

   When creating the cassandra nodes above, I created *two* machines
   with persistent images on the same host at the same time.  This
   failed.  The reason is because the command I use to make the image
   creates the same filename for both, the moves it!  Need to use
   a temporary subdirectory.

Two options to fix this:

  (1) figure out if/how to get guestfish to directly create img with name other than test1.img
or
  (2) make test1.img in unique subdirectory, then move it out.

According to the official manpage, "Prepared disk images are written
to file "test1.img" in the local directory."... so option (1) isn't an
option, evidently.  So I'll do (2).

Yes, the bug is fixed.  Nice.

----------------------
[x] BIG TASK: Implement user account creation and login.

   - no fancy html yet!
   - user *must* have a valid email address
   - limited number of accounts per email address
   - blanket "terms of usage screen"
   - approval process for users to do anything nontrivial.  This I've learned!
   - One big mistake with the sage notebook is having multiple
     accounts all linked to the same *person*.  We could get around
     that by requiring a verified email address and considering that
     to be canonical... and only having one account per email address.
     But don't make it the username for spam reasons...
   - maybe for now I should only allow login via google... since I've
     implemented that already, and it's safer than allowing anybody.
   - authentication should be plugable (witness the LDAP sagenb debacle)

[x] test out my existing google oauth thing to see if it still works
     - it does!

[x] look at what it gives and think.
[x] write some code in cassandra.py


  get_users_permissions(email_hash)  # things user with given email can do  (keyed on *hash* of email)
  stateless_exec(email_hash, ...)    # cursor over stateless exec by this user  (extra stuff to search by range, sorted by date)


** very interesting discussion about how to use sockjs with authentication **

      https://github.com/mrjoes/sockjs-tornado/issues/19

[x] decide on schema for account table
     standard:
        - user_id (counter)  -- row key
        - username          -- indexed
     optional:
        - password_hash
        - email address
        - google openid code (whatever that is)
        - etc.
[x] write cassandra.py code to create account table
[x] write cassandra.py code to implement basic operations:
        - create_new_user (input user_name; returns id)
        - changer_user_property (property:value)
        - get_user_property
[x] account creation -- where/how?
        - login and creating an account *could* be handled entirely
          through the websock connection... though what about a
          persistent cookie?

cqlsh:salvus> alter table users add time_limit int;
cqlsh:salvus> update users set time_limit=10 where email_sha1='abc';
cqlsh:salvus> select * from users;
 email_sha1 | name | time_limit
------------+------+------------
        abc |  foo |         10

cqlsh:salvus> update users set name='foo bar' where email_sha1='abcxyz';
cqlsh:salvus> select * from users;

---

This is an excellent clear systematic formal description of CQL3:

   http://cassandra.apache.org/doc/cql3/CQL.html

============
----------

[ ] BIG TODO: version upgrades -- everything but tinc is behind!

For each:

1. create new file in src/
2. test build locally and running
3. check into repo and push

  [x] cassandra: 1.1.2 --> 1.1.5

From http://www.datastax.com/download/community
Get this: http://downloads.datastax.com/community/dsc.tar.gz
but filename changes on click...
Then:

cd salvus/salvus/src;
git rm dsc-cassandra-1.1.2.tar.bz2
tar jxf dsc-cassandra-1.1.5-bin.tar.gz
tar jcf dsc-cassandra-1.1.5.tar.bz2 dsc-cassandra-1.1.5
rm -rf dsc-cassandra-1.1.5-bin.tar.gz dsc-cassandra-1.1.5
# elsewhere >>>  import admin; reload(admin); s = admin.Services('conf/deploy_local/', username='wstein'); h=s._hosts; s.stop('cassandra')
cd ..; . salvus-env; ./build.py --build_cassandra
# elsewhere >>> s.start('cassandra')

It did not work.  Somehow the schema got all corrupted:

cqlsh:salvus> select * from sage_servers;
 address   | column1 | value
-----------+---------+-------
 127.0.0.1 | running |  \x01
 localhost | running |  \x00

Note the "column1", which should be "running". and the values should be boolean.

I installed the old version of cassandra and the old data works again.

I then did what the instructions at http://www.datastax.com/docs/1.1/install/upgrading#completing-upgrade say,
namely

   nodetool repair
   nodetool drain

and it works fine now.  Cool.   So be careful when upgrading!

  [x] haproxy: 1.5-dev11 --> 1.5-dev12

Download (very, very slow!) from http://haproxy.1wt.eu/#down
http://haproxy.1wt.eu/download/1.5/src/devel/?C=M;O=D

It just worked no problem at all.

  [x] nginx: 1.3.2 --> 1.3.7

Fast from here http://wiki.nginx.org/Install
Seems to have worked perfectly.

  [x] stunnel: 4.53 --> 4.54

Get it here: http://www.stunnel.org/downloads.html
It just worked easily, with no problems.

  [ ] protobuf: 2.4.1 --> 2.5.0

There's no tarball, so I did the following on 05salvus:

   svn export http://protobuf.googlecode.com/svn/trunk/ protobuf-read-only
   mv protobuf-read-only protobuf-2.5.0
   cd protobuf-2.5.0
   sudo apt-get install libtool automake
   ./autogen.sh
   cd ..
   tar jcvf  protobuf-2.5.0.tar.bz2 protobuf-2.5.0
   rm -rf protobuf-2.5.0

then copy back to laptop and test.

Then I do:

   ./build.py --build_protobuf

... and the build fails on OS X.   Screw this... I'll wait for the next release.

Once *every upgrade* works locally:

 [ ] make new base vm "salvus4"
 [ ] from scratch rebuild of salvus code on that
 [ ] set base in services file for every service
 [ ] be very careful with cassandra nodes -- do "nodetool repair" and "nodetool drain" before shutting them down.
 [ ] restart everything
 [ ] test

---

[ ] SMALL TODO -- make it so I can do:

   s.firewall(... same options as start/stop/etc. but resets the firewall with current config ...)

    [ ] restart and test that it is working.


[ ] SMALL BUG -- nodetool isn't working locally:

In [20]: import admin; reload(admin); s = admin.Services('conf/deploy_local/', username='wstein'); h=s._hosts

In [21]: h.nodetool('ring', wait=True)
wstein's password:
INFO:root:127.0.0.1 (sage):
INFO:paramiko.transport:Connected (version 2.0, client OpenSSH_5.6)
INFO:paramiko.transport:Authentication (publickey) failed.
INFO:paramiko.transport:Authentication (keyboard-interactive) successful!
INFO:paramiko.transport:Secsh channel 1 opened.
INFO:root:hostname=127.0.0.1, command='salvus/salvus/data/local/cassandra/bin/nodetool ring'
INFO:root:{'exit_status': 127, 'stderr': 'bash: salvus/salvus/data/local/cassandra/bin/nodetool: No such file or directory\n', 'stdout': ''}
('127.0.0.1', 'sage') :  bash: salvus/salvus/data/local/cassandra/bin/nodetool: No such file or directory

[ ] VERY SOON -- implement a max number of simultaneous sage sessions
that sage_server will allocate at once... to avoid trivial DOS.  Make
part of startup options.  Return a message if fail.

[ ] maybe upgrade sockjs (?): june 26 --> oct 3

-------------

TIRED/Sat Oct 13 at 7:58pm...


salvus -- It's 6:52pm and I'm laying important groundwork for writing
the salvus client browser code.  My experiments earlier this summer
make me concerned about javascript:
   (1) the resulting code is pretty big -- e.g., ipython notebook is huge
   (2) the code can be very hard to read/follow/organize...

So I'm looking into CoffeeScript.

Iced CoffeeScript -- better async...

   http://maxtaco.github.com/coffee-script/

    npm install -g iced-coffee-script

Neato:

coffee> foo = (a,b) -> [a*b, a+b, a/b]
[Function]
coffee> [x,y,z] = foo(2,3)
[ 6, 5, 0.6666666666666666 ]

Reading this article about how Dropbox just switched from Javascript to CoffeeScript a few
weeks ago (!) https://tech.dropbox.com/?p=361

Also, they mention Python is used massively by DropBox.

I need to put this in my workflow: "In production, we compile and
concatenate all of our CoffeeScript source into a single JavaScript
file, minify it, and serve it to browsers with gzip compression."

How to test javascript code:  http://pivotal.github.com/jasmine/

   "Jasmine is a behavior-driven development framework for testing JavaScript code."

I will *have* to get Jasmine up and running ASAP, since it's a good
way of testing that salvus works.

Debugging coffeescript in the browser... http://ryanflorence.com/2012/coffeescript-source-maps/

I installed emacs coffee-mode from

   https://github.com/defunkt/coffee-mode
   https://raw.github.com/defunkt/coffee-mode/master/coffee-mode.el

by putting the script in ~/.emacs-scripts/, M-x byte-compile-file,
and adding this to my ~/.emacs file:
   (load "coffee-mode")
The syntax hilighting looks good.

If I wanted 4-space indent:
   (custom-set-variables '(coffee-tab-width 4))


[ ] Rewrite index.js using CoffeeScript and see what it is like.

 First, I go to this website and paste in the code:

  http://js2coffee.org/

Nice, but I'm not sure about the ordering... so I reordered everything
to be more logical.  Then:

blastoff:static wstein$ cp index.js  ~/tmp
blastoff:static wstein$ coffee -w -c `pwd`

I also installed the CoffeeConsole plugin for chrome... which seems very useful!

The website http://js2coffee.org/ only does 2-space indentation.  If I
use 4, I need to fix that.

Remark -- it is very cool how http://js2coffee.org/ gives
functionality... then has a button to get more info below.
That's the sort of thing I want with salvus.

Do a local install of js2coffee:

     npm install -g js2coffee

To switch to 4 spaces:

  1. Add to .emacs as mentioned above
  2. Use js2coffee on the command line, as usual
  3. Then use tab2space to convert the resulting file.

I'm going to redo index.coffee...

actually the steps above fail; maybe js2coffee does *not* output tabs?  Yep it doesn't.

I will just try this script in python now:

   filename='index.coffee'; s = open(filename).read().replace(' '*2, ' '*4); open(filename,'w').write(s)


Nice page about jquery + coffeescript: http://www.coffeescriptlove.com/

Very cool library for doing 2d HTML5 canvas graphics:  http://kineticjs.com/

I'm going to convert my other js...

Converting sagews.js --> salvus.coffee, is getting confused / not
working automatically, mainly due to sagews.js not really being
properly object oriented.  I'll fix this first thing in the morning...

See Object Orientation here:
   http://css.dzone.com/articles/its-time-learn-coffeescript?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%3A+zones%2Fcss+(CSS+Zone)


------------------------------------------------------------------------

  [x] finish porting "salvus.coffee"... currently broken

Testing starting/stopping of tornado and making sockjs connection:

In [13]: !kill -STOP 28228
In [14]: !kill -CONT 28228

  [ ] try to find some way to use javascript/node (?) to test my site???  even something very, very minimal.

  [ ] implement "logged_in" message type (in proto?) -- see comment in salvus.coffee



IDEA: for picking files -- https://www.filepicker.io/products/javascript_v1/


  [ ] I'm tempted to try writing tornado_server.py in NodeJS just
      because NodeJS has so much momentum, e.g., it has a CQL driver.

        https://github.com/racker/node-cassandra-client

Here's doing something in coffee:

  $ npm install cassandra-client
  $ coffee
  coffee> System = require('cassandra-client').System
  coffee> sys.describeKeyspace('salvus', (err, def) -> console.log(def) if not err)

It has pooled connections!

  coffee> PooledConnection = require('cassandra-client').PooledConnection
  coffee> connection_pool = new PooledConnection({'hosts': ['localhost:9160'], 'keyspace': 'salvus'})


  coffee> Connection = require('cassandra-client').Connection
  coffee> con = new Connection({host:'localhost', port:9160, keyspace:'salvus'})
  coffee> con.connect(->)    # see http://stackoverflow.com/questions/9076046/nodejs-cassandra-client-node-cassandra-client
  coffee> con.execute('select * from sage_servers', [], (err, rows) -> console.log(rows))
  coffee> con.execute('select input from stateless_exec', [], (err, rows) -> console.log(rows[0..5]))


  coffee> walltime = () -> (new Date()).getTime()
  coffee> t=walltime(); con.execute('select input from stateless_exec', [], (err, rows) -> console.log(walltime()-t))
  coffee> 141

  coffee> t=walltime(); con.execute('select input from stateless_exec', [], (err, rows) -> console.log(walltime()-t))
  coffee> 106

The Python driver seems way faster for this:

  In [17]: t=time.time();  cassandra.cursor_execute('select input from stateless_exec'); (time.time()-t)*1000
  Out[17]: 28.01799774169922

(With same timings if we read one thing from the cursor.)

I think I'll not worry about "switching to NodeJS" for now.



Tuesday, Oct 16, 2012:
=======================

Actually, I'm going to attempt to rewrite tornado_server.py using
NodeJS, mainly because there is a huge amount of momentum behind
nodejs right now and because of CoffeeScript.

Things my tornado server does:

  * sockjs server for browser clients
  * connect to cassandra via CQL3  (and create schema, provide model)
  * connect to sage servers via TCP
  * work with protobuf messages
  * oauth (at least with google)
  * connect to other tornado servers to forward messages to clients
  * provide a command line interface (as a deamon or not)
  * coordinate execution of code

Plan (what is below):

  * figure out what the options are (list websites, etc.) for node.js
    so that it can do all of the above; if any are completely missing,
    stop.

- https://github.com/joyent/node/wiki/modules

  * write sample snippets of code that show how do to do each thing.

  * write the actual new code.

Most depended on packages:

   * https://npmjs.org/browse/depended
   * http://toolbox.no.de/

Version of nodejs in Ubuntu 12.04.1: v0.6.12; on laptop (OS X): v0.6.15; latest released: v0.8.12

Writing async code: https://github.com/caolan/async

[x] Building from source
--------------------

  * motivation: " in my personal experience, using a package manager
    like Homebrew or Aptitude is a recipe for disaster. These projects
    are moving very quickly, and are still very young as of the time
    of writing this."  http://joyent.com/blog/installing-node-and-npm/

  * download link here: http://nodejs.org/download/
    to: http://nodejs.org/dist/v0.8.12/node-v0.8.12.tar.gz
    - ./configure --prefix-?; make -j30
    - build time on 05salvus in parallel:
         real    0m34.221s
         user    5m52.274s
         sys     0m29.362s
    - build time on laptop:
         real	2m45.748s
 	 user	7m39.467s
	 sys	0m49.534s
    - includes its dependencies :-)
    - npm:
         export ??? PATH=`pwd`:$PATH # in node build
         # move out
         git clone git://github.com/isaacs/npm.git && cd npm && make

Files for examples/demos of everything: salvus/notes/node

[x] SockJS Server
-------------

  * https://github.com/sockjs/sockjs-node
       -- looks rock solid.

$ npm install sockjs
$ npm install node-static
See salvus/notes/node/chat


[x] CQL3
----

--> * https://github.com/simplereach/helenus
         -- supports CQL3, had connection pool, etc.
  * https://github.com/racker/node-cassandra-client
       -- does *not* support CQL3, at least as of 2 months ago -- https://github.com/racker/node-cassandra-client/issues/38

[x] USE  -- node/cql/test3.coffee  # helenus seems to work great

helenus = require('helenus'); pool = new helenus.ConnectionPool(hosts:['localhost'], keyspace: 'salvus', timeout: 3000, cqlVersion: '3.0.0'); pool.connect( (err,keyspace) -> ); execute = (cql,v=[]) -> pool.cql(cql, v, (err, results) -> console.log(results))

execute('select * from sage_servers')

Nice talk?   http://www.datastax.com/wp-content/uploads/2012/08/10_SimpleReach_NodeJS-Helenus.pdf


[x] Protobuf: conclusion -- I'll switch back to JSON, but keep a file describing the message format.
--------

   * http://stackoverflow.com/questions/12225253/which-node-js-module-should-i-install-to-be-able-to-use-google-protocol-buffers
   * https://github.com/chrisdew/protobuf

----->   * a thought -- perhaps I should use JSON, but use the protobuf spec to *describe*
     my message format?  <-----

** USE **

blastoff:proto wstein$ ln -s ~/salvus/salvus/data/local/include/google .
blastoff:proto wstein$ export CPPFLAGS="-I`pwd`"
blastoff:proto wstein$ npm install protobuf
blastoff:proto wstein$ protoc --descriptor_set_out=mesg.desc --proto_path=$HOME/salvus/salvus --include_imports $HOME/salvus/salvus/mesg.proto
blastoff:proto wstein$ node

 var fs = require('fs');
 var Schema = require('protobuf').Schema;
var schema = new Schema(fs.readFileSync('mesg.desc'));


[ ] Authentication
------------------

   * This looks very good, since it supports *tons* of different sites:
        https://github.com/ciaranj/connect-auth

   * This looks even better: http://passportjs.org/
        - excellent documentation
        - normalized user profile info: http://passportjs.org/guide/user-profile.html
        - argument by passport author about why he thinks it is better than everyauth:
              http://stackoverflow.com/questions/3498005/user-authentication-libraries-for-node-js

-->* This one might be even better yet: https://github.com/bnoguchi/everyauth

...




[x] CLI
----------

   * "optimist" -- https://github.com/substack/node-optimist
        - and: http://docs.nodejitsu.com/articles/command-line/how-to-parse-command-line-arguments
        - "optimist is really good because it does not require configuration at all."
        - "Seconding optimist. One of the absolutely sweet piece of module to work with."
        - "no config doesn't mean good if you want the free documentation, other than that I'd say optimist is sweet"

   * commander -- https://github.com/visionmedia/commander.js
        - looks very good as well
        - blog: http://tjholowaychuk.com/post/9103188408/commander-js-nodejs-command-line-interfaces-made-easy
        - "I recently changed from optimist to commander for all our internal tooling. The api feel really natural but what else would you expect from tj, he's an API god."
   * nomnom -- https://github.com/harthur/nomnom
        - looks good too

[x] USE  -- see notes/node/cli




[x] Daemon
----------


 --> * https://github.com/niegowski/node-daemonize2
         - simple and exactly what I need

     * https://github.com/nodejitsu/forever
         - very complicated and does way too much

[x] USE:

blastoff:daemon wstein$ npm install daemonize2
!! Hmm, exactly the code on their site does not work!!
blastoff:daemon wstein$ node app2.js start
Starting sampleapp daemon...
sampleapp daemon failed to start:  Module '/Users/wstein/salvus/notes/node/daemon/app2.js' stopped unexpected

OK, really need two scripts.  That works.

Try forever:

blastoff:daemon wstein$ npm install forever-monitor

wait... surprisingly, a little module called "init" actually does what I want?!

blastoff:daemon wstein$ more in.js
init = require('init');

init.simple({
    pidfile : 'myprog.pid',
    logfile : 'myprog.log',
    command : process.argv[3],
    run     : function () {
        while(1) {}
    }
})

This works perfectly... no.?

I've looked at probably a dozen daemon modules!

Finally, this one looks good:

   https://npmjs.org/package/start-stop-daemon

This is AWESOME -- it built on that "forever" script, but works
with a single process, which of course I want.   As an added bonus
if the server crashes it is automatically restarted.

** SOLUTION **

  Definitely *use* start-stop-daemon.

  It works. And it works with command line options.  It also gives
  the added robustness of automatic restart.   See the file soln.coffee:

----


[x] USE -- see daemon/soln.coffee

start-stop-daemon works super well, though it is crucial to compile
the code to js first, then run it. But that is OK.

[x] Logging to a file
-----------------

-->    * https://github.com/flatiron/winston
         - looks very good, easy, similar to python logger

    * https://github.com/nomiddlename/log4js-node
         - looks confused/harder to use/not as documented

[x] USE -- see daemon/soln.coffee
    Winston works well!

[x] web framework -- Express ?
===================

  * https://github.com/mauricemach/zappa
   - basically express + coffeescript.


  * https://github.com/visionmedia/express

Demo:
Made salvus/notes/node/express/hello/package.json, and app.coffee.
Typed "npm install"
Then run webserver with:  coffee app.coffee

The page http://expressjs.com/guide.html  talks about "Express behind proxies"
which looks *very* important -- "


[ ] TCP to Sage
---------------

--> * http://nodejs.org/api/net.html

    * I should write this as its own stand-alone library, so I can
      easily embed sage in any nodejs app.


[ ] TCP to other Servers
--------------------

-->   * http://nodejs.org/api/net.html

   * Or maybe use dnode: https://github.com/substack/dnode
      "dnode is an asynchronous rpc system for node.js that lets you call remote functions."

   * This could be useful: https://github.com/weixiyen/messenger.js
      - "Messenger.js is a library that makes network communication via JSON dead simple and insanely fast!"
      - It is written in coffeescript.


Oct 17, 2012:
============

7:40am Very quick 1-hour session before teaching prep

1. I just did http://stackoverflow.com/questions/9881528/coffee-script-path-exists-deprecated-notice-from-node-js
   i.e., emacs /Users/wstein/local//lib/node_modules/coffee-script/lib/coffee-script/command.js
   and change path.exists to fs.exists.

7:31pm.  Now I'm going to figure out daemon in node.

Went well

8:30pm:  I'm going to try to add a bit more to node_server.coffee.  What remains:
----




   [x] sockjs server

This book on coffeescript looks very, very good:  http://coffeescriptcookbook.com/

    [x] test use of cql from node_server.
    [x] read "Server-Side JavaScript Injection"
    [ ] (not done) read the coffeescript cookbook straight through


Oct 19, 2012:
5:30am-10am

   [x] CQL cassandra database connection
   [x] test sockjs works with node_server

   [ ] authentication
   [ ] handling evaluation
   [ ] integration with admin.py:
          -  node_secrets

I'm going to hit this very focused now for the next two hours:

[ ] connection to sage (need to make sage accept json format -- lot of work!)
    * Basically I have to intrusively change everything in my stack to use JSON and get rid of protobuf.

     [x] skim through sage_server.py
     [x] rewrite sage_server.py to use JSON  -- turned out to take < 30 minutes total!
     [x] test client part to make sure it works

     [ ] delete all protobuf stuff from salvus:
             - source code
             - build
             - mesg.proto, etc.


Oct 20, 2012:
approx 5am - 10am salvus session:
    [x] reading a lot.

    [ ] someday -- later (!) maybe -- I should make sure and compress the JSON messages using zlib (say):
               http://nodejs.org/api/zlib.html#zlib_zlib
        Because "cpu cheap/bandwidth expensive"
        But if can't decompress in browser this may be stupid.  Also, what if sockjs already compressed?
        Anyway, there may be hidden costs.

    [ ] implement support for sage_server from nodejs, e.g, sage.coffee

      [x] separate out message code, in a way that can be used by both javascript client and server

      [ ] make it so sockjs connection can actually be tested to work
          (e.g., return answers to requests)

      [ ] write code to actually handle queries!



IDEA -- local GUI for sage using https://github.com/rogerwang/node-webkit;  http://appjs.org/



I had this code somewhere in node_server.coffee:




Oct 20, 2012 at 7:20pm:
=========================

I ran https://www.ssllabs.com/ssltest/analyze.html?d=salv.us and
https://salv.us gets an A! However, there are some vulnerabilities,
coming from stunnel.

Plan for now: finish writing node_server and cleaning it up.

  [x] node_server -- fix streaming of output

  [ ] implement caching of results using Cassandra
     (nearly done; next need to put in the actual queries for a cache, then test the code I just wrote for eval)

  [ ] use cassandra to choose sage servers
  [ ] deal with failure to connect to sage much more robustly
  [ ] code cleanup/refactor
       [ ] sage.coffee -- proper headers and comments



Oct 21 in the morning for several hours:
[ ] MAJOR BUG/issue -- I've realized that sage_server.py has issues
with many simultaneous connections, race conditions or something.

Oct 23, 2012 at 6am
---------------------
I mostly fixed the race condition mentioned above, but there are issues:

   [ ] an important feature -- automatic cleanup of subproces -- is gone and I have to figure out how to add it back.
   [ ] even now it isn't perfect.

As I was laying in bed last night, I started feeling that a big
blocker for completely finishing salvus (to some milestone) at this
point is not having a detailed todo list for the whole project.


Hmmm, I'm pissed about this sage_server.py hang thing.  It does
reliably hang.  I *must* fix this and have a nice way to test that
this continues to work.  Having the key component randomly hang is not
cool.

Facts:

  - several procs hitting the server will get it to hang reliably.
  - log:
     INFO:child66110: received message "{u'event': u'terminate_session'}"
     _recv(4)
     blocking recv (i = 0), pid=66112
     got it = ''
     _recv(29)
     blocking recv (i = 0), pid=66112
     got it = '{"event":"terminate_session"}'
     INFO:child66112: received message "{u'event': u'terminate_session'}"
  - when hung, control-c is ignored suddenly.
  - when hung, there are a bunch of zombie subprocs:
25473 s000  S+     0:00.05 bash /Users/wstein/sage/build/sage-5.2/local/bin/sage-python ./sage_server.py -p 10000
25474 s000  S+     0:00.01 bash /Users/wstein/sage/build/sage-5.2/spkg/bin/sage -python ./sage_server.py -p 10000
25477 s000  S+     2:08.71 python ./sage_server.py -p 10000
66109 s000  Z+     0:00.00 (python2.7)
66110 s000  Z+     0:00.00 (python2.7)
66111 s000  Z+     0:00.00 (python2.7)
66112 s000  Z+     0:00.00 (python2.7)
66113 s000  Z+     0:00.00 (python2.7)
66244 s010  S+     0:00.00 grep python
blastoff:salvus wstein$

  - one single client maybe not enough to cause a crash..

  - I switched to "s.listen(0)"
    I got it to hang again with a script that makes 10 node_clients at once.
    This took about 5 minutes.  There are *no* zombie pythons left this time.

  - I bet s.listen(0) defaults to s.listen(5) since the docs say the
    number of connections must be >=1.

        Trying s.listen(1) now.

    I see -- when that happens there is only one client accepting at a
    time, though of course we can have many clients.  I wonder if when
    there is more than 1, two forking at the same time screws up some
    important state?

    With this setting I have not been able to get sage_server.py to
    actually crash.  Although, occassionally, I get...

           error: sage connection error: Error: write EPIPE

and trying again gives:

          "Error: connect ECONNRESET"

    on node_server, which is not caught, and kills the node server right now.
    I only get that when I have two distinct node servers trying to connect
    to sage_server.py at the same time.
    It is very easy to get this.

After a very long time, no sage_server.py crashes.

I put back in the thread that kills stuff and the Connection object, and
*did* get a hang after a while.

Getting rid of that (thread/Connection stuff), I think I got a hang
after a long time when I tried to do some long-running computation.
Argh!

I got rid of any logging in addition to whatever else, and now it seems
to be working.

Immediate goals:

  [ ] sage_server.py -- find a robust way to implement cleanup of subprocs and files
  [ ] make static c4c changes for trademark stuff
  [ ] deploy node-based version with changes
  [ ] polish code bigtime

Oct 24, 2012, morning work.
===========================

I just ran a test of sage_server.py.  It spawn 340028 sessions
(wrapping around the pid space a few times!), then stopped accepting
connections.

Python has a ForkingTCPServer mixin thing.  Maybe there is just something
stupid regarding resources that I'm missing.  I'll try their class
and see what happens.
   http://docs.python.org/library/socketserver.html

That was a total fail -- it's worse in every possible way.  Much worse.

The problem could just be an OS X bug or extreme resource limit.  I
should test on a linux vm to see what happens.  I will soon.

Now I'm going to do what is needed to deploy this, which is to fix the
automatic timeout code.

  [x] sage_server.py -- find a robust way to implement cleanup of subprocs and files
  [x] make static c4c changes for trademark stuff
  [x] admin service for NodeServer class.
  [x] use CQL to get the sage servers

  [x] deploy node-based version with changes to cluster
      - cassandra nodetool repair
      - create a new base image with newest code, using 08salvus which isn't running any images

Surprisingly, after apt-get upgrade'ing it the network stopped working!  So I'm skipping that step for now.

Need libbz2-dev to build bz2 module in Python to build node:

  sudo apt-get install libbz2-dev; ./build.py --build_python; ./build.py --build_nodejs; ./build.py --build_nodejs_packages

No coffeescript.

DONE:
But now the network just stopped working again.  ... because I'm an idiot -- I firewalled myself by doing deploy_local!!!!

      - try running deploy_local in that to test linux compat

import admin; reload(admin); s = admin.Services('conf/deploy2/', username='salvus'); h = s._hosts


      - restart whole cluster with that base image:

# edit services file so vm's use new base
import admin; reload(admin); s = admin.Services('conf/deploy2/', username='salvus'); h = s._hosts

      - if total fail, revert to previous image, make image with just static changes, and go.
      - if success, test more
      - email C4C


ISSUES:

  [x] mobile -- Oh crap, I totally broke that.

-----------

  [x] cassandra firewall -- had to disable it because it wasn't working; need to fix that.

root@cassandra01:/home/salvus/salvus/salvus# ufw default allow incoming
Default incoming policy changed to 'allow'
(be sure to update your rules accordingly)
root@cassandra01:/home/salvus/salvus/salvus# ufw default allow outgoing

   [?] thought -- wait -- there is a fundamental argument against using a
       firewall at all, which is that when a sage node gets rooted,
       then the attacker can just disable the firewall and attack
       everything else.  If instead we put the sage nodes on a
       separate tincd network, then even if rooted, they only have
       network access to the nodejs servers on that subnet.  "only"?
       That would be bad.  So we would be forced to firewall the
       nodejs servers from the sage servers.

       [ ] eventually I will have to figure out how to configure tincd
           servers so that they will not route traffic from certain
           clients beyond the vpn...


--------
Oct 25, 2012
------------
   [x] check up on test running on 10dot19dot1dot1 (on 08salvus):
        it ran for 12 hours with no problems with 21 clients, 3 servers, and not the slightest hicup.  Nice.
        turning off that vm

   [x] come up with a viable overall plan for finishing the first beta
       release (which I can charge for) by Dec 7.  This is 6 weeks.

   [x] websocket with sockjs for mobile safari broken again.  I think
       this happened before with tornado, and I patched the sockjs
       tornado code.  I probably need to include the same patch in
       sockjs for node!
     -- works fine on ipad and mobile identities of safari on desktop;
        doesn't seem to work on my old iphone, but i'll worry about that later.

   [x] ensure proper generation of coffeescript --> js when nginx is started (that symlink failed).

   [x] Remove tornado completely from salvus

   [x] rename: nodejs_server --> something meaningful, since i'll have many node.js based servers.
       ideas for name:
          * middle
          * websocket_server
          * center
          * vertex, nucleus, heart, core, hub
          * i like "hub" the best

   [x] enable twitter bootstrap in my index.html page and make sure it looks ok on mobile...

   [x] then remove jquery mobile support and mobile/ directory

   [x] deploy with the rename node --> nodejs; support for mobile browsers again; fix issues that come up



SOON:
   [ ] Dynamic Firewall: clarify/cleanup/refactor code:

       [ ] - improve the sage firewall function in admin.py in the same way that I fixed up the cassandra firewall.
           What we want:
               - incoming: port 6000 from nodejs servers; port 22 from control
               - outgoing: port 655 to tincd servers that this machine connects to, which will already be established

       [ ] - make "dynamic firewall" updateable, i.e., control can update
             firewalls on all nodes that need them without having to restart
             any services.

       [ ] - *every* service should have some explicit firewalls
             associated with it, and corr host vm should let in the minimum
             required for those services.  Then when a sage vm gets rooted,
             it won't be able to attack any other vm's, since they will not
             have whitelisted it.

-----------

Oct 25 at 5:52pm:
   [ ] support persistent sessions

Plan:

./hub --port 5000 --tcp_port 5001 --address 127.0.0.1 --database_nodes 127.0.0.1
watch hub.coffee ./hub restart --port 5000 --tcp_port 5001 --address 127.0.0.1 --database_nodes 127.0.0.1  --log_file=data/logs/hub

    [x] caching single session in db, just because I should, and it is a trivial app of cassandra


Oct 26 at 5pm:
--------------

    [ ] hub.coffee -- make it so clients can create persistent sessions:

DATABASE MODEL -- new cassandra tables with ttl, so if node
doesn't actively check in to confirm that session or user is alive,
they vanish.

hub_connections:
   uuid, type, address, port   # with a ttl

where:

    uuid = a unique id
    type = 'sage', 'user', 'sagenb'
         = etc... later could include a sagenb connection, an ipython notebook connection, etc.
    address = address of a hub
    port = port of a hub

 * Node process stores uuid's of users that it broadcasts incoming
   messages from a session (or other users) to.  This works because a
   session exists only as long as the node process it is connected to
   exists.  This also avoids not-needed queries to the DB, which are
   expensive.

When we have projects, the project will also store the last
session_uuid it used as metadata.  If that session is live, when
opening the project, we can connect back into it.

 [x] python wrapper class for session object
 [x] CS wrapper class for session object

---

Not sure that was a good idea.  More generic would be having
UUID:Value stores, and add ttl support to both them and KeyValueStore.
That would be useful in many places, and solve this problem nicely.
 [x] uuid:value store



---------

[x] new message type that *only* needs to be supported in javascript:
        {event:'new_session',
         session_uuid:'....',
         max_walltime:...}
    This message goes *from* Hub server to browser client.

[x] Enhance the start_session message type from the browser client to the hub by:
         - confirming/shrinking max params,
         - allocating new session connection to a sage server and saving in module-scope object
         - saving to the connected_sessions cassandra table

Oct 27 at 7:57am
----------------

TODO:

[x] Install windows 8 enterprise for client testing. ICK.

[x] support persistent sessions by extending the execute_code message
    to take an optional session_uuid.  If specified, then execution
    takes place in that session.  If not specified, then an ephemeral
    session is made for that one single code execution.

    NOTE: When execution is taking place in a session, the
    session_uuid has to be added back into messages coming back from
    that sage connection.

[x]
    Make a simple node client to test that the above works right, and
    is fast.  It should be extremely fast due to not having to fork
    and exchange a bunch of messages.

    [x] make a coffeescript client that can use persistent session over websocket
    [x] client ability to send SIGINT, SIGKILL, etc.
    [ ] refactor code in sockjs_test_client.coffee:
          [x]- move pure client code to a new coffee-script file, so can
                be shared between testing and client code

[x] Setup a module system and optimizer to organize my client webapp
    and easily allow for code sharing between client and server.

step back -- I'm having trouble writing cleanly organized code
for both the browser and node at the same time without getting my head
around modules in the browser.  Namely: http://requirejs.org/

 r.js -o name=require-jquery out=a.js baseUrl=.

I'm not loving this requireJS at all.  It's really annoying.  Another thing seems good:

   https://github.com/medikoo/modules-webmake

    [x] STRUCTURE my code plan:

I want a file called salvus.js which is output by webmake.  It will
provide a namespace containing all the relevant functionality needed
to implement the view part of the application.

    [x] structured CoffeeScript that represents a connection to the hub and works in both browser and node

    [x] Add uuid support to misc.

-----------------

I want the following, with same code in both node and in the browser.

conn = client.Connect('https://localhost')  # in browser
conn = client_node.Connect('http://localhost:5000')  # in node

conn.execute_code(code, preparse, (mesg) -> )

session = conn.new_session({walltime:60, cputime:60, numfiles:200, vmem:2000})

[ ] Support events: http://pragprog.com/magazines/2011-08/decouple-your-apps-with-eventdriven-coffeescript

  x- running into trouble with events do to lack of support in webmake.
  x- trying browserify, which supports events:
         https://github.com/substack/node-browserify

[x] while I'm here, might as well add minifcation/uglification as an option.

  npm install uglify-js2


[x] back to events


x session.on("close", (mesg) -> )
x session.on("output", (mesg) -> )
x session.execute_code(uuid, code, preparse)
x session.send_signal(signal)
x session.terminate()
-----------------

    [x] upgrade nodejs from 0.8.12 to 0.8.14

    [x] enable execute_code for connection (not just sessions)

    [x] once get the above, rewrite index.coffee to use it.

    [ ] add *view* support for persistent sessions

    [ ] TODO: limits should include a disk quota
        -- see sage_server.py todo that mentions quota

    [ ] TODO: change salvus_message.coffee to use "defaults" function in misc.coffee

    [ ] persistent session termination:
            - client should get a message when persistent session terminates, e.g., from being killed, timed out, etc.
            - remove session uuid stuff from in-memory table

    [ ] enforce strong limits on stateless_sage_exec, since it has no way to send interrupts (?)

    [ ] make it so client can find out how much wall and cpu time remains in a given session (store in db?)
    [ ] nice looking spinner during computations
    [ ] html: change current html so that session is persistent
    [ ] html: move input box below output and make output scroll up, exactly like a command line
    [ ] html: make output accumulate and include input

    [ ] rename salvus_message: id --> exec_uuid


video about cassandra: http://www.myvidster.com/video/8292247/Apache_Cassandra_noSQL_Yes_to_Scale


[ ] Apache Benchmark -- I should make something that uses this be a standard part of admin/monitor:

   ab -n 40000 -c 1 localhost:5000/

Doesn't work at all on my laptop, but works fine on a linux box.

[ ] I just made: http://salvusmath.blogspot.com/

          [ ] - at some point add support for non-websocket sockjs client, for better testing  ??
https://github.com/sockjs/sockjs-client-node

---

Oct 28, 2012:
=============
at 11:20am:
  [x] change styling of "about" link a little?; demos: single cell --> script, command line, worksheet
  [ ] implement user login:

       [x] check demos to make sure can do it easily
I just tried https://github.com/bnoguchi/everyauth a bunch and couldn't get anything to work.  Boo!
Next, trying: http://passportjs.org/
Yes, passport it definitely is!  It's by far the best.

The example here actually works well: /Users/wstein/salvus/notes/node/github-login

-----

Login:

     [ ] implement database model for accounts:
            [x] define user accounts schema in cassandra.py
            [x] define user plans schema in cassandra.py
            [x] wrapper objects in cassandra.py for:
                 [x] Account
                 [x] Accounts
                 [x] Plans
                 [x] Auths
                 [ ] debug/test that it really works

            [ ] cassandra.coffee wrapper objects for:
                 [ ] Account
                 [ ] Accounts
                 [ ] Plans
                 [ ] Auths

            [ ] datastructure in hub that says which -- if any -- user a
                sockjs connection is authenticated as.

     [ ] transition hub to be an express application
     [ ] integrate in login support as in app2.coffee


  [ ] store state/history of command line demo in database (up to a cutoff)
  [ ] store script(s) in database
  [ ] browse past scripts

  [ ] add some anti-load/DOS stuff to hub.coffee, e.g.,
           * bound on number of persistent sessions on a given server
           * bound on simultaneous number of exec's


  [ ] I should implement a database-free way to dump/backup each important cassandra db table...
  [ ] up arrow history for command line
  [ ] desparately need heartbeat so auto-reconnect is rock solid.  This is *horrible* as is.
  [ ] android: scroll into view for command line isn't working...?

  [ ] FAIL:   https://mail.google.com/mail/u/0/?shva=1#inbox/13aadfd160c80679
          print "effectuées "


[ ] alter table plans add description varchar;

Oct 29, 2012:
-------------

New plan for today: 6pm - 2am (8 hours)

 [ ] 2 hours (started at 5:52pm): accounts, etc., database code

   [x] simplify accounts, etc. schema as much as I can -- let it grow as needed!
   [ ] finish CS node database client code
       [ ] create test framework ASAP:
              - drops all tables in salvus_test keyspace
              - creates tables
              - does tests with them

       [ ] callbacks should *always* pass error as first argument, since that is
           where client code will use it.
       [ ] plans
       [ ] accounts
   [ ] move code back that *needs* to be in cassandra.py; delete anything else?
   [ ] integrated ability to dump table to disk.
       [ ] ability to get timestamp of record (can use for backups)

  [ ] 2 hours: authentication and login system:
       [ ] github
       [ ] google
       [ ] local

  [ ] 2 hours: persist everything currently implemented

  [ ] 2 hours worksheet -- try to implement something based on https://github.com/xing/wysihtml5

TOTAL FAIL.  Replan:
I have 9pm - 2am or 5 hours.

  9pm - 11pm
  -----------
     [x] setup a testing framework and write tests for cassandra.coffee

  @11pm: replan

Ok, I successfully setup a very nice testing framework.
however, it requires having code to easily create the whole db schema in node.
It is weird having the schema defined in code in cassandra.py.
So I want to make a schema that is in a file, then make it
so both cassandra.py and cassandra.coffee can make all tables
and indexes in the schema that aren't already defined.
This will be more maintainable, easier to read, etc.


11:17 - 12:33am
   [x] create schema from file.

12:33am - 1:30am
   [x] write more tests -- it's going well; i'm finding bugs, so i'm going to keep at it.  This is important!
       I am finding that everything not tested is buggy... so this is good.

    There is a lot more to test with cassandra.coffee, but at least
    the key value stuff does exercise everything else well, and now
    works.    Testing other things is important as well.   I'll
    use my last few minutes for some client testing.

    Even the most basic test of client isn't possible now because hub.coffee is maybe broken.
    Fix all that next chance I get.  We have direction!






 1:00am -?
   PLAN










Soon:


 [ ] Fix that hardcoding of 6000 in cassandra.coffee -- should
     actually make the address in the db: "hostname:port", since can't
     have index on non-primary key tables.  Then split it after query,
     and return {host:..., port:...}.

--------------------------------

Oct 30 at 2:08pm - until 2am

   [x] evaluate google compute engine (using free trial):
           Conclusion: ignore until salvus is making money.  Even a token presence is well over $2K/year.


starting at 2pm:  Google Compute Engine

   ** IMPORTANT: This is under my uw.edu email, not my gmail.com email **

Published rates: https://cloud.google.com/pricing/compute-engine

>>> .145*24*8*30 # GCE per month for 1 core
835.200000000000
>>> 1.16/.145
8.00000000000000
>>> pph = 1.16*2*4  # price per hour from google for same as what I have in tour
>>> pph
9.28000000000000
>>> pph*24*365  # price per year from google
81292.8000000000
>>> 80*12
960
>>> 64*8*.10  # price for disk per month
51.2000000000000
>>> 2*.145*24*365 # price per year from gce for equiv of servedby
2540.40000000000
>>> 80*12
960

In short, what I get for $5000/year costs $90K/year from Google.

---

[x] Google Compute Engine:

Goal:
   [x] provision a machine -- done through webpage
it looks like they have a couple of specific VM's, but one can't upload your own image.
   [x] provision a disk -- done through webpage
   [ ] be able to ssh in
   [ ] build/install salvus
   [ ] build/install sage
   [ ] setup tinc (?)

SSH'ing:

blastoff:~ wstein$ gcutil --project=uw.edu:salvus ssh test
Go to the following link in your browser:

    https://accounts.google.com/o/oauth2/auth?scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute.readonly+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdevstorage.full_control+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdevstorage.read_only+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdevstorage.read_write+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdevstorage.write_only+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&response_type=code&client_id=1025389682001.apps.googleusercontent.com&access_type=offline

Enter verification code: 4/bYXPuOh6N9xpIMhR8Uojkk7vJPQH.Mkq67l9oocsQuJJVnL49Cc-WrTlndQI
Authentication successful.
WARNING: You don't have an ssh key for Google Compute Engine. Creating one now...
Enter passphrase (empty for no passphrase):
Enter same passphrase again:
INFO: Updated project with new ssh key. It can take several minutes for the instance to pick up the key.
INFO: Waiting 300 seconds before attempting to connect.
----

It logged me in, but as user wstein, which is useless.  I couldn't install anything, such as g++.

blastoff:~ wstein$ gcutil --project=uw.edu:salvus --ssh_user=root ssh test
INFO: Updated project with new ssh key. It can take several minutes for the instance to pick up the key.
INFO: Waiting 300 seconds before attempting to connect.


[[GCE is frustrating]]

blastoff:salvus wstein$  gcutil listinstances --project=uw.edu:salvus
+------+---------------+-----------------------------------------------+---------+---------------+---------------+-------+---------------+------+---------+----------------+
| name | machine-type  |                     image                     | network |  network-ip   |  external-ip  | disks |     zone      | tags | status  | status-message |
+------+---------------+-----------------------------------------------+---------+---------------+---------------+-------+---------------+------+---------+----------------+
| test | n1-standard-8 | projects/google/images/ubuntu-12-04-v20120912 | default | 10.240.194.87 | 108.59.95.234 | test  | us-central1-a |      | RUNNING |                |
+------+---------------+-----------------------------------------------+---------+---------------+---------------+-------+---------------+------+---------+----------------+

A micro-benchmark:

   python -c "import time; t=time.time(); a=sum(xrange(10**8)); print(time.time()-t)"

GCE n1-standard-8: 0.623615026474
my 3ghz salvus boxes: 0.934787034988
my laptop 1.8Ghz: 1.12881588936
my laptop 2.6Ghz: 1.09943389893
bsd.math: 1.5067961216  # older version of python!
a vm running on 3ghz salvus box: 0.94832611084
combinat.math: 1.39991497993
servedby1: 1.52679586411
servedby2: 1.40613603592

sage.math (using python 2.7.3 in sage): 1.42585396767

----

I'm going to try again following https://developers.google.com/compute/docs/hello_world

https://developers.google.com/compute/docs/hello_world

# set default

gcutil getproject --project=uw.edu:salvus --cache_flag_values
gcutil addfirewall http2 --description="Incoming http allowed." --allowed="tcp:http"

gcutil addinstance --machine_type=n1-standard-4 --zone=us-central1-a --disk test test
gcutil addinstance --machine_type=n1-standard-4 --zone=us-central1-a  and


gcutil ssh test
   ls -lh /dev/disk/by-uuid

Comparing virtual disk performance:

GCE: about 150MB/sec
root@test:/home/wstein# /sbin/hdparm -t /dev/vda1
/dev/vda1:
 Timing buffered disk reads: 446 MB in  3.01 seconds = 148.37 MB/sec

My VM: about 535MB/sec (due to caching!)
root@cassandra01:/home/salvus# /sbin/hdparm -t /dev/vda1
/dev/vda1:
 Timing buffered disk reads: 242 MB in  0.45 seconds = 535.10 MB/sec

My hardware: 142MB/sec
root@08salvus:/home/salvus# /sbin/hdparm -t /dev/sda1
/dev/sda1:
 Timing buffered disk reads: 242 MB in  1.71 seconds = 141.75 MB/sec

Another test:

GCE disks are better:
root@test:/home/wstein# time dd bs=1M count=512 if=/dev/zero of=test conv=fdatasync; rm test
512+0 records in
512+0 records out
536870912 bytes (537 MB) copied, 3.82017 s, 141 MB/s
real	0m3.822s

On my VM:

root@cassandra01:/home/salvus# time dd bs=1M count=512 if=/dev/zero of=test conv=fdatasync; rm test
512+0 records in
512+0 records out
536870912 bytes (537 MB) copied, 6.06796 s, 88.5 MB/s

On my hardware:
salvus@08salvus:~$ time dd bs=1M count=512 if=/dev/zero of=test conv=fdatasync; rm test
512+0 records in
512+0 records out
536870912 bytes (537 MB) copied, 4.30397 s, 125 MB/s

---
Network:

I did an iperf test from GCE to 08salvus:

root@test:/home/wstein# iperf -c 128.95.242.139

varies a *lot*, but got timings: 43.2 Mbits/sec,  280 Mbits/sec, 116 Mbits/sec,  219 Mbits/sec, 248 Mbits/sec

For comparison, from 01salvus to 08salvus (across campus):   475 Mbits/sec consistently
From 07salvus to 08salvus:
                     (using vpn) 253 Mbits/sec
                     (not using vpn)  944 Mbits/sec, 944 Mbits/sec (hey it is gigabit eth!)

From 02salvus to 01salvus:
                     (using vpn)  259 Mbits/sec
                     (not using vpn) 943Mbits/sec

From vm on 01salvus: (not using vpn) 41.4 Mbits/sec, 87.3 Mbits/sec, 51.8 Mbits/sec,
                     (using vpn)  209 Mbits/sec,  216 Mbits/sec, 206 Mbits/sec


salvus@01salvus:~$ iperf -c 128.95.242.139
------------------------------------------------------------
Client connecting to 128.95.242.139, TCP port 5001
TCP window size: 23.5 KByte (default)
------------------------------------------------------------
[  3] local 128.95.224.230 port 57092 connected with 128.95.242.139 port 5001
[ ID] Interval       Transfer     Bandwidth
[  3]  0.0-10.0 sec   567 MBytes   475 Mbits/sec


-----

Building Sage:

blastoff:~ wstein$ gcutil addinstance --machine_type=n1-standard-8 --zone=us-central1-a --disk test test


$ sudo su
$ fdisk /dev/disk/by-id/google-test
$ mkfs.ext4 /dev/disk/by-id/google-test
$ mount /dev/disk/by-id/google-test /mnt
$ mkdir /mnt/sage/; chown wstein sage. /mnt/sage
$ apt-get update; apt-get install g++ gfortran m4 make dpkg-dev libssl-dev screen
$ # change to wstein user
$ wget http://sage.math.washington.edu/home/release/sage-5.4.rc2/sage-5.4.rc2.tar
$ screen
$ export MAKE="make -j8"; make ptestlong


I wrote a blogpost about trying GCE: http://salvusmath.blogspot.com/2012/10/trying-out-google-compute-engine.html

---

  sudo apt-get install libncurses5-dev libssl-dev libreadline-dev git; git clone https://github.com/williamstein/salvus.git; cd salvus/salvus; . salvus-env ; ./build.py --build_all

---

  The only thing left is to wait for sage and salvus to build... for what it is worth.   I'll check up on that in a few hours.

5:40pm: time to regroup
========================

Things to do:

   [x] cassandra.coffee -- should the cb not be part of opts?  Look at code and decide.
        Decision -- no, leave it as is!  Reason is that it makes it
        clear which function is the callback when reading the code.

   [x] update salvus_messages.coffee to use opts pattern, and all node
       client code.
-->         - fixing client code now

   [x] implement client heartbeat:
          - client receives a message:
              - "i will send a heartbeat every k seconds"
          - in response client sets up a setInterval (every k*c seconds) that
            checks that a heartbeat message was received.  if not,
            close connection and reconnect.
       [x] create new message event: "heartbeat"
            -- even better, create "ping" message and corresponding "pong" back.
            -- integrate into client
            -- provide function to get last ping time and display in gui (at least for testing)

TODO   [ ] there is a subtle bug/issue with the design right now, which
   will vanish in time when things are done right.  The issue is that a sage session (see the code of
   create_persistent_sage_session) is tied via a closure to the specific sockjs conection that
   made it.  However, if the user connects later or reconnects, then that connection no longer exists.
   NOTE: right now in index.coffee in onopen we just make a new command line session in this case.

   [x] change client*coffee to use opts

.... took until 12:22am, but I made the connection far more robust
against network failures.  There is still a problem where it will
stupidly sometimes fallback to a non-websocket protocol. But overall
this is much, much better.


   [x] write client*coffee test suite

   [x] more cassandra*coffee test suite


---

It seemed like I got way behind and didn't get much done, but now that
I think about it, I guess I did a lot tonight that I planned to do.  I
solidly addressed that annoying issue with reconnecting, which will be
a major plus in the long run.  I improved testing a lot and refactored
and cleaned up quite a bit of code.  I wrote a blog post.    I have
improved quite a bit at coffeescript/javascript/node.

Next up -- what I've been planning to do for days!?!

   [ ] account creation
   [ ] storage of data
I wish I did not have to sleep.

Oct 31, 2012:
==============

   [ ] account creation

         - message.create_account(username, email_address):
              [ ] creates entry in accounts table in database setting
                  account to be "not_validated", unless there are
                  already too many accounts with a given email_address

BAH -- an attacker can use the above as a way to send lots of email out!  not good.

              [ ] a given ip address
              [ ] sends out an email to the email_address with a link in it
              [ ] when user visits that link, database changes account to "validated"

The whole reason to link an email address to an account is so that users can reset
their password, and so messages can be sent.  It may be better to:

      - user chooses an auth provider from a list, including github, google, etc.

---

I just went through how Dropbox does things.  They have the fastest
and most straightforward easy-to-use not-linked account system
*imaginable*!  I mean, seriously, it is clean and easy.  I bet they
have some anti-DOS measures behind the scenes that are subtle, of
course.

I will implement something similar to what they do, since it is really
massively user friendly, and they are super good at growing their user
base.  It's what I secretly want but am scared of.

Dropbox:
  * they do not validate the email address of users.
  * they require typing password to change linked email address
  * they require knowing email address linked to an account to do a password reset
  * at most one account per email for saneness of their database backend -- but one can just do + trick to get more...

--

My plan: implement account stuff exactly like them, but with the
additional option of linking accounts to other providers, specifically
for the purpose of sharing git repos, or any other *specific* reason
why one would want to link like that.

And, do setup 2-factor using google authenticator, like dropbox does.

OK, at least I have a very solid plan!


----

OK, here we go. The steps:

   [x] change accounts schema:
          account_id, first_name, last_name, email_address, password_hash

   [x] function on client.coffee:
          .call(message, timeout, cb):
       that: (1) slaps a uuid on the message
             (2) sends the message
             (3) waits for up to timeout in seconds for a message back tagged with that uuid,
                  - if get message, call cb with it:
                           cb(null, message)
                  - if don't get message, call cb to report an error
                           cb(true, "timeout after 5 seconds")
             (4) in either case in 3, the callback is then removed.

        WHY?  Because I want 100% of the client/server communication
        to go via messages, not via AJAX, HTTP, etc., so that other
        client apps -- e.g., pure TCP -- will be 100% doable.  Also,
        this makes testing via client_node.coffee much easier.

   [x] define all of the messages we need to implement everything in
       salvus_messages.coffee to implement the entire login protocol.

             # client --> hub
             message.create_account(id, first_name, last_name, email_address, password, agreed_to_terms)
             # client <--> hub
             message.email_address_availability(id, email_address, available)
             # client --> hub
             message.sign_in(id, email_address, password, remember_me)
             # hub --> client
             # sent in response to either create_account or log_in
             message.signed_in(id, account_id, first_name, last_name, email_address, plan_name)
             # client --> hub
             message.sign_out(id)
             # hub --> client
             message.signed_out(id)

             # client --> hub
             message.change_email_address(id, old_email_address, new_email_address, password)
             # hub --> client
             message.changed_email_address(id, old_email_address, new_email_address)

             # client --> hub
             message.change_password(id, email_address, old_password, new_password)
             # hub --> client
             message.changed_password(id, error, message)
              # if error is true, that means the password was not changed; would
                happen if password is wrong (message:'invalid password'),
                or request is too frequent (message:'too many password change requests')

             # client --> hub
             message.password_reset(id, email_address)
             # hub --> client
             message.password_reset_response(id, email_address, success)
                success true if message sent; success false if no such email_address in the database

   [x] anti-DOS/spam measures:

          [x] hard limit account creation from a given ip address to a maximum
              of 200 accounts per week.

              - this will require using a key:value store with ttl:
                    name = 'account_creation_attempts'

               key:ip_address
               value:number_of_attempts
               ttl:1 day

              - due to proxy and a class of students all creating accounts at once,
                we only have an upper bound up to some amount of time.

              - if number of accounts from ip exceeds 10 (say), require a captcha (?)

          [x] bruteforce attack: limit change_password requests to 25 per day.

                name = 'change_password_attempts'

                key:email_address
                value:number_of_attempts
                ttl:1 day

3:58pm
--------------------

-- I finished and tested backend account creation, except the above DOS restrictions.
-- Also, backend login doesn't actually set *any* backend state -- but that will be easy.

Next:

   [ ] implement password reset functionality:
        done - define a new message: client <--> hub  "password_reset(email_address)"
        done - write function in client that sends that message
         - write code in hub: password_reset(email_address) that
          done  1. looks up user with that email_address
          done  2. if doesn't exist, respond with error
         done   3. if exists but there has been a password_reset request
               from the same ip address in the last 2 minutes, deny.
         done   3. if exists, generates a uuid and puts an entry in a
               uuid:email_address table called "password_reset" with a ttl of
               15 minutes.
         done   3.5 while at it, put entry in recent_password_reset_requests key:value table with ttl of 2 minutes:
                   ip_address_of_password_reset_request:email_address
        done 4. put just-in-case entry in another key:value table also called "password_reset_requests" with ttl of 6 months
                   email_address:ip_address_of_password_reset_request
        done 5. send email with link:
                  https://salv.us/hub/password_reset?key=uuid
        done  6. store email password more safely
         - define password_reset url handler, which displays a form:
                   [password]
                   [retype password]
           and gives no information about the account, in particular, not the email address.
           the form will include resubmitting the uuid.
         - form submits again to
                  https://salv.us/hub/password_reset
           but this time using POST instead of GET, so that the
           password doesn't appear in the URL, so we can distinguish them,
           and because GET shouldn't change state, but POST does.
         - confirmation dialog
         - then redirects to https://salv.us
   [x] implement the anti-DOS measures described above

   [x] implement change password:
       done  - define a message
       done  - client
       done  - hub

   [x] implement change email address:
       done  - define a message
       done  - client
       done  - hub

   [x] rate limit both password changes and sign_in attempts: at most 1 every 5 seconds, say

   [ ] implement some frontend client gui to use all this...
         [ ] login
         [ ] account creation
         [ ] password reset link upon failure to login

   [ ] create a ClientConnection object in hub.coffee

   [ ] change property of ClientConnection on successful login

   [ ] integrated "report an issue" feature linked to an account.
       report goes to database.
       contains any internal state information that the hub knows!!!

....

I'm out of steam....

There is one error detected by the test suite in changing passwords.
Should be easy to fix when I can think again.


----

Nov 1, 2012
===========

Up at about 7am, and going to work very, very hard on Salvus all day today.

[x] finished execute_code cleanup.

[-->] I'm going to update the deployed salvus right now, just because it
    is good practice to be good at this, and most components are out
    of date in the current install...

[x] new messages needed for login

Nov 2, 2012:
===========

  I had only a few minutes (literally) to work on salvus -- I worked
  on fixing a password checking issue in my code that was messing up
  testing, and uncovered another issue with loging.  Then went to
  teach/work on the ramp/hang out with Tish, and catch up on sleep.


Nov 3, 2012:
===========

  [ ] test suite for login system

 [x] get rid of this HEX:{"account_id":{"hex":"3acb0196-b9eb-48b7-884f-335f563f244d"},"client_ip_address":"127.0.0.1"}


-->[ ] feedback function

     [x] database schema entry

feedback_id uuid
time timestamp

account_id uuid      // user who reported the feedback (if known)
type    varchar      // 'bug', 'idea', 'comment'
description varchar  // description of feedback by user
data    varchar      // JSON object -- anything system wants to record about feedback, e.g., fields with state info (!)
nps     float        // "how likely are you to recommend salvus to one of your friends or colleagues?" -- good time to ask

status  varchar      // 'new' (user just reported it), 'issue' (opened ticket in feedback tracker), 'closed' (resolved), 'invalid'
notes   varchar      // notes by salvus developers about this issue
url     varchar      // pointer into feedback tracking system, if feedback is sent to a standard feedback tracker (e.g., a github issue)

PRIMARY KEY(feedback_id, time)   // time so we can sort on it

CREATE INDEX ON feedback(account_id);   // so we can show (via GUI) all feedback issues associated with a given user to a user

---

     [x] function in cassandra

report_feedback = (account_id, type, description, data, nps)
feedback_from_user = (account_id)

     [x] function in client

report_feedback = (type, description, nps)
feedback_from_me = ()

     [x] new messages

1)
event:'report_feedback'
type: required
description: required
nps:undefined

2)
event:'feedback_reported'
     [x] function in hub
report_feedback = (account_id, type, description, nps, data)


---------


Plan now:

   [ ] write more test code

   [ ] implement html/gui code to use new client.coffee functions

IMPORTANT TODO:

   [ ] when executing code in script mode, we get no indication at all that a computation has timed out!  Horrible UI!

Nov 5, 2012 @ 7:44pm:
=====================


  [ ] create account gui
  [ ] link to terms of service
  [ ] template terms of service page
  [ ] login gui
  [ ] report an issue gui

I started writing this code (as html/CS) and am feeling unhappy with
how I'm obviously going to have to badly reinvent the wheel.  I'm now
reading:
    http://addyosmani.com/blog/building-spas-jquerys-best-friends/

Backbone is popular, but http://spinejs.com/ looks very intersting,
because it is lightweight, well documented, and very
CoffeeScript-oriented.

...

That said, I'm just not sure that these MVC javascript libraries are
what I need for Salvus, which isn't a "browsing a bunch of images" or
whatever sort of app.  It's very much centered around highly dynamic
interactive websocket content, chat, etc.  It's real-time.
But the MVC idea is important.

---

I've written code today to get a feeling for how things are working,
and I'm just going to forge ahead and write some nice CoffeeScript +
jQuery + HTML.

The code I have is kind of messy now, but it works and I feel I could
do anything, and I can refactor it to make it much cleaner.  So I'm
going to work on refactoring and cleaning it!

Another weird "surprise" is that somehow I completely broke salvus in
IE.  The layout, etc., has gone to hell.  I have no idea why or how...

---


I will define a class called Page.

When you create a Page it:

  (1) registers itself centrally somehow
  (2) has a

.. it's just not clear what I need until I write some more code..


Writing a jQuery plugin for salvus-pages might work well to register
things and make it easy to display them with options?  Maybe not.

  http://coffeescriptcookbook.com/chapters/jquery/plugin


--------

Goal for Nov 7, 2012
=====================


"Remember me" research -- Evidently this is the best guide:
       http://fishbowl.pastiche.org/2004/01/19/persistent_login_cookie_best_practice/
      but *ignore* the Improved practices link!

From http://news.ycombinator.com/item?id=4754190: To provide a
counterpoint, the section about the "Remember Me"-cookie is rather
terrible (I stopped reading after that).  It's not fundamentally
flawed but rather inelegant (and potentially expensive) to store a
magic number server-side for each session. You can implement the same
thing more easily by handing out tamper-proof (HMAC) cookies
containing the start- and end-time, and storing only the
last_logout-timestamp for each user on the server-side.  Any cookie
where expire_at < now or created_at < last_logout is rejected at
validation time.




Nov 7 at 7:19pm:
----------------

* learned about https://www.easel.io/, which is a web-based gui for developing bootstrap sites.

 * Definite guide to authentication here --
     http://stackoverflow.com/questions/549/the-definitive-guide-to-forms-based-website-authentication
   Let's see how my design does:

Now it is 8:25pm and I'm caught up on a lot HAcker News, etc.  Time to program!

  [x] Add back in Google Analytics code for salvus.  Make a new file
      "page/analytics.html" and put it there.

Admin --> Tracking Info

It's javascript so put that somewhere (?) -- tracking.coffee


  [x] Make it so "Sign in" navbar item changes to the User's Name.


  [x] Add a new page to account.html that is displayed by default if the
      user *is* logged in -- namely a big Sign out at the top, then
      tabbed groups of user settings (nothing there yet).


Nov 8, at 6:09am:
=================
I just got up.

I'm going to warm up by:

  [x] separate the settings page into two tabs, with
       account type and code eval wells in one tab
       and other stuff in other

  [x] Add a whole new page (with link from nav menu) to give feedback.

  [x] make it so feedback send button is live



---

Nov 8, 2012
-----------

Now it is 4pm and I'm working as hard as I can until midnight.

  * Mathematica is ahead.  Fuck.  http://www.wolfram.com/mathematica-online/

  * I have *GOT* to make major progress on salvus ASAP.  This is getting scary and frustrating.  The time is now.  Now.

  [x] implement the "Sign out" link

Nov 9:
  6:30am - 7:52am:
  [x] make the contents in the account settings page actually valid

Look into this ASAP -- more major competition!

   [x]  http://continuum.io/blog/introducing-wakari
        http://news.ycombinator.com/item?id=4763354


Nov 9 at 5:37pm.

Plan:

  [x] change evaluate key:

      options --

          shift-enter
          control-enter
          enter (shift-enter for newline)

  [x] change default compute system

       eventually: make list of options a query to the server where
       list depends on the user (paying customers can potentially get
       more; options can change dynamically)   ??
       for now:
          Sage
          Python
          Octave
	  R
          Maxima

  [x] Write function for setting and getting value of HTML element so account settings fully works
  [x] make evaluate key setting actually do something
  [x] make default system be respected (and meaningful) in demos


  [x] caching for single cell needs to distinguish between whether or not code was preparsed.

  [x] change email address - gui

Sunday, Nov 11 at 7:18am:
=========================

I did not work at all on Salvus yesterday, due to working on the ramp all day and
wanting to just goof off in the evening for once.

Now I will work for about 3 hours, and will work for a few hour after
the vert ramp.

For now:

  [x] I stubbed this implement this function used in hub!! verify_account_password**

Next up, this -- report (and make sure) of *any* errors on email change before closing modal:

  [x] change email:
        - give error on fail
        - check that new address available
  [x] correct focus for login, change email, etc.


-----|

Nov 12
=================
at 6:31am:
Verterans Day!

--> [ ] new policy: Like dropbox, I'm going to make salvus only warn people if they choose a stupid password, but still allow it.
        But then salvus will nag them until they change it to be strong.

  [x] (5) change password gui

  [x](5) password reset gui
[x] -- basic problem: how to do password reset with a consistent gui/ui look, etc.
     Obviously, I could just add another url handler, but that would be ugly.
     It would be nice if I could do:

          https://salv.us#forgot?forgot_key=UUID

     and have (1) the forgot_key get set as a javascript variable (or in DOM)
              (2) a modal dialog be displayed
Is this possible? YES.  easy.

  [x] (3) refix test suite, broken by change password dialog


  [x] ** rewrite sign_in function in hub.coffee to have an anti-DOS policy! **

  [x] Clicking on SalvusTM logo should give about page; other randomly polish

  [x] make it easier to close alerts -- now just click on them anywhere

  [x] disable connect-to box

  [x] (3) sign-up: enable automatic zxcvbn.zxcvbn
  [x] (3) change password -- enable automatic zxcvbn.zxcvbn

  [x] disallow the very weakest possible passwords.

Nov 13
------

I'm just working fulltime today on salvus.

  [x](7) remember me cookie:
somehow,
    - store in cookie a uuid
    - store in database a secure_hash(uuid):value table with ttl and value=account_id
    - if client initially connects and has a cookie uuid such that
      secure_hash(uuid) is in table, then automatically sign them in.

The solution is to send (via sockjs) an id, then use that to send the
cookies *back* to some URL... AFTER we have made the sockjs
connection.  This will work and keep cookies httponly. (http://www.codinghorror.com/blog/2008/08/protecting-your-cookies-httponly.html)
  [x] implement sign out -- make no sense without connection session id

  [x] some sessions are very flakie, because some pexpect interfaces are.
      Only provide the very robust interfaces and nothing else.

- OK, I've now got all login functionality working.  Of course, it is
fake, since hub.coffee doesn't even check users are authenticated
before processing messages.


Next up:

   [x] refactor the conn object in hub.coffee

   [x] when user disconnects should delete the clientconnect object to avoid a leak

   [x] make it so hub verifies user is authenticated before doing stuff for them. (done, except for demo exec stuff, which is temporary)

   [x] clean up the clientconn code after refactor

   [x] make "remember me" sign-in's get recorded in the database log

   [x] for ease of use (de-normalization), add an extra "misc" column to successful_sign_ins that
       includes JSON:  {first_name:*, last_name:*, email_address:*, remember_me:bool}
   [x] ensure that users have to be authenticated before performing
       actions that require auth.



----


OK, it is now time to start working on the actual Sage-related compute functionality of Salvus.

   [ ] right now persistent sessions don't work because client tries to start *it* before sign-in even happens
       Fix: big / nontrivial / do things right.


---> Let's implement the following:

   - An "html5 sage session editor"
   - A sortable list of saved sessions that user can bring back up easily, which are stored in database.

This is just like the sage notebook, and will be useful.  Full
projects with directories, etc., can be done later.

worksheet schema:

CREATE TABLE worksheets (
     worksheet_id     uuid  PRIMARY KEY,
     title            varchar,
     last_edited      timestamp,
     users            varchar,       // JSON object giving user access for this worksheet:
                                     //    [{first_name:?, last_name:?, account_id:?, read_only:bool}, ...]
     content          varchar,       // JSON object representing the actual contents of the worksheet
);

// We also have the following table, since the users property of worksheets can't be queried:

CREATE TABLE worksheet_access (
     account_id       uuid,
     worksheet_id     uuid,
     read_only        boolean,
     PRIMARY KEY(account_id, worksheet_id)
);

---


update worksheets set title='WS 0',users='user info',content='0+0' where worksheet_id=0;
update worksheets set title='WS 1',users='user info',content='1+1' where worksheet_id=1;
update worksheets set title='WS 2',users='user info',content='2+2' where worksheet_id=2;

update worksheet_access set read_only='false' where account_id=0 and worksheet_id=0;
update worksheet_access set read_only='false' where account_id=1 and worksheet_id=0;
update worksheet_access set read_only='false' where account_id=2 and worksheet_id=0;
update worksheet_access set read_only='false' where account_id=0 and worksheet_id=1;
update worksheet_access set read_only='false' where account_id=0 and worksheet_id=2;

To list all worksheets for a given user:

    select * from worksheet_access where account_id=1;

That gives the worksheet_id's;  to convert these to titles in *ONE TRANSACTION*, do this sort of thing:

   cqlsh:test> select title from worksheets where worksheet_id in (0,3);

This does not work because "in" only works on the first part of the primary key:

   select * from worksheet_access where worksheet_id in (0,1,3);

But that's ok, since that info is in the users part of the worksheet, and we can do

    select title,users from worksheets where worksheet_id in (0,3);

Storing snapshots of worksheet:

CREATE TABLE worksheet_snapshots (
     worksheet_id     uuid,
     time             timestamp,
     content          varchar,
     PRIMARY KEY(worksheet_id, time)
);

Getting rid of old snapshots would just involve querying then doing
delete with IN and a list of timestamps.



Questions about sage sessions:

 What are the running sage sessions in a project?


I want to have a full terminal emulator command line session ability.
If I had that -- like c9.io already has -- then salv.us would instantly
become very, very useful to people.

After searching forever, I've found this: https://github.com/chjj/tty.js/

This is very cool!   Initial issue building due to me having "export MAKE"...
It uses socket.io.

This tty.js is very interesting!

I need to totally understand/improve it, since it will significantly
impact my approach/design.    I need to start by having projects and
a very good command line session.

I figured out how to get the real demo to work:

   git clone git://github.com/chjj/tty.js.git
   cd tty.js
   npm install
   bin/tty.js -p 8099


NOTE: There is a potential license issue with tty.js:  https://github.com/chjj/tty.js/issues/47

Looking at /Users/wstein/salvus0/tty.js/example, with some mods and it seems like I can implement
something pretty awesome as follows:

   - run a nodejs process (1) on "sage session server" computer.
   - it connects somehow (tcp?) to hub (2)
   - hub connects to user client (3) via sockjs
   - define a message type that is "data from session with given id",
     and whenever client receives that message, it calls an
     appropriate function on all
   - whenever there is output on (1), it goes to (2), which then pushes
     the output to all tuned-in clients
   - the clients (3) receive the message and call term.write with the
     data as input.

With this, we could have 100% full-on awesome safe remote command line
terminal sessions, with vi/emacs/sage/whatever 100% working, and
moreover, if multiple people (or the same person more than once)
connects to the same session, they will see exactly the same thing on
all screens!   Wow.  This will totally work and be awesome.

Once that works, of course I'll want to provide support for graphics
output.  Then editing .sws files, which will be a different story
altogether.

Now, let's work out how the worksheets will work :-)

The bootstrap version of wysihtml5 seems nice, but isn't actively updated.
However, the generic version is very regularly updated:
   https://github.com/xing/wysihtml5
WAIT, yes it is updated regularly: https://github.com/jhollingworth/bootstrap-wysihtml5

This wysihtml5 has tons of events -- it's like codemirror, but for a full html document.


Nov 14, 2012:
==============

This is the last night to work before getting back to working on the vert ramp...

Anyway, I tested the command line shell with aly and she was not so
excited, especially because it felt pretty laggy for her -- that is of
course an issue with command line shells -- roundtrip of keystrokes,
etc. Latency kills!  I forgot about that in my excitement.

I'm going to focus first on getting a core notion of projects and gui
for managing them (at a very simple level) in place.  A project is:

    * uuid
    * title (that one can change)
    * type
    * metadata JSON object
    * blob

Instead of saying "project = git repo", I will make it more open
ended.  At some point "git repo" will be one of the possibilities for
the blob.

Goals for tonight:

  [x] define message format for projects
  [x] define database schema for projects

  [x] fix all bugs in test suite introduced by improving sign-in security, refactoring code, etc.:

      [x] fixed bug: creating a new session then immediately sending it code
          to eval gets immediately returned from cache with no eval
          happening.   The reason is that the whole session-creation code
          has no callbacks, like it must....  since I was just learning.
          So that needed to get rewritten.

Implement these messages in client/hub/cassandra:

      [x] create_project
      [x] get_projects
      [x] set_project_title

      [x] write some test code for test suite, and fix up / improve test suite

  [ ] Implement HTML for viewing list of projects in various ways, creating projects, changing title.


      [x] html "create new project" modal dialog
      [x] CS code for new project dialog

      [x] coffeescript code to load list of projects: on login


I'm back -- Nov 16, 2012 at 6:11pm.   Will work until 1am (or so) if I can!
--------------------------------------------

      [x] implement -- all/public/private filter buttons
      [x] fix "enter to submit" for all modals...
      [x] bug -- user can "change password" to be very weak (even though can't create account that way).
      [x] pager -- just not going to do it.
      [x] coffeescript code to load list of projects: on receiving a push message


      [x] change feedback to be a modal, with a link from bottom/account/etc.

      [x] adding a support_level option.

cqlsh:test> alter table plans add support_level varchar;
cqlsh:test> select * from plans;
 plan_id                              | current | description | max_session_time | name | price | ram_limit | session_limit | stateless_exec_limits | storage_limit | support_level
--------------------------------------+---------+-------------+------------------+------+-------+-----------+---------------+-----------------------+---------------+---------------
 13814000-1dd2-11b2-0000-fe8ebeead9df |    True |        null |               30 | Free |  null |      2000 |             3 |                  null |           250 |          null

cqlsh:test> UPDATE plans SET current='true', name='Free', session_limit=3, storage_limit=250, max_session_time=30, ram_limit=2000, support_level='None' where plan_id=13814000-1dd2-11b2-0000-fe8ebeead9df;
cqlsh:test>


      [x] files html page
        [x] html empty page with tab
        [x] html basic info about project at top of page with gui to edit

------------------------

      [x] make it so clicking on a project sets the "files" page up and shows it.


      [ ] cassandra.coffee -- accounts_with_access_to_projects
      [ ] files.html/coffee -- change title/description
      [ ] files.coffee -- handle when project is changed, so all relevant clients see the change


      [ ] in project list, change the All/public/private buttons to something so we can
          select from "public/private/owner/collaborator/viewer" and select multiple at once.

      [ ] add "files" attribute to database.  This will be a JSON
          varchar that is the filename+meta-info tree of the project.

      [ ] file browser -- can be written using new db attribute "files" above.

      [ ] html: create new file


----------------------------

      [ ] files html page, which has many editors for various file types: .sws, .sage, .py, etc.
          will have tabs for each open file.

      [ ] cassandra iso date --> how recent (in javascript, computed in browser)

      [ ] implement clients storing where they connect to database on connection
          See client_connections in hub.coffee

      [ ] implement pushing messages to connected clients on other hubs


      [x] get rid of project "type" -- makes no sense;

      [ ] way I fork to startup sage (?) seems to totally break gap mode ??



      SEE https://github.com/williamstein/delete/admin for examples of how github does this:
      [ ] Delete project - with big red warning
      [ ] client/hub/cassandra: delete project
      [ ] Make private
      [ ] Make public
      [ ] Transfer ownership


November 18, 2012 at 5:35pm -- probably have until 9:30pm, not sure.
---------------------------------------------------------------------


    [x] easy way to close a project page

    [x] rename "files.html", etc., to "project"; get rid of "edit.html",

    [x] actually implement -- make it so there can be multiple project pages open at once


--
IDEA for embedded console:

   https://github.com/replit/jq-console

This looks very nice.  It was easy to modify the echo.html
example... but does vi/emacs work through this?  This license is clear
MIT -- much better -- and the entire code is *small* coffeescript!
The min is 24K.     On the other hand, it "isn't a VT100".
Also, tty.js's license just got sorted out!

Nov 19, 2012:

I plan to work for 10 hours today on salvus, if all goes well.  We'll see.

  TODAY'S GOAL: very simple worksheet mode; ability to open worksheets; create new files,  etc.

How to get there:

  [x] come up with the ui design
  [x] make static html of it

  [x] make eval in scratch work

  [ ] create a simple Salvus worksheet:

     [x] find an html5 editor that:
           - works on my nexus4 and ipad
           - opera/safari/firefox/chrome

So far: wysihtml5 *DOES* when I run the demo locally (but not their demo online)
        bootstrap-wysihtml5 does *NOT* work.

        aloha seems neat/powerful/annoying, but is GPL. :-(

WOAH, http://jejacks0n.github.com/mercury/ this just knocked my socks off!!!!

  [x] I have 100% decided to definitely go with Mercury.  It has the right license, is beautiful code, looks good, etc.  It's frickin' sweet.

  [x] use Mercury to make an editable area in the scratch pad.

--

 [ ] I didn't restrict permissions in salvus directory in version I pushed out in VM!
     To fix:

       (1) start new version of VM of deployed version on 08salvus
       (2) fix salvus5 there --> salvus5b
       (3) push out new vm image
       (4) modify deploy2 in 01salvus (new "salvus5b" base)
       (5) restart sage servers

Update deploy2
Reload:
 import admin, deploy2; reload(admin); reload(deploy2); s=deploy2.services; h = s._hosts
- stop sage servers
 s.stop('sage')
- restart vm's
 [s.restart('vm',hostname="sage0%s"%i)  for i in range(2,10)]; [s.restart('vm',hostname="sage%s"%i)  for i in range(10,17)]
- ssh into a vm and confirm fix
 done
- drop table
  In [7]: !cqlsh -3 10.1.1.2 -k salvus
  truncate stateless_exec
- start sage

ok, that was pretty hard, but it worked.  I ended up using one of the web vm's as master control, since they
can connect to cassandra...






     virsh list  --all # check that no base vm is running
     export PREV=salvus5
     export NAME=salvus5b
     qemu-img create -b ~/vm/images/base/$PREV.img -f qcow2 ~/vm/images/base/$NAME.img

      virt-install --cpu host --network user,model=virtio --name $NAME --vcpus=16 --ram 32768 --import --disk ~/vm/images/base/$NAME.img,device=disk,bus=virtio,format=qcow2,cache=writeback --noautoconsole  --graphics vnc,port=8121
     virsh -c qemu:///session qemu-monitor-command --hmp $NAME 'hostfwd_add ::2222-:22'; ssh localhost -p 2222

       # IMPORTANT!
       sudo chown og-rwx -R salvus

       sudo apt-get update; sudo apt-get upgrade;
       sudo reboot -h now
       cd salvus/salvus; git pull https://github.com/williamstein/salvus.git
       . salvus-env


# for example:
       ./build.py --build_stunnel --build_nodejs --build_nodejs_packages --build_haproxy --build_nginx --build_cassandra --build_python_packages

     virsh --connect qemu:///session undefine $NAME
     virsh --connect qemu:///session destroy $NAME
     virsh --connect qemu:///session list --all

     cd ~/salvus/salvus; . salvus-env;  push_vm_images_base.py
-----

Goal for today:

   [ ] make the scratch area legit... but how?


 [ ] mobile friendly version of top bar...

 [x]  move the modification of mercury.min.js to scratch.coffee for better maintainability



First, let's describe some ways we would like it to work, even if I have no clue how to implement this:


 * Make a text mode that is just like "bold", but is "sage".  Turn mode on by default.

 * When in that mode and press execute shortcut (or hit big button),
   then all code in that region is executed.
   Result is

 * input to sage will thus be arbitrary html -- we'll somehow parse code out of it by --> plain text'ing it.
   but this will be nice, because people can bold/color/math/whatever their *input*.
 * Where is the cursor?

----

 [x] fix bugs in test suite caused by refactoring.

 [x] get mercury to work on my small laptop.

Hmm... I might be making a big mistake with using mercury.  I'm going to try wysihtml5 again, more seriously.
No, mercury is pretty amazing, because it really does give DOM access, functional config, etc.
Wysihtml5 does not seem to provide that.


----

  [ ] Mercury -- Figure out where the cursor is or at least what the current region is.  HOW?...

OK, here's my plan:

[x] Come up with a way to build Mercury without using rails at all
    from coffeescript sources, or die trying.

    I got it to work outside tree, at least for all the coffeescript
    files, though I didn't worry about css or menus yet!  It wasn't
    tooooo hard.

[x] Move the source code of Mercury with build system into salvus repo and minimize.
[x] Modify/fork it extensively to build the Salvus editor/notebook.
This must be better than starting from scratch!
[x] get rid of "assets" subdirectory.

--------
[ ] understand mercury enough to shift-enter = evaluate.

--------

 [ ] I need a plan since Mercury currently only works on non-IE.  !
     Plan:
        * a Salvus worksheet is an HTML document that happens to have sage code in it, and
          for which salvus enables powerful interactive usage.
        * Mercury will be the prefered editor
        * Other lesser editors will be supported as a fallback.

I will implement the following as several different scratch worksheets in salvus,
then

  Level 0: basic editor functionality:

      - input code in sage cells
      - evaluate code
      - enter text between cells in a different font.

Write something that ensures this works very well on everything, then move on to level 1.

  Level 1:
      - move cells around
      -

----------

WAIT one fucking minute!   Contenteditable in built into web browsers these days, and
might be exactly what I really want at the core of all this bullshit.

OK, this mercury shit has got to 100% go.  Everything I need is already nicely available
in contenteditable, which is built-in to all modern web browser.   I would have spent
a month deleting everything except one life of mercury!

 [x] remove mercury from salvus completely


[ ] how to use contenteditable:
     https://developer.mozilla.org/en-US/docs/Rich-Text_Editing_in_Mozilla

[ ] finding cursor?
http://stackoverflow.com/questions/2213376/how-to-find-cursor-position-in-a-contenteditable-div


[x] rangy -- add to salvus

[x] implement basic idea for salvus worksheet *NOW* using rangy.  (but didn't end up using rangy at all...)

Nov 23, 2012 at 5:28am
=======================

I got up early and am going to sneak some time into salvus now.
I have a basic worksheet idea that I like now! Plan:

  [x] worksheet: fix multi-line output
  [x] style stderr output
  [x] fix multi-line input  -- easy: just change style; amazing.
  [x] change background of note area on mouse over
  [x] syntax highlight input
  [x] up and down arrow to move between input lines -- something very, very rudimentary
  [x] control-c/escape interrupt
  [ ] indicator that a cell is running.
  [ ] enable/disable keyboard shortcuts on switch to/from worksheet page
  [ ] fix on iOS -- keep getting logged out (?)
  [ ] insert cell (how?)
  [ ] execute/interrupt buttons.
  [ ] separate output div for stdout/stderr/etc.
  [ ] abstract worksheet1
  [ ] drag and drop to re-order
  [ ] auto-focus sage: prompt on switch to page.
  [ ] mobile tests - bug fixes so works on more than just chrome

soon.
  [ ] click on "sage:" prompt to change it to another system -- and it *stays* changed

Nov 24, 2012
=============

I'm switching to using Linux again for development.  I'm so sick of
being so behind/frustrated/broken with everything on OS X.  I'm also
very tired of being locked into Apple's bullshit.  Very tired of it.
After 5 years.  I going to use the VM for a while, but maybe switch to
native if necessary... we'll see.

First problem -- wrong java version causes cassandra crash:
Fix:

     sudo apt-get install openjdk-7-jre
     sudo apt-get remove openjdk-6-*

Next problem:
    * no ipython readline history

Suddenly the network stopped working in the vm, when I unplugged my laptop.
Gees.  No -- that was the firewall from starting salvus locally.

Other big problem -- I only have sage-5.0 installed in this ubuntu; this will
take a while to fix.


8am: I'm up and running. First task:

   [x] get salvus test suite to pass locally:

      [x] the websock sockjs linux node client isn't working:
             conn = require("sockjs-client-ws").create("http://localhost:5000/hub")
          BOOM.
          UFW firewall issue again, of course.  I uninstalled ufw.


New local script:

import admin; import cassandra; reload(admin); reload(cassandra); s = admin.Services('conf/deploy_local/', username='wstein', keyspace='test'); h=s._hosts


Bugs (some found with Dennis):
-----------------------------

  [x] don't require user to sign in to send feedback!
  [x] Modal feedback doesn't close
  [x] Password strength only where setting password
  [x] firefox linux -- text input box in worksheet is too thin.
  [x] Labels on password
  [x] Forgot your password -- link at front does nothing

Regarding the worksheet interface, I'm going to just do something very
much like the sage notebook, but with small tweaks.  I can do other
interfaces later.  This will mean I don't have to do too much design,
and won't alienate users too much.

---

New plan: get to a new release with "similar" functionality to the
sage notebook in time for ICERM.


ascii code of a character: .charCodeAt(0)
---

Nov 27 at 11:11am
------------------

Now I'm going to make it so the scratch worksheet is saved.  For
simplicity, I'll just store this as a column of the user account.
Then it gets loaded when the user logs in, and saved every so often.

alter table accounts add scratch_worksheet varchar;

I have a feeling there is something wrong with this... somewhere.
The problem would be if in implementing things I piggy back too much
on existing code and make something that doesn't make any sense.
Better would be to put in client.coffee two functions:

    save_scratch_worksheet(html:html, cb:cb)
    load_scratch_worksheet(cb:cb)

When user is logged in, we use the database.
When user is not logged in, we use local storage.
The rest of page code doesn't know the difference.

  [ ] implement javascript local version first, and test -- no schema needed

cookie
   -- limited to 4k (!)
   -- sent with each http request.
   -- what about html5 local storage
  [ ] implement database version


-------

Nov 29, 2012:
-------------

I'm getting very frustrated with how long it is taking for me to create a worksheet editor.
I should just define what a worksheet *is*, make a straightforward editor that does enough,
then focus on all the backend stuff around that, which is a lot of work that has to get
done and won't get undone.    I can then go back to finding some new clever way of making an editor...
but life isn't so easy, is it?




TODO:

TOP PRIORITY:

---------------------------------

XX  [ ] editing highlighted code as is just doesn't work -- too subtle and not cross-browser.
      change to have two input pre's, one with code and one highlighted. So... next
      top-priority todo item is: INTEGRATE CODEMIRROR 3 in for input compute cells! ASAP.
PLAN:
  x- make a new nearly empty page "worksheet-cm.html"
  x- make worksheet-cm.html the default page
  x - include codemirror3 in static/codemirror3 directory
  x - remove old static/codemirror directory
  x - make a single codemirror3 textarea that auto-expands

** Oh my god!! Codemirror3 kicks frickin' ASS!  **

 xx - type in code and when hit "shift-enter" all code back to the last output is evaluated.
    (fake it with a stub)
  x    - where is the cursor
  x     - insert a lineWidget right there with the output

MY NEW DESIGN GOAL: make this editor system I'm creating for salvus the editor I would want to use for everything instead of emacs.

x  - evaluate sage code instead of javascript.
x  - the text output is displayed using markText
x - change background style of output so it is clear which is which.

  -  "   allow_cache  : true" ?
  - save/load: use markClean() and isClean()
  - undo/redo buttons
  - keystroke to insert new break (i.e., empty output)
  - keystroke to insert a line widget that is a standard html5 editable div
  - bring over the button bar, etc., from worksheet1 to make this actually call to sage, save state, etc.
  - can use a gutter to set the evaluation mode
  - If i'm going with this, use ~/salvus0/codemirror-3.0rc1/bin/compress to make a single thing.
  - scroll on *top* and bottom: http://suwala.eu/blog/2012/02/01/horizontal-scrollbar-top-and-bottom-element/

  - evaluate result!

... I'm *NOT* loving this! -- it turns out that this approach just feels wrong during usage, though in theory it seems neat.
-----------------------------------

<<<<<<< HEAD
DONE:
  [x] worksheet1 keyboard shortcuts should only be active when
      that page is displayed, e.g., tabbing in sign in doesn't work.
  [x] save non-logged in user's scratch worksheet to cookie (?)
  [x] support using scratch worksheet *without* having to first login <--

  [x] make it so re-loaded scratch worksheet is *active*
  [x] save logged in user's scratch worksheet to database
  [x] add tooltips for the control buttons
  [x] Rainbow highlighter -- has a bug with triple quoted strings:
      - the regexp is wrong: correct version is /('{3}|"{3})[\s\S]*?\1/gm
        """[\s\S]*"""
  [x] bug -- extra space on highlight
  [x] ability to use cursor keys to move between compute cells

  [x] evaluating a large input cell takes a huge time to do nothing -- why?
      it turns out getting the plain text using the .text() method of rangy is *DOG SLOW*!
      What about that HTML parser I ditched?
      It is 10000 times faster than rangy for this!  W00t.  OK, I keep it.

  [x] kill/restart session
  [x] lame evaluate button / interrupt button (just at the top and bottom and nowhere else)
  [x] lame "running" indicator just to have something easy to implement
  [x] .html/text -- how to gets rid of newlines, html, etc... (but webkit doesn't) -- use https://github.com/tautologistics/node-htmlparser/


  [x] modify worksheet1.coffee so the cells are codemirror cells, but otherwise it is the same
  [x] remove worksheet-cm from visibility
  [x] get rid of including Rainbow.
  [x] get rid of including htmlparse.
  [x] saving and loading scratch worksheet again:
[x] Define the JSON format --
{
 title:
 description:
 cells:[ {id:<uuid text>, note:<html>, input:<text>, output:{stdout:<html>, stderr:<html>}} ]
}
[x] function that serializes the scratch worksheet from DOM
[x] functions that loads serialized scratch worksheet into DOM


  [x] insert a new cell between existing cells
  [x] add paren matching
  [x] delete cell
  [x] paren matching -- there is an easy codemirror plugin

  [x] finish implementing dirty and clean for worksheet state.


-----

  [ ] change worksheet format so that the output is:

            output:[list of messages...]
      where we can optimize it upon serialization by combining adjacent things of the same time.

      streams:
         stdout
         stderr
         stdhtml

  [ ] html output

  [ ] very basic implementation of a manipulate control using
  bootstrap, in order to make sure my approach can work.
      @interact
      def f(function=['sin', 'cos', 'tan']):
          print function
  and get a nice button bar.

  [ ] refactor worksheet1 code to a new object oriented structure, now that it is clear, and enough is done.

  [ ] message when session times out, then restart it.... will happen only if user's browser is there!

  [ ] tab completion
      [x] page: determining where the cursor is
-     [x] page: putting the cursor back
      [ ] sage_server: decide which type of introspection to do
      [ ] sage_server: introspect completions
      [ ] sage_server: introspect docstring
      [ ] sage_server: introspect source code
      [ ] page: display all introspects as alert_message for now...

  [ ] tab completion optimization idea:
       - cache completions for identifier... until next execute
=======
Dec 1, 2012
============

7:30am.
The final push begins.  I have exactly one week to do a significant release.
I have ALL weekend.

I'm going to first try one easy or impossible bug:

   [x] force the keyboard to appear when clicking on edit cell on iOS ipad3.... only after tab completion.
       - idea: blur, then focus.
 --> soln -- don't autofocus back -- force user to.

It seems like a good warm up. ... and yet it seems like a major pain.  Probably a better solution is to
write a better completions UI.

What to do next?!  So many things:
   [x] joining cells as in the sage notebook
       - working on desktop and ios

-->   [x] splitting cells as in the sage notebook
9:31am -- start (at victrola)
10:24am -- fiddled with button bar a lot; cleaned it up; set up wiring for cell split/join/move
tot=1:16   10:47am -- move cell up/down is now done... and pretty awesome!  if I add clear way to select multiple cells at once, this will be very useful indeed.
10:47am -- move cell up/down is now done... and pretty awesome!  if I add clear way to select multiple cells at once, this will be very useful indeed.
tot=1:20   10:50am -- off to REI or something with tish for a bit...
1:34pm -- BACK to work after shopping with tish, etc. -- NOPE... went skating in back yard (excercise!)
tot=1:20   2:37pm -- now really back to work.... on splitting cells
tot=1:45   3:02pm -- cell splitting is done

   [x] evaluate all -- might as well
   [x] delete all output -- easy peasy

tot=2:23 -- 3:40pm   [x] hide/show all output

tot=3:13 -- 4:29pm   [x] change output so it is an optimized stream

tot=3:37 -- 4:53pm   [x] change so that can output raw html (not put into a pre span).

tot=3:39 -- 6:53pm [ ] back at work, after making dinner, etc. with kido-s

   [x] change worksheet storage format, so output is an optimized
       stream of messages (store the type in the DOM as data, so
       serializing code doesn't have to know types?).

6:55pm - 7:30pm ---> Linux playing around!

tot=3:40 [ ] Finally, really back to work

   [x] add execute-javascript command, which will be a foundation for other things
- define a message
- implement command in sage_server
- implement handling in client

tot=5:40 -- 9:30pm -- finished javascript/coffeescript functions:
                       salvus.[...]


Dec 2, 2012
starting at 6:35am.
Not fucking around. 

   [ ] edit mode that shows the underlying JSON object of the
       worksheet in a nicely presented way, and allows
       editing/saving/reloading.  This would be good to do ASAP, since
       it will allow me to constantly have a clear view of how the
       underlying structure of worksheets is evolving.

 - add a page for editing
 - add buttons for switching between two modes
 - redo edit mode more nicely:
     - codemirror
     - nicely indented and structured/colored
     - verify that edited JSON is valid before saving; give useful error messages
tot=1:30: 8am

tot=2:15: 9:06am
     - wasted an hour on code folding; will wait on this
9:06am: now at stumptown with tishy!
        I'm going to do some straightforward but tedious things that have got to get done anyways.
tot=3:30 10:50-12:10:brunch with tish
tot=4:30 12-1pm: finished edit mode v.1
1-5:33pm: worked on the vert ramp, shower, dishes, tish, dinner, etc.
5:33pm - 9:00pm: time to bust ass before "Dexter, Homeland"
goal for now:
   [x] make it so disabled buttons don't do anything.
   [x] indicator when session restarts + countdown timer

   [x] jquery spinner -- might as well do it.

------

Dec 3, 2012:
============
   [x] tab completion
   [x] count-up timer about how long a computation has been going (part of spinner stuff... and I think countdown has this feature?)


   [ ] simple interacts:
GOAL:

@interact
def f(n=(1..10), m=[1,2,3]):
    print n, m



--------



---------

   [ ]  MOBILE ISSUE: codemirror has no vertical scrollbars...:
        solution -- add pager buttons.

   [ ] idea for a way to implement "%auto". 
       have a control output type, e.g., mesg.control =
       with various commands (given as objects), e.g,.
              mesg.control = {on:"load", command:"execute_code"}

       *or* just have a cell tag attribute, and have lots of tags that
       impact things.... and make that list of tags editable by the user.

   [ ] very, very simple mini interact
salvus.html("""
<br>
<input id="i1" value="2"></input>
<input id="i2" value="3"></input>
<button class="btn btn-large btn-success">Compute It!</button>
<hr>
""")
salvus.coffeescript("""
cell.find("button").click () -> 
    salvus_exec
        input: "#{cell.find('#i1').val()} + #{cell.find('#i2').val()}"
        cb: (mesg) -> append_cell_output_from_mesg(cell, mesg)
""")

   [ ] autosave

   [ ] make it so the tab key introspects when it should, and is a tab key otherwise.

   [ ] finish implementing genuine introspection

   [ ] plotting images

   [ ] rewrite worksheet/cells to be classes and encapsulated, so I can have multiple instances
       on the page

   [ ] * interrupt button could be green, but change to yellow when code is running.
      * restart could be green, but change to yellow when a session is active.

-----------------------

Dec 4, 2012:
============
6am - 1pm, except for breakfast.

How is an interact <input> going to work, say:

  - worry about "where" later.

Input:
  a variable, which means -- namespace & name
  a control type and properties -- "input"
   - generate a uuid
   - when variable is changed, send a message to browser to update the control
   - when control is changed, send message to sage to update variable to match (but obviously shouldn't fire message back to browser!)
   - when variable is changed in any way, fire any on-change events, which are calls to Python code.

# start with this

import sage_salvus
a = 10
ib = sage_salvus.input_box(variable='a', globals()) 

a = 20
ib.update()  # will cause control to change in browser

# now change to 30 in browser
print a
# get 30

def f(e):
    salvus.execute_coffeescript("alert 'hi'")

ib.on('change', f)





m = interact.namespace()
m['a'] = 5
ib = interact.input_box(variable='a', namespace=m)
m['a'] = 20




Dec 4 at 6:53pm:
----------------

Fix some issues I noticed with Simon in my office:

  xx - when tab completing, sage.schemes.ell[tab] fails.
  xx - when tab completing, width of list is too small, e.g., EllipticCurve[tab]
---

  [x] introspection -- finished showing source code and docstring...


[ ] make a list of some controls:

   - input_box(default=None, label=None, type=None)

   - slider
   - color selector
[ ] make jquery plugins them:
[ ] put it together
[ ] layout designer?  -- use jquery draggable and save style coords?
   [ ] multiple scratch worksheets...
   [ ] potential execution issue  -- if session is interrupted, then some exec's will never get a mesg.done ?

n
---------------

Dec 4 at 10:30pm.
----------------

Purely functionality-wise, not worrying about code quality or beauty or bugs, what is left?

  [ ] mathjax
  [ ] interact
  [ ] %modes
  [ ] 2d graphics
  [ ] 3d graphics
  [ ] data/ -- attachements
  [ ] print view

Dec 5:
--------

An example where control-c/SIGINT won't interrupt:

n=100; Q1092.<z>=CyclotomicField(1092); M=Matrix(Q1092,[[Q1092.zero_element() for i in range(n)] for j in range(n)])

This reveals many issues -- e.g. -- timer vanishing is annoying.

 [x] mathjax usage -- run it on <div class="tex"> and <span class="tex"> in html output.  That's it...
     or even better - have a message format. 

Dec 6
--------

 [x] json editor -- this looks good and is under very active development -- http://jsoneditoronline.org/; https://github.com/josdejong/jsoneditoronline
 [x] setup salvusmath twitter account, since I could easily live blog a lot about salvus.
 [x] let's have some fun first! :-) -- color selector:
Somewhat surprisingly, this seems like the best one:  http://modcoder.org/?ptab=jquery&item=excolor
The website isn't awesome, but it's MIT licensed, and it works solidly on iOS, android, and desktop unlike pretty much all the others!
A fork is here: git://github.com/rjfranco/excolor.git
Finally settled for bootstrap-colorpicker, and now this works:
salvus.html("""
<input id="mycolor3" name="mycolor" type="text" class="color-input" value="164,208,237" readonly="readonly"/>
""")
salvus.coffeescript("$('#mycolor3').colorpicker()", once=False)

 [x] worksheet file support (graphics)-- the plan

  - add support to Salvus object in sage_server to send a file object message:
        - create a uuid.uuid4()
        - make message with file attribute

 - [] sage.coffee -- send using new message format
 

  - add support to hub (and haproxy) so that this URL works:
           https://salv.us/files/filename?uuid=[the uuid]
    just stub it initially so that it always returns an image or a simple text file
  - in browser, add support for handling file output message, e.g.,
    display it depending on mime type or extension or something (?)
  - add support for a new message TYPE in various places:
        * blob instead of JSON
        * length (encoded as before)
        * will have to have some special character to indicate message type now.
  - make it so sage_server sends a blob message when sending a file object
  - make it so when hub receives blob message, it stores it in cache -- also worry about size
  - make it so hub also stores blobs in a uuid:value store in the
    database; these should expire after some short amount of time; if
    saved as part of a worksheet, they would get copied somewhere
    else.
  - use the above to implement display of static 2d plots...

 [x] i wonder how hard svg rendering for 2d plots would be...
Amusing demo using Rapael:
s = """
r = Raphael(cell.find(".salvus-cell-output").css('border','1px solid black').draggable()[0], 800, 100)
circle = r.circle(50, 40, 5)
"""
for x in range(0,200):
    s += 'r.circle(%s*10,10*%s+30, 3)\n'%(x,float(sin(x)))
salvus.coffeescript(s)

I just tried raphael for a while, hoping for its image reader support to work... it does NOT.


WE HAVE 2d plots:

def s(p):
    p.save('/tmp/a.svg')
    salvus.file('/tmp/a.svg')
    os.unlink('/tmp/a.svg')
s(plot(sin(x^2), (x,-10,10), color='red', figsize=[8,4]))
--
def s(p, ext='svg'):
    file = '/tmp/a.'+ext
    p.save(file)
    salvus.file(file)
    os.unlink(file)
for n in [1..5]:
    f = sin(x^n)/x^n + cos(1/x)
    salvus.tex(f)
    salvus.stdout(": ")
    s(plot(f, (x,1,3), figsize=[8,2], axes=False, frame=True), 'svg')
    salvus.stdout('\n')
--

And embedding images in html:

p = plot(sin,0,3).save('/tmp/a.svg')
uuid = salvus.file('/tmp/a.svg', show=False)
salvus.html('<h2>THIS: <img src="/blobs/tmp/a.svg?uuid=%s" width=300></h2>'%uuid)

-----

Top priority goal for today:

 [ ] terminate sage session when corresponding websocket connection dies
 [ ] save scratch worksheet to a project
 [ ] make project (as collection of worksheets for now) actually work
 [ ] disable some settings stuff not supported:
       - account upgrade
       - submit key
 [ ] add big "TEST WARNING" to top banner
 [ ] BUG:  salvus.tex (Ell[tab]
 [ ] search for bugs
 [ ] re-deploy


--
Then...

 [ ] make a uuid:blob store, which stores blobs instead of strings (varchar)
 [ ] switch to using uuid:blob store in hub, and ensure that sending images works (need to change sage.coffee too).
 [ ] finish documenting salvus object in sage_server.py
 [ ] document/refactor worksheet1.coffee (!)
 [ ] slide show mode
 [ ] mathjax in note field;  div class="math", "math-display" post processing.
       * interact
       * multiple scratch worksheets
 [ ] mathjax -- can I cache the rendered html/css?
 [ ] BUG: big gap in output when input is:
for n in [1..10]:
    salvus.stdout("\n%s: "%n)
    salvus.tex(taylor(1/(1-x)^n,x,2))
 [ ] mathjax in note:
       -- make it so typing "$" inserts a math mode span and puts cursor in it.
       -- make it so <div> and <span> math get turned back to $$'s 
       -- make to so span class math's get hit by jsmath
       -- make it so when mathjax modifies something it saves original html
       -- make it so worksheet save recovers original html

