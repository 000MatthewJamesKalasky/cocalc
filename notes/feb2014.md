# February 2014.

- [ ] general longterm planning
- [ ] (0:30?) fix new project dialog -- modal-body div looks all weird.
- [ ] upgrade to sage-6.1
- [ ] cgroups
- [ ] snap: delete snap cassandra user in database credentials
- [ ] fix this printing issue -- https://mail.google.com/mail/u/0/?shva=1#inbox/14367e63a3fa1052
- [ ] check -- is there anything in help or faq about /scratch (?) -- it is out of date.
- [ ] new encrypted offsite backup.
- [ ] add haskell editor syntax highlighting support.
- [ ] project move (dialog to provide info about targets).
- [ ] zfs replication: several (3) errors that aren't getting sorted out.  RESOLVE THEM.
- [ ] quota watch:
        94a3a0b8-0257-43ef-9986-89d7fc789a72 -- a bunch of texing, but has used 5GB in *snapshots*.
        6ff32b4a-1747-4760-9f83-aaf694e17006
        9ac5a8ca-7dbf-4b27-adea-4709b0fb7105
        e51a48b0-8b47-4d20-ac2d-e9aa7a4462ba
- [ ] update fontawesome: https://github.com/FortAwesome/Font-Awesome/wiki/Upgrading-from-3.2.1-to-4
- [ ] update coffeescript: http://ihackernews.com/comments/7139175
- [ ] update node.js
- [ ] update node.js proxy library, and see if it solves this -- res isn't even defined in
        /home/salvus/salvus/salvus/node_modules/http-proxy/lib/http-proxy/passes/ws-incoming.js
      so it is a bug in the proxy library.  DANG.   Check on upstream.  For today, just edit manually.
      I just *temporarily* commented out the whole emit line, since "emit is not defined".
- [ ] (5:00?) upgrade to bootstrap 3: http://stackoverflow.com/questions/17974998/updating-bootstrap-to-version-3-what-do-i-have-to-do
- [ ] (0:30?) increase open file timeout; right now it is shorter than the time it often takes to open a closed project
- [ ] storage: zfs quota reporting; fix, make much more visible, etc.
- [ ] storage: write code to scan and find all projects that are within 90% of running out of space.
- [ ] copy: enhance copy functionality to send/recv a file to/from all linked projects via a command line function for now
- [ ] add a "always confirm on page close" setting
- [ ] fix franco's security issue: https://mail.google.com/mail/u/0/?shva=1#search/franco/143ca2f6db753e56
- [ ] (1:00?) upgrade to jquery 2.1 -- http://blog.jquery.com/2014/01/24/jquery-1-11-and-2-1-released/
- [ ] (1:30?) truncate large output in worksheets
- [ ] (1:30?) SMC in SMC -- get it working again
- [ ] route 53 dns
- [ ] close_stale_projects is deciding what is closed/open based on location, which doesn't get reset on close, so it does WAY too many closes
- [ ] re-automate "nodetool cleanup"
- [ ] automate "zpool scrub" -- code in storage.coffee (?)
      - should do "zpool status" on each machine in parallel
      - parse output to seen when last scrub was successful or if there are errors
      - if output indicates any errors, notify me
      - if it was one week ago or more, do another scrub
- [ ] (1:00?) ensure rebooting and/or restarting the cassandra nodes works; maybe have to put some zpool import functionality in admin/startup script.
- [ ] (1:30?) upgrade sockjs, which has seen new development: https://github.com/sockjs/sockjs-client/commits/master
- [ ] (3:00?) bug: fuse is now broken for users. UGH. -- this is because of BIG uid's....
      Idea to fix this:
          - figure out what the uid cutoff is in both npm and fuse; it's probably the same and fairly universal.
          - figure out the uid's of all existing projects; how many exceed the cutoff?
          - fix the ones that exceed the cutoff, somehow.
          - for the ones that don't leave 'em.
          - store the account uid in the projects database; changing code of create_project_user to take uid as non-optional input.
          - when creating a new project, choose a random uid, check if it is in use, and if not grab it, then check again (to avoid race).
- [ ] (2:00?) storage: edge case replication loop -- in this case 10.1.12.4 gets deleted one time, then 10.1.14.4 overwrites it in stage 2, ad infinitum.
        project_id: 306cce9f-fa4a-481b-93ef-afd69a7fb5b4
        current location: 10.3.3.4
        usage: {"avail":"4.97G","used":"26.9M","usedsnap":"8.03M"}
        last_replication_error: {"error":{"src-10.1.14.4-dest-10.1.12.4":"problem -- destination has snapshots that the source doesn't have -- destroying the target (really
         safely renaming)"},"timestamp":"2014-01-20T15:23:13"}
        snapshots:
                10.1.5.4 (dc 0): 2014-01-13T19:32:08, 2014-01-13T19:32:41, 2014-01-13T19:31:43, 2014-01-13T19:30:34, ...
                10.1.1.4 (dc 0): 2014-01-13T19:32:08, 2014-01-13T19:32:41, 2014-01-13T19:31:43, 2014-01-13T19:30:34, ...
        (old)   10.1.12.4 (dc 1): undefined, undefined, undefined, undefined, ...
        (old)   10.1.14.4 (dc 1): 2014-01-13T19:32:41, 2014-01-13T19:31:43, 2014-01-13T19:30:34, 2014-01-13T19:25:19, ...
                10.3.4.4 (dc 2): 2014-01-13T19:32:08, 2014-01-13T19:32:41, 2014-01-13T19:31:43, 2014-01-13T19:30:34, ...
                10.3.3.4 (dc 2): 2014-01-13T19:32:08, 2014-01-13T19:32:41, 2014-01-13T19:31:43, 2014-01-13T19:30:34, ...

    In this case, fix by doing: s.destroy_project(project_id:'306cce9f-fa4a-481b-93ef-afd69a7fb5b4',host:'10.1.14.4',safe:true)

- [ ] (0:30?) hit return for submit on password reset dialog
- [ ] (0:30?) enter password twice in password reset dialog
- [ ] codemirror unindent bug
- [ ] rotate out very long .sagemathcloud.log's
- [ ] on first run: first /home/salvus/salvus/salvus/data/local/sbin/tincd --kill   then normal tinc
- [ ] run web nodes on GCE
- [ ] run database on GCE
- [ ] add to monitor: available space on each project zpool
- [ ] add to monitor: root fs disk usage on compute machines (due to storage temp files)
- [ ] see "# TODO: must fix this -- it could overwrite a user bash or ssh stuff.  BAD." in create_project_user.py
- [ ] port forwarding: "ssh -L cloud1.math.washington.edu:4567:10.1.2.4:4567 ce2d267d00df42deab4464509a5f3e74@10.1.2.4"



