# Next base image update:

# February 2014 -- these are my goals:

--> - [ ] (0:45?) storage: in safe move, don't re-issue the zfs rename command if one is already running...

- [ ] move to the new SSD:
      (keep checking on https://cloud.sagemath.com/projects/3702601d-9fbc-4e4e-b7ab-c10a79e34d3b/files/monitor2.term)

- [ ] (3:00?) properly renaming and deleting files -- https://github.com/sagemath/cloud/issues/4; https://github.com/sagemath/cloud/issues/36

- [ ] (0:20?) make vm.py use newest base if not given.

- [ ] (1:30?) replication: change to just leave the send/recv files and let the crontab deleted them all once per 12 hours (say); also, change filename to just be canonical so no need to recreate if need it again.   PROBLEM--have to know if send is done getting created.

- [ ] (1:00?) sage-6.1

- [ ] (0:30?) think more about database use of toInt, what should it do if value doesn't fit in int!

- [ ] (0:15?) check on backup vm; shutdown and start making a copy of the qcow2 to usb disk.

- [ ] (1:00?) cgroups: store project memory/cpu limit in projects table entry; also have defaults in database maybe storage topology table (based on capacity of vm)

- [ ] (1:00?) cgroups: pass in parameters to the script, which we get from the database

- [ ] (0:30?) hit return for submit on password reset dialog
- [ ] (0:30?) enter password twice in password reset dialog

- [ ] document -- just spend time documenting how to use SMC at <https://github.com/sagemath/cloud/wiki>
- [ ] (3:00?) bug: fuse (and nodejs) is now broken for some users. UGH. -- this is because of BIG uid's....
      Idea to fix this:
          - figure out what the uid cutoff is in both npm and fuse; it's probably the same and fairly universal.
          - figure out the uid's of all existing projects; how many exceed the cutoff?
          - fix the ones that exceed the cutoff, somehow.
          - for the ones that don't leave 'em.
          - store the account uid in the projects database; changing code of create_project_user to take uid as non-optional input.
          - when creating a new project, choose a random uid, check if it is in use, and if not grab it, then check again (to avoid race).
- [ ] (2:00?) proper worksheet restarting, with much better ui and actually working

- [ ] (1:00?) Add a link to the "error opening" dialog to open project settings with the "Restart" pane highlighted #94
- [ ] (1:30?) storage: zfs quota reporting; fix, make much more visible, etc. -- https://github.com/sagemath/cloud/issues/78
- [ ] (1:30?) command line "open" command not showing the right tab (buggy, since not done)
- [ ] (1:30?) SMC in SMC -- get it working again
- [ ] (1:00?) bug -- sometimes other users cursors don't animate away, e.g., if their connection is dropped at the wrong time.
- [ ] (1:00?) re-automate "nodetool cleanup"
- [ ] (1:30?) automate "zpool scrub" -- code in storage.coffee (?)
      - should do "zpool status" on each machine in parallel
      - parse output to seen when last scrub was successful or if there are errors
      - if output indicates any errors, notify me
      - if it was one week ago or more, do another scrub
- [ ] (1:00?) inverted hashtag selection -- https://github.com/sagemath/cloud/issues/88
- [ ] (2:00?) finish upgrade to sage-6.1
- [ ] (1:00?) route 53 dns
- [ ] (1:00?) latex escape mode for worksheet printing -- https://github.com/sagemath/cloud/issues/82
- [ ] (2:00?) zfs replication: several errors that aren't getting sorted out.  RESOLVE.
- [ ] quota watch:
        94a3a0b8-0257-43ef-9986-89d7fc789a72 -- a bunch of texing, but has used 5GB in *snapshots*.
        6ff32b4a-1747-4760-9f83-aaf694e17006
        9ac5a8ca-7dbf-4b27-adea-4709b0fb7105
        e51a48b0-8b47-4d20-ac2d-e9aa7a4462ba
- [ ] (2:00?) storage: write code to scan and find all projects that are within 90% of running out of space.
- [ ] (3:00?) copy: enhance copy functionality to send/recv a file to/from all linked projects via a command line function for now
- [ ] (1:00?) rotate out very long .sagemathcloud.log's
- [ ] (1:00?) make it so rebooting and/or restarting the cassandra zfs-based nodes works; maybe have to put some zpool import functionality in admin/startup script.

- [ ] (0:45?) storage: change to just copy to all destinations simultaneously, since the zfs send/recv takes way, way longer than transfer, and this approach will thus take much less time overall; it also deals with that potential loop issue.


# NEXT MONTH:

- [ ] (0:30?) monitor: check that cron is running.
- [ ] (1:30?) update node.js
- [ ] (1:30?) update node.js proxy library, and see if it solves this -- res isn't even defined in
        /home/salvus/salvus/salvus/node_modules/http-proxy/lib/http-proxy/passes/ws-incoming.js
      so it is a bug in the proxy library.  DANG.   Check on upstream.  For today, just edit manually.
      I just *temporarily* commented out the whole emit line, since "emit is not defined".
- [ ] (1:30?) update fontawesome: https://github.com/FortAwesome/Font-Awesome/wiki/Upgrading-from-3.2.1-to-4
- [ ] (1:30?) update coffeescript: http://ihackernews.com/comments/7139175
- [ ] (5:00?) upgrade to bootstrap 3: http://stackoverflow.com/questions/17974998/updating-bootstrap-to-version-3-what-do-i-have-to-do
- [ ] (1:30?) fix franco's security issue: https://mail.google.com/mail/u/0/?shva=1#search/franco/143ca2f6db753e56
- [ ] (1:00?) upgrade to jquery 2.1 -- http://blog.jquery.com/2014/01/24/jquery-1-11-and-2-1-released/
- [ ] (1:30?) upgrade sockjs, which has seen new development: https://github.com/sockjs/sockjs-client/commits/master
- [ ] improve sync: see, e.g., https://mail.google.com/mail/u/0/?shva=1#sent/1442c211a278415a
- [ ] (2:00?) backend project-level autosave -- separate from user autosave; could prevent stupid data loss.  configure per project (default to 2 min?)

- [ ] monitor: check that all hub, etc., are running...
- [ ] live markdown preview (and similar for many document types...)
- [ ] don't delete vm tinc public key, temporary images, on controlling python script exit... since it can die for various reasons --?  all the python scripts on cloud4 died, causing havoc.
- [ ] (1:30?) run at least one web node on GCE
- [ ] (2:00?) run database on GCE
- [ ] (1:00?) cloud monitor -- move to running in a subproc with a timeout -- hanging ain't cool.
- [ ] (2:00?) codemirror4? -- https://mail.google.com/mail/u/0/?shva=1#inbox/1442314ecfed212b
- [ ] make octave more usable: https://github.com/sagemath/cloud/issues/97
    - [ ] set this on Sage server startup:
        os.environ['GNUTERM']='svg'
    - [ ] improve octave mode to detect plotting and output an image...?
- [ ] (1:00?) implement new ZFS backup, which just dumps the whole project (never diffs) to a very compressed file, if it has changed.   Store in a big directory.  Inconsistent histories -- keep both versions.
- [ ] (3:00?) 2FA -- https://github.com/sagemath/cloud/issues/85
- [ ] keyboard shortcuts
- [ ] write a user's guide
- [ ] write a developer's guide
- [ ] (1:00?) see "# TODO: must fix this -- it could overwrite a user bash or ssh stuff.  BAD." in create_project_user.py
- [ ] (1:00?) monitor script: add check that scratch on all compute machines is actually *working*
- [ ] (1:30?) monitor: should alert me somehow when exceptions occur regularly in hubs
- [ ] idea for port forwarding into vm's: "ssh -L cloud1.math.washington.edu:4567:10.1.2.4:4567 ce2d267d00df42deab4464509a5f3e74@10.1.2.4"
- [ ] sigmajs?  http://sigmajs.org/
- [ ] way to delete an account and go through and delete all accounts with non-lower-case email addresses
- [ ] (1:00?) create a temporary (for the next 3 months) very-easy-to-recover from backup mechanism for all projects, that could have slight corruption:
      NO:
         - create a 3.9TB salvus volume on disk.math
         - rsync the 7 compute[x]-projects.qcow2 images to it periodically. (prob 5 hours per image).
         - if everything is destroyed, but those images aren't destroyed, then we can very easily get back up and running using them, with minimal loss of work.
      OR -- NOPE.
         - create a bup volume on cloud1
         - copy all 7 images into it.
         - depending on size, do similar to disk.
         - depending on size, do similar to usb disk in my office that I offline.
         - WAIT: it takes 10 hours to make the backup of a 400-ish GB image locally; since restore would be serial and no better, it would take at least 1 *week* to restore.  That's NOT acceptable.
      OR -- OK, actually implement that dump of all ZFS filesystems, after all. That really is the way to go.

         time sudo zfs send -R projects/4a5f0542-5873-4eed-a85c-a18c706e8bcd@2014-02-07T00:42:54 > a
         gzip -9 a.gz   # of course, do in one step; decompress is fast, and savings is worth it
         openssl aes-128-cbc -in a.gz -out a.gz.ssl -pass file:blah # encrypt
         [type password twice]
         time openssl aes-128-cbc -d -in a.gz.ssl -out c.gz -pass file:blah # decrypt
         # 1. create gzip-9'd stream on compute vm
         # 2. copy that stream back to backup control vm
         # 3. encrypt on backup machine (so only it knows key) --?
         # 4. copy somewhere else...



- [ ] backup/storage: write recover code...


- [ ] (1:00?) add to monitor: root fs disk usage on compute machines (due to storage temp files)

# DONE



- [x] (0:15?) r packages question

- [x] (0:30?) debug replication of project with id 9e92655b-aea8-4de6-8246-8c614b0273b7


- [x] (2:00?) storage: edge case replication loop -- in this case 10.1.12.4 gets deleted one time, then 10.1.14.4 overwrites it in stage 2, ad infinitum.
        project_id: 306cce9f-fa4a-481b-93ef-afd69a7fb5b4
        current location: 10.3.3.4
        usage: {"avail":"4.97G","used":"26.9M","usedsnap":"8.03M"}
        last_replication_error: {"error":{"src-10.1.14.4-dest-10.1.12.4":"problem -- destination has snapshots that the source doesn't have -- destroying the target (really
         safely renaming)"},"timestamp":"2014-01-20T15:23:13"}
        snapshots:
                10.1.5.4 (dc 0): 2014-01-13T19:32:08, 2014-01-13T19:32:41, 2014-01-13T19:31:43, 2014-01-13T19:30:34, ...
                10.1.1.4 (dc 0): 2014-01-13T19:32:08, 2014-01-13T19:32:41, 2014-01-13T19:31:43, 2014-01-13T19:30:34, ...
        (old)   10.1.12.4 (dc 1): undefined, undefined, undefined, undefined, ...
        (old)   10.1.14.4 (dc 1): 2014-01-13T19:32:41, 2014-01-13T19:31:43, 2014-01-13T19:30:34, 2014-01-13T19:25:19, ...
                10.3.4.4 (dc 2): 2014-01-13T19:32:08, 2014-01-13T19:32:41, 2014-01-13T19:31:43, 2014-01-13T19:30:34, ...
                10.3.3.4 (dc 2): 2014-01-13T19:32:08, 2014-01-13T19:32:41, 2014-01-13T19:31:43, 2014-01-13T19:30:34, ...

    In this case, fix by doing: s.destroy_project(project_id:'306cce9f-fa4a-481b-93ef-afd69a7fb5b4',host:'10.1.14.4',safe:true)

- [x] (0:45?) (0:10) fix bug in my codemirror unindent plugin

- [x] (1:30?) business model ideas
- [x] (0:20?) (0:09) (sunday eve after 6pm) verify encrypted copy of backup vm is copied to bsd, then restart backup vm again.
- [x] (0:30?) (0:05) monitor: cassandra phase 1 check is often wrong - make more robust
- [x] (1:00?) (0:41) don't bother to backup projects that have never been open, like these 'f9372541-a2d9-44e4-91ce-ce8163bd176e', 'fdd137ba-5b7a-4ea3-be03-3f2bc8a9ad79', 'ffaf662a-0662-4d60-9da1-ac890d6991d8'.  We could do this by setting the status to 'new', which would change to opened once they are opened once.   Then ignore anything set to 'new' when doing backup.
- [x] (1:30?) (3:20) login email address is case sensitive -- make it not case sensitive #95
         - x canonicalize on account creation
         - x make login checking code use canonicalized email address.
         - x change all my code like this ".email_address.toLowerCase()" to call a function in misc and first check that the type is string.
         - x fully test on my laptop vm install.
         - x make database snapshot
         - x fully dump backups of any tables we're going to modify: accounts and email_address_to_account_id

         - write code to update db
           Considerations:
             - deal with accounts that have same email up to case:
                  - for each, lowercase the one that has the most recent login, as determined by
                       select * from successful_sign_ins where account_id=f70dc55e-eeec-434e-ab03-12a5713d009f
             - write code to go through email_address_to_account_id table and create record with lower cased email_address.toLowerCase()
             - write code to go through accounts table and replace record by one with email_address.toLowerCase()

           Algorithm:
             - get list of all pairs (account_id, email_address)
             - use to make map  {lower_email_address:[[email_address, account_id], ... }
             - for each key of that map:
                   - if multiple addresses, query db to see which was used most recently
                   - set accounts record with given account_id to lower_email_address
                   - set email_address_to_account_id --> accounts mapping to map lower_email_address to account_id.
         - test db update on vm
         - now the scary part:
               - push new code to web servers;
               - restart servers
               - make another database table dump
               - run above database update code.


- [x] (0:15?) monitor: add visudo for zpool list for storage use to base vm's.
- [x] (0:15?) monitor: fix bug in zpool monitoring.
- [x] (0:15?) (0:06) admin monitor: raise timeouts, since dns test seems to fail too much
- [x] (0:30?) (0:13) backup script: make it run repeatedly.
- [x] (0:45?) (0:38) backup script: move projects out of the way and retry once on fail (and start running)
- [x] (1:00?) (0:50) close_stale_projects is deciding what is closed/open based on location, which doesn't get reset on close, so it does WAY too many closes; so stupid that it really slows things down for a few minutes, even now.
   - ideas:
       - add a state field to projects, which is one of: 'opened', 'closed'.  On opening a project, set to opened.  On closing a project, set to closed.
       - state would impact whether the killer bothers to kill all procs of process
       - state could also impact UI at some point.
       - could have boolean: `is_open` instead of a state.  last flexible, but less room for error


- [x] (1:00?) (1:08) add an "always confirm on page close" setting; off by default
    alter table accounts add other_settings     varchar;
- [x] (0:20?) (0:54) answer question about r whitespace at http://stackoverflow.com/questions/21591225/suppress-the-extra-white-space-from-compiling-in-r, http://ask.sagemath.org/question/3500/is-there-any-way-to-suppress-the-extra-white-space
- [x] (0:30?) (1:21) install fenics -- http://fenicsproject.org/download/ubuntu_details.html
- [x] fix some /scratch issues: zfs create -o quota=64G -o mountpoint=/scratch projects/scratch; zfs set compression=lz4 projects/scratch    ;  chmod a+rwx /scratch
- [x] (1:00?) monitor: check available space on each project zpool: "ssh storage@10.1.19.4 'sudo zpool list projects'"

- [x] (0:45?) add to my normal monitoring script that every ip address that cloud.sagemath.com maps to actually responds when directly connected to.


- [x] (1:00?) new encrypted offsite backup -- disconnect from internet--done
          time kvm-img convert  -f qcow2 -O qcow2 -o encryption=on,cluster_size=2M  backup1-projects.img  ENCRYPTED-backup1-projects.img
          # real 866m20.407s

- [x] (1:00?) (0:31) opaque background behind a user's name next to their cursor -- https://github.com/sagemath/cloud/issues/2
- [x] (1:00?) (0:35) pretty print broken -- https://github.com/sagemath/cloud/issues/45
- [x] (1:00?) (0:22) another block parser bug -- https://github.com/sagemath/cloud/issues/46
- [x] (1:00?) (0:20) funny filenames -- https://github.com/sagemath/cloud/issues/67
- [x] (0:30?) (0:15) highlight play button -- https://github.com/sagemath/cloud/issues/68
- [x] (0:30?) (0:29) add haskell editor syntax highlighting support.
- [x] (0:30?) (0:10) increase file open timeout to 45 seconds; right now it is shorter than the time it often takes to open a closed project
- [x] (0:30?) (0:01) snap: delete snap cassandra user in database credentials: "DROP USER snap"
- [x] (0:30?) (0:27) new project dialog -- modal-body div looks weird.
- [x] (0:30?) update faq about /scratch (?)

- [x] (1:30?) (4:55) truncate large output in worksheets -- https://github.com/sagemath/cloud/issues/93
       - ideas for what to do with large output:
            - put as blob in database
                 - with ttl
                 - on re-evaluating a cell, do a pass and delete all {omit:uuid} blobs.
                 - will use bandwidth; maybe ttl is BAD -- output might be looked at much later and be very *valuable*.
                 - only big things would go in db.
            - store in file named .[worksheetname]-[output_uuid].txt, one message per *LINE*.  When re-evaluating that cell, if previous output had "omit" in it, then delete file.  But ignore any messages after a point.
                - what happens if worksheet file is moved or deleted?  either way all big output is broken.
                - what happens if user is using git and forgets to add big output?
            - could store *all* extra output in one file named worksheetname.sagews-big_output.txt
            - could store extra output in the same file, but would have to change diffsync mechanism to ignore part of file, etc.  HARD; bad.

       - how to display?
            - for v0 could just show raw file of messages, one per line.
            - for v1 will definitely use existing parser/renderer code to show extra message properly, in the proper place (?)

       - another possibility (attempted: this is way too slow):
            - make the cutoff a lot bigger on local_hub side
            - change only the *renderer* on the client side to ignore extra stuff with a link to show it.
            - do none of the above.

       - another possibility:
            - don't save messages to the worksheet at all or to filesystem.
            - but send them to be rendered *only* by connected clients
            - but don't send "too much"

            This IPython-notebook style, solidly goes against our core design principles, which assume that the
            browser will crash at any moment, and that we do not want to loose work.

       Conclusion: I use the blog database for images, and it has worked surprisingly well so far.  Large output is
       basically the same thing as images -- they *are* large output -- so let's use it for this in pretty much
       the same way.

           - Really do it the same way, by putting the code right in sage_server.py
           - By putting code here, can interrupt running computation if too *many* messages instead of having them mysteriously vanish or not be recorded.
           - Replaces text processing by just knowing the original data (e.g., number of messages), which is much better.
           - Is as similar as possible to what we do already with images.

       This involves starting over.

       Wait, I ended up doing something else entirely... which is simply limiting the number of messages and the size of each message, which seems pretty good.  I did it right at the level of sage_server.py, instead of getting output, then truncating this, we cut the problem at the root.  Also, it is all configurable.  We'll see if this causes people a lot of trouble.  It's *really* short and simple.


- [x] (0:03?) start backups running again.
- [x] (3:00?) project move (dialog to provide info about targets): basically done and sitting in "move" branch.
- [x] (0:30?) (0:28) automatically restart sage worksheet server on open error; just don't do it too frequently.
- [x] (0:10?) (0:19+) shutdown machines on cloud2:

    Doing a proper shutdown of 10.1.2.4.  All done on cloud3.

    # STEP 1: kick all user projects off smoothly

        # cqlsh_connect 10.1.3.2
        update storage_topology set disabled=true where data_center='0' and host='10.1.2.4';
        # cd ~/salvus/salvus; . salvus-env; coffee
        s=require('storage'); s.init(); # WAIT for this to work, then....
        x={};s.close_all_projects(host:'10.1.2.4', limit:20, cb:(e)->x.e=e)
        # the above "close_all_projects" should take a long time; it is paranoid and does a full snapshot of
        # everything on there and replicates it out to all other replicas.  Run more than once just be sure.

        # once the above close works, can login and do
        zpool export projects
        # cd ~/salvus/salvus; ipython    # kill machine
        import admin; cloud = admin.Services('conf/deploy_cloud/')
        cloud.stop('vm',hostname='compute2a')

    Bring it back up later:

        cloud.start('vm',hostname='compute2a')
        # update sources by login as salvus and do - this is only for web VM!
        cd salvus/salvus; . salvus-env
        git pull
        ./make_coffee

        # wait for this to show nothing
        ps ax |grep zpool
        # after it comes up fully
        update storage_topology set disabled=null where data_center='0' and host='10.1.2.4';

    ALSO, I disabled and re-enabled DNS pointing to cloud2 in godaddy...
    (actually, I noticed it was already disabled probably from weeks ago.  128.208.160.166)

- [x] cloud email
- [x] (0:20?) cloud blog post based on australian question -- https://mail.google.com/mail/u/0/?shva=1#search/australia/144055cc6927ea88
- [x] (0:20?) answer email from australia -- https://mail.google.com/mail/u/0/?shva=1#search/australia/144055cc6927ea88



  - [x] investigate again using zfs on image files AND/OR Zvols:  I could
         - have a few 1000 sparse image files stored in a single deduped compressed zpool
         - complete replication and backup of a project is just tarring and sending the sparse images (pretty fast)
         - only zpool import when needed
         - what's import time like though if there are 1500 snapshots?

     - investigate using zvol's, and especially can I dump/restore a zvol easily:

        - idea -- make one thin zvol for each project, and put a different zpool on each zvol

        - to try, start a new vm image on cloud3 with some persistent space to scratch on:

                ./vm.py --base=salvus-2014-02-01-1709 --hostname=zvol --vcpus=8 --ram=4 --disk=zvol:64:none:qcow2 --ip_address=10.11.1.1

        - create zpool

                zpool create -f projects /dev/vdb
                root@zvol:/home/salvus# zfs set compression=lz4 projects
                root@zvol:/home/salvus# zfs set dedup=on projects

        - create thin zvol in it -- http://zfsonlinux.org/example-zvol.html

                root@zvol:/home/salvus# zfs create -V 5G -s projects/x
                root@zvol:/home/salvus# zfs list
                NAME         USED  AVAIL  REFER  MOUNTPOINT
                projects     133K  62.5G    30K  /projects
                projects/x    16K  62.5G    16K  -

        - create zpool on vol

                zpool create abc /dev/zvol/projects/x

        - send/recv my todo project (nearly 500 snapshots)

                root@zvol:/home/salvus# time cat a.lz4 | lz4c -d -| zfs recv -Fu abc


        - test with my huge 1770 snapshot cloud dev project

                zfs snapshot abc2@init
                time zfs send -RD projects/y@init | lz4c - > y.lz4  # takes 4 minutes, 1.3GB

                zfs create -V 10G -s projects/z
                zpool create abc3 /dev/zvol/projects/z
                time cat y.lz4 | lz4c -d -| zfs recv -Fu abc3
                # I gave up after about 20-30 minutes on this!!

     OK, using image files and tar seems like a really interesting approach after all.

        root@zvol:/home/salvus# zfs create images/6cd832d3-c523-41e3-9e54-c8f2d2e8fa2a
        root@zvol:/home/salvus# cd /images/6cd832d3-c523-41e3-9e54-c8f2d2e8fa2a/
        root@zvol:/images/6cd832d3-c523-41e3-9e54-c8f2d2e8fa2a# mkdir images
        root@zvol:/images/6cd832d3-c523-41e3-9e54-c8f2d2e8fa2a# truncate -s 2G images/0.img
        root@zvol:/images/6cd832d3-c523-41e3-9e54-c8f2d2e8fa2a# ls -lhts images/
        total 512
        512 -rw------- 1 root root 2.0G Feb 15 01:11 0.img
        root@zvol:/images/6cd832d3-c523-41e3-9e54-c8f2d2e8fa2a# zpool create projects-6cd832d3-c523-41e3-9e54-c8f2d2e8fa2a -f `pwd`/images/0.img
        root@zvol:/images/6cd832d3-c523-41e3-9e54-c8f2d2e8fa2a# zfs set dedup=on projects-6cd832d3-c523-41e3-9e54-c8f2d2e8fa2a
        root@zvol:/images/6cd832d3-c523-41e3-9e54-c8f2d2e8fa2a# zfs set compression=lz4 projects-6cd832d3-c523-41e3-9e54-c8f2d2e8fa2a
        root@zvol:/images/6cd832d3-c523-41e3-9e54-c8f2d2e8fa2a# time cat a.lz4 | lz4c -d - | zfs recv -Fu projects-6cd832d3-c523-41e3-9e54-c8f2d2e8fa2a
        real    2m18.155s
        user    0m0.524s
        sys     0m10.969s

  -->--> - [x] (2:00?) when starting a project have option to create a new cgroup for that user (modify my create_project_user.py script)
           - [ ] create group [project_id] with dashes removed: cgcreate -g memory,cpu:projectid  # multiple runs are ok
           - [ ] set the rules like so.
                     echo "10G" > /sys/fs/cgroup/memory/projectid/memory.limit_in_bytes
                     echo "250" > /sys/fs/cgroup/cpu/projectid/cpu.shares
           - [ ] append line to /etc/cgrules.conf:  # multiple copies of line are ok...
                      projectid  cpu,memory  projectid
           - [ ] restart cgred daemon: service cgred restart
           - [ ] run cgclassify on all current processes of projectid to ensure they get classified to the cgroup.



           - [ ] create group [project_id] with dashes removed: cgcreate -g memory,cpu:projectid  # multiple runs are ok
           - [ ] set the rules like so.
                     echo "10G" > /sys/fs/cgroup/memory/projectid/memory.limit_in_bytes
                     echo "250" > /sys/fs/cgroup/cpu/projectid/cpu.shares
           - [ ] append line to /etc/cgrules.conf:  # multiple copies of line are ok...
                      projectid  cpu,memory  projectid
           - [ ] restart cgred daemon: service cgred restart
           - [ ] run cgclassify on all current processes of projectid to ensure they get classified to the cgroup.



           - [ ] create group [project_id] with dashes removed: cgcreate -g memory,cpu:projectid  # multiple runs are ok
           - [ ] set the rules like so.
                     echo "10G" > /sys/fs/cgroup/memory/projectid/memory.limit_in_bytes
                     echo "250" > /sys/fs/cgroup/cpu/projectid/cpu.shares
           - [ ] append line to /etc/cgrules.conf:  # multiple copies of line are ok...
                      projectid  cpu,memory  projectid
           - [ ] restart cgred daemon: service cgred restart
           - [ ] run cgclassify on all current processes of projectid to ensure they get classified to the cgroup.

   - [x] add hard coded line to hub.coffee to make cgroups start to get used.

   - [ ] get cgroups to work on gce -- nearly impossible or at least totally FUBAR -- http://stackoverflow.com/questions/21337522/trying-to-use-cgroups-in-debian-wheezy-and-no-daemons


   - [x] update /usr/local/bin/create_project_user.py  # chmod a+rx, etc.
   - [x] sudo su; cd /usr/share/fonts/truetype; ln -s liberation ttf-liberation   # needed for octave
   - [x] sage -sh
         umask 022; pip install theano mahotas scikit-image
   - [x] also install theano, mahotas, scikit-image into new sage-6.1
   - [x] apt-get install cgroup-bin


 - [x]  R package:
        umask 022
        sage -R
        install.packages(c("glmnet"), repos='http://cran.cs.wwu.edu/')
        # and
        /usr/local/sage/sage-6.1/sage -sh
        install.packages(c("glmnet"), repos='http://cran.cs.wwu.edu/')
        # and
        sudo su; umask 022; R
        install.packages(c("glmnet"), repos='http://cran.cs.wwu.edu/')

- [x] (0:45?) (0:22) project move: gce status isn't available -- fix

- [x] (0:30?) (0:13) project move: confirmation/clarification dialog after clicking, which could provide more info.


- [x] (0:45?) (1:39) project move: disable option to move to a machine that isn't available (according to db)
    grant select on table storage_topology to monitor;

- [x] email about "project move"

- [x] (0:30?) (0:07) on first run: first /home/salvus/salvus/salvus/data/local/sbin/tincd --kill   then normal tinc; just fixed by editing /etc/rc.local on base vm.



- [x] deal with "zpool list" slowness and my monitor.
- [x] (0:30?) compute vm's: automate ensuring this line to storage crontab on startup:
           0 5 * * * find /home/storage/.storage* -type f -mmin +120 -delete
           */5 * * * * sudo zpool list > /home/storage/.zpool.list&& cp /home/storage/.zpool.list /home/storage/zpool.list
- [x] (0:45?) fix this printing issue -- https://mail.google.com/mail/u/0/?shva=1#inbox/14367e63a3fa1052
- [x] database -- don't throw uncatchable exceptions, which drop all clients stupidly, break command line sessions, etc. ; also increase reconnect timeout from 3.5 to 5 seconds.

        error: Query cql('UPDATE projects USING ttl 180 SET replicating=true WHERE project_id = ? ',params=1211169b-6460-4d3e-ba7c-4646f742da48) caused a CQL error:
        TimeoutError: Get a connection timed out

        events.js:72
                throw er; // Unhandled 'error' event
                      ^
        TimeoutError: Get a connection timed out
            at checkNextConnection (/home/salvus/salvus/salvus/node_modules/node-cassandra-cql/index.js:146:18)
            at Object._onImmediate (/home/salvus/salvus/salvus/node_modules/node-cassandra-cql/index.js:156:15)
            at processImmediate [as _immediateCallback] (timers.js:330:15)


- [x] this is happening:

        storage@compute4a:~$ sudo zfs rename projects/7ab872da-5f48-4e31-8414-7c5510a1ee4c projects/DELETED-2014-02-16T12:23:50-7ab872da-5f48-4e31-8414-7c5510a1ee4c
        storage@compute4a:~$ cannot rename 'projects/7ab872da-5f48-4e31-8414-7c5510a1ee4c': dataset is busy
        # nothing using lsof, or ps
        storage@compute4a:~$ lsof |grep 7ab872da-5f48-4e31-8414-7c5510a1ee4c
        storage@compute4a:~$ ps ax |grep 7ab872da-5f48-4e31-8414-7c5510a1ee4c
        23694 pts/11   S+     0:00 grep 7ab872da-5f48-4e31-8414-7c5510a1ee4c

    snapshot works still, so maybe if I just switch to using clone (?) instead, then things will be fine?

    I tried the suggestion at https://github.com/zfsonlinux/zfs/issues/1810 and indeed there is a weird ass mount
    but for a completely different filesystem (?).  This makes no sense at all.

        root@compute4a:/home/salvus# df -h /projects/7ab872da-5f48-4e31-8414-7c5510a1ee4c
        Filesystem                           Size  Used Avail Use% Mounted on
        ab872da-5f48-4e31-8414-7c5510a1ee4c  5.0G  128K  5.0G   1% /projects/7ab872da-5f48-4e31-8414-7c5510a1ee4c

    Unmounting that fixed everything.   The uptime is very long, so maybe this is just left over:

        storage@compute4a:~$ uptime
        02:35:56 up 34 days, 22:23,  2 users,  load average: 0.42, 0.68, 0.76


- [x] try this rootkit on a vm to see what happens: https://news.ycombinator.com/item?id=7246836
- [x] this project's replication situation is ugly: d183213f-ed8f-4350-9a88-38fb77858aed
- [x] (0:30?) make the default setting for "delete trailing whitespace" be false.



-->    - [x] have to do full proper shutdown of 10.1.2.4 (kick everybody off) and reboot host vm, since a zfs process is stuck; maybe this isn't so bad, since it will make it easier to do a full proper filling of project data, testing, etc., without having to worry about users on the machine.

       - [x] (0:30?) setup 100GB, and 860GB encrypted partitions --

            fdisk or gparted (?)
            sudo cryptsetup --verify-passphrase --hash=sha256 --cipher=aes-xts-plain:sha256 --key-size=256 luksFormat /dev/sdd1
            sudo cryptsetup --verify-passphrase --hash=sha256 --cipher=aes-xts-plain:sha256 --key-size=256 luksFormat /dev/sdd2

            # I put this in /home/salvus/mountcrypt
            sudo cryptsetup luksOpen /dev/sdd1 cassandra1
            sudo cryptsetup luksOpen /dev/sdd2 projects1

            # check -- device appears lin /dev/mapper/cryptssd

      - [x] (0:30?) modify vm.py script to allow for making a physical device available

      - [x] (0:15?) start a vm with physical device attached, create zpool; do tests, etc.

            root@crypt:/home/salvus# zpool create cassandra -f /dev/vdb
            # Do NOT bother with compression or dedup for cassandra, since they are a waste, as cassandra does all that itself.

      - [x] (0:30?) send/recv data; shutdown cassandra on that node; send/recv data again; switch; startup cassandra.

      - [x] projects

             root@compute2a:/home/salvus# zpool create projects-new -f /dev/vdd
             root@compute2a:/home/salvus# zfs set dedup=on projects-new
             root@compute2a:/home/salvus# zfs set compression=lz4 projects-new
             root@compute2a:/home/salvus# time zfs snapshot projects@migrate
             real    0m8.162s

    Then I made a big qcow2 by adding this in services: "tmpdump:1500:ext4:qcow2".
    Also, change the ip address, so other stuff stops replicating to this machine temporarily.
    Restarted the machine.  Do NOT do this...

             root@compute2a:/home/salvus# time zfs send -RD projects@migrate2 -v > /mnt/tmpdump/a

    Instead, I wrote a new function that returns all projects officially hosted on a vm, along with
    all that were used and hosted there in the last day,week,month.  My plan is to simply restore
    all that were touched in the last day, then week.  Once that is done, I'll make this new projects pool
    live, and continue the restore of the other 8000 projects while it is live, which should be relatively
    safe (when things fail, it self heals).   As long as no user code runs there, this won't be a
    problem.  Since storage_topology doesn't allow users to run here, this should be fine.
    This is good practice for developing my restore system.

- [x] (1:00?) modify vm.py script to have an option to kill an ephemeral machine explicitly, which does all the cleanup of temp stuff.
      When doing the "stop" or "restart" option in admin, explicitly call this vm.py option.


