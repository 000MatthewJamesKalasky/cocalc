# Major projects:
    - better containment through containers, LXC (?), cgroup?, and project rebalancing.
    - fix bugs

# Insanely High priority issues

- [x] show project collaborators in project listing.
- [x] upgrade to node 0.10.21 (there was a security vulnerability that they patched)
- [x] add more x's for clearing search boxes.
- [x] (0:05) serving raw dot files broken: https://github.com/sagemath/cloud/issues/54
- [x] (0:36) latex preview: remove trailing pages when they are removed from latex file

- [x] (0:36) latex editor: removing trailing pages leads to them being re-inserted in reverse order.

- [x] (0:06) latex editor: changing the zoom level doesn't cause a document sync

- [ ] latex editor: look into being more clever about running bibtex and sagetex...

- [ ] make "Save" button rock solid robust with better messages... (nontrivial)

- [ ] centrally cache k new projects -- ready to go -- in the database.
- [ ] sage server -- if broken, don't restart whole project server; instead provide a big warning.
- [ ] input has whitespace lstrip'd - don't do that - easy to see if we make a simple % mode.
- [ ] real progress bar when opening new projects
- [ ] latex editor: allow custom resize by dragging

- [ ] loading ipython notebooks first time is maybe flakie/broken?
- [ ] images not saving?
- [ ] directory browsing: do even more to make it more robust...?
- [ ] collaborators: invite by email


# Very High Priority


- [ ] sagews: try to implement dynamic line numbers for error messages... if possible. (?)  Interesting challenge... or put the errors where they occur as annotations to the input (as widgets!?)

- [ ] make sure to update database to only make new projects on new machines: change admin.py to set entries in compute_servers cassandra table, with option in services file to specify compute server status: off, on but no new projects, on and allowing new projects.

- [ ] file upload for brand new accounts before logout/log-back-in might be broken: https://mail.google.com/mail/u/0/?shva=1#inbox/1418037f49439328


# High priority

- [ ] chat notifications: http://stackoverflow.com/questions/2271156/chrome-desktop-notification-example
- [ ] backup server: moved the rsync-based backup from only 10.1.1.3 to a general service (either separate or part of hub) that has locking and runs on *all* nodes, and has various constraints, targets, rules, etc.     This needs a lot of thought... but right now backups have a single point of failure which is bad.

- [ ] project status; can be displayed in project listing, and will make the slowdown when loading *much* more bareable.

- [ ] if opening a projects takes > 1 second, show a 15-second progress bar and say "starting project server".... it's stupidly purely psychological, but would help people to know that this is expected behavior.  Better -- server could actually respond with a message so this is really meaningful.

- [ ] the SMC in SMC project wouldn't start -- I did "touch .sagemathcloud/installed" and it started instantly.  So...

- [ ] snap servers usually stop working after a while --  they serve requests but just don't make new snapshots.  Since there are so many servers and they all do the same thing, this hasn't actually been a problem yet.  However, it is very dangerous.

- [ ] open image on handheld = no way to close it!

- [ ] better analytics for connect versus reconnect: https://mail.google.com/mail/u/0/?shva=1#search/from%3Aharald/14178e870f62ceac

- [ ] sagetex bug -- it doesn't automatically re-run if input changes...: https://mail.google.com/mail/ca/u/0/#inbox/1418019f0fb715a1

- [ ] SMC in SMC/cassandra -- strangely, in Cassandra, it seems that nodetool doesn't require a username/password, even after auth is enabled.  Maybe upgrading cassandra will help.  WEIRD.; This is not much of a security issues since we firewall cassandra.  But still, it is not good either!

- [ ] If this message comes back from hub, instead of ignoring it, auto-re-sign the user in!  To clarify, make it a new message type rather thna just error, so we don't have to match on output.  This just bit me.
    debug: hub --> client (client=c0cc94b5-22e4-420b-99ab-6564bd5b0843): {"event":"error","id":"fd420696-11d3-46f3-8d30-57b3e844b16c","error":"user must be signed in before accessing pr
ojects"}
    NOTE: fix_connection is not enough... ?

# REVISIT:

HOLD ON!?  Hellenus (the node.js cassandra driver I'm using) doesn't support cassandra2!
I'm looking into https://github.com/jorgebay/node-cassandra-cql, which looks very exciting -- it's the new async binary native
protocol... but actually looks mature.

This might be a fork that would work: https://github.com/gwicke/helenus

  - [ ] Update database to cassandra 2.0.1, then expand to all nodes

  - [ ] upgrade storm to cassandra 2.0.1:
        - see http://www.datastax.com/documentation/cassandra/2.0/webhelp/index.html#upgrade/upgradeC_c.html#concept_ds_yqj_5xr_ck
        - upgrade to latest salvus repo
        nodetool snapshot; nodetool drain # as above
        - build ccassandra version 2.0.1 (as above)
        sudo update-alternatives --config java  # select 0 = oracle java 7

  - [ ] upgrade cloud db to cassandra 2.01

  - [ ] update repair scripts to repair all cassandras -- nodetool_repair

  - [ ] rewrite this to handle all nodes -- update_version_all



# DONE:



## The two common big queries, and both are slow because the use a secondary index or filtering.  These are the only two that do this.

  - [x] `email_address --> account_id`
        This one is probably completely straightforward -- just make one table and keep it up to date.  nOthing ever deleted, so easy.
        and is a bijection so easy.

        --> - [x] make a function that generates this table straight through called: "update_?"
        - [x] test on cloud and see how fast ;-)
        - [x] make code so that when creating an account an entry is added.

NOTES:
     create user wstein with password '...';
     grant all on table email_address_to_account_id to wstein;
     grant select on table accounts to wstein
     grant all on table accounts to wstein;

  - [x] table of counts:
          - user count
          - project count
        plus function to compute these
        but also make it so anything that changes the counts updates this table.
        Then make them consistency every so often....
        So that stats is fast.

  - [x] but even with above, stats takes 9 seconds with QUORUM!... and 4 seconds with consistency 1.
    currently, without those opts, it takes 8.5 seconds with consistency 1.
    There are 76 hubs all doing that query every 60 seconds... to 10 target nodes.
    That alone should lead to issues (except due to threading).
    This could explain the high load on the cassandra nodes.

    SO... implement a global cache, so we can afford the 9 seconds with quorum for good results,
    so we don't record 76 of these per seconds in the db, which is STUPID, etc.
    Do this with a stats_cache 1-entry table that has a ttl.

[x]-- ARGH.  It turns out that collections don't work with Helenus... and there not working gets worse with the next few versions.
   It would be better to sue the CQL3 binary driver, but that doesn't work with older node.js versions!
   I used node 0.8.? because of the http-proxy module.  However, there is a new fork of that code that
   supposedly works with 0.10.  https://github.com/nodejitsu/node-http-proxy/tree/caronte

   PLAN:
     - install node 0.10 in devel project -- builds
     - see if binary cql3 driver is actually usable and good and solid -- YES!
     - if so, see if works for collections -- YES!
     - if so, see if caronte node-proxy works:
           - git clone https://github.com/nodejitsu/node-http-proxy.git -b caronte
           - npm install node-http-proxy/
           - looks like I have to "totally rewrite" my hub node proxy code, since
             the api is completely different.  ugh.
             options: https://github.com/nodejitsu/node-http-proxy/blob/caronte/lib/http-proxy.js

     - CONCLUSION: it all works!

     [x] - if so, switch it all!
           - Add node-cassandra-cql to npm package list
           - get rid of node v0.8.25

 - [x] rewrite cassandra.coffee to use node-cassandra-cql instead of helenus.

 - [x] TEST: maybe now I can switch to cassandra2 (?)

 - [x] rewrite project group stuff:

     - cassandra.coffee
     - in hub.coffee: "Permissions related to projects" (line 3581)

 - [x] `project_users by account_id` -- or I could just add project collabs, etc. to the project itself and the account. Hmm.
        that feels dangerous.  What would happen though would:

            alter table projects add owner set<uuid>;
            alter table projects add collaborator set<uuid>;
            alter table projects add viewer set<uuid>;
            alter table projects add invited_collaborator set<uuid>;
            alter table projects add invited_viewer set<uuid>;

            alter table accounts add owner set<uuid>;
            alter table accounts add collaborator set<uuid>;
            alter table accounts add viewer set<uuid>;
            alter table accounts add invited_collaborator set<uuid>;
            alter table accounts add invited_viewer set<uuid>;

          - in accounts() have 3 entries: that is the list of projects that this account owns, collabs, views
          - in projects() have 3 entries: accounts that owns, collabs, views

          Use the new cassandra collection types?
          This seems like exactly the sort of normalization that noSQL folks like.
          And it will make all relevant queries "scary fast".


  - [ ] worry about *firewall*

---



# Current: increase number of cassandra nodes to 19.

- try adding cassandra node 6...

(NOT SO SIMPLE!)
--> - [ ] storm: expand cassandra to 19 nodes (but with 4GB/VM -- minimize waste)
        - rewrite the repair script to work on all nodes.
        - new base vm image with cassandra 1.2.9
        - modify services file
        - start new vm's -- has to make new disks, etc.
        - restart cassandra host vm's:
             [storm.restart('vm',hostname='storm-cassandra%s'%i,wait=False) for i in range(1,8) + range(10,22)]
             storm.wait_until_up()
        - start all cassandra's:
             storm.start('cassandra', parallel=True, wait=False)
        - nodetool repair on all

        - change params of how many nodes have each piece of data
        - nodetool repair on all

- [ ] cloud: expand cassandra to 19 nodes...
      (still need to add nodes 13,14,15,16,17,18,19,20,21)


- [x] (0:33) it seems like the green spinner when computing in a worksheet... is just gone?  It's not working for me.  I have no idea how this broke, but this should confuse the hell out of everyone!   FIX THIS for next release for sure.

- [x] unicode issues when running sws2sagews.py: https://mail.google.com/mail/u/0/?shva=1#inbox/14183fbe5ba59141 where I changed the unicode calls around line 106 to:
    BETTER: use encoding='utf8') !
- [x] (0:51) implement harald's auto-kill-old-stuff approach -- decided not to deploy it
- [x] add a --timeout option to the `local_hub`, where it will pkill everything running as that user after a given amount of time.


- [x] setup so harald can do dev.

- [x] **CRITICAL** raw file download/browsing seems broken now.  CRAP.  Probably the result of an nginx build/config issue.  Fixing this is top priority.

- [x] next release:
    x- apt-get upgrade
    x- new deb packages: haskell-platform haskell-platform-doc haskell-platform-prof  mono-devel mono-tools-devel ocaml ocaml-doc tuareg-mode ocaml-mode libgdbm-dev mlton sshfs sparkleshare dropbox
    x- install SML (requested on google+)
    x- install ocaml
    x- install some haskell thing that's maybe mentioned on github issues (??): haskell-platform
    x - make it so on startup the ssh keys in the /home/salvus/ directory are deleted, since they aren't needed on the compute
        machine, and are dangerous, since they provide access to other things...
    x - rebuild nginx on base machine, and ensure that "-V" doesn't show rewrite module missing!
    x - manually login to each compute node and change salvus account password, since
        that password is stored in the external image for each compute machine...
    x- update vm again; push out
    x- update repos on cloud1-4
    x- update services files in cloud/ and storm/

    x- restart storm system
    x- quickly paste in code to add slavus/hub/snap users: see file box/DANGER.txt in laptop
    x- if stuff doesn't just start working:
          storm.restart('hub'); storm.restart('snap'); storm.restart('haproxy')
    x- test storm very hard due to all the `base_url` stuff -- make sure that doesn't break stuff.

    x- update main cloud same way as above.


- NEXT goal: make it so I can 100% do cloud development in cloud, i.e., make it so my "cloud.sagemath" project
  is a complete running copy of cloud, using some port forward.  Why:
      x- snapshotting
      x- aleviate my limited-memory laptop issues
      x- collab.

    - [x] Ports -- make it possible to customize all of these via services file, in one line at top
        HAPROXY_PORT = 8000
        NGINX_PORT   = 8080
        HUB_PORT       = 5000
        HUB_PROXY_PORT = 5001
        CASSANDRA_CLIENT_PORT = 9160
        CASSANDRA_INTERNODE_PORTS = [7000, 7001]



    - [x] cassandra is "wide open" -- must implement auth; first do each of these in my devel db,
          then for laptop, then storm, then cloud during next release:

          1. Add this on the [cassandra] line of services (in all of the services files):
   'authenticator':'org.apache.cassandra.auth.PasswordAuthenticator', 'authorizer':'org.apache.cassandra.auth.CassandraAuthorizer'

           - increase replication level of system_auth keyspace (only for cloud and storm) -- run "nodetool repair" on all nodes
             after doing this (done on cloud and storm, and nodetool repair started):

                ALTER KEYSPACE system_auth WITH REPLICATION = {'class' : 'NetworkTopologyStrategy', 'DC0' : 2, 'DC1' : 2};

           - create 'salvus' superuser, and 'hub', 'snap' users:
           - remove cassandra superuser:
           - grant rights to 'hub' and 'snap' users...

                CREATE USER salvus WITH PASSWORD '<random 16 characters>' SUPERUSER;
                CREATE USER hub WITH PASSWORD '<random 16 characters>';
                CREATE USER snap WITH PASSWORD '<random 16 characters>';
                DROP USER cassandra
                GRANT ALL ON KEYSPACE salvus TO hub;
                GRANT ALL ON KEYSPACE salvus to snap;   /* make more restrictive later once this is done and working */

                cqlsh:salvus> list users;
                 name   | super
                --------+-------
                 salvus |  True
                   snap | False
                    hub | False


           - make 'hub' use its user
           - make 'snap' use its user

       x- Make sure clear docs in cassandra.py about exactly how to set these things up.
       x- Make passwords random 16-character strings in secrets/ directory, and NOT in github.
       x- Make one set for local dev, and a different set for deployed cloud.



    - [x] update hosts so cloud10-21 hostnames just work...

    - [x] tmux: way to easily parallel ssh to specific list of cloud[n]'s all at once....

    - [x] add every cloud[n] to the vpn as 10.1.n.1.
          - [x] Since it didn't build due to liblzo2-dev:
                apt-get install liblzo2-dev
                cd salvus/salvus; . salvus-env; ./build.py --build_tinc

          - [x] generate public/private 2048-bit keys
                    cd salvus/salvus/; . salvus-env; cd data/local/etc/tinc/; rm rsa_key.priv; vi tinc.conf; vi tinc-up; tincd -K;  vi hosts/`hostname`
                    sudo /home/salvus/salvus/salvus/data/local/sbin/tincd
                    ...On cloud1...
                    scp cloud12:salvus/salvus/data/local/etc/tinc/hosts/cloud12 salvus/salvus/conf/tinc_hosts/
                    ...then test.
          - [x] check-in hosts to repo on cloud1
          - [x] push repo to all nodes
cd salvus/salvus/conf/tinc_hosts/; rm pixel sage_admin; git pull cloud1:salvus/
          - [x] on all nodes, change the tinc.conf so that ConnectTo points to all other machines.
          - [x] update *all* repos with newest salvus repo with this checkin
          - [x] on all nodes, restart tincd, so that ConnectTo takes effect, so all traffic doesn't go through 1-4 only.
          - [x] Add this to /etc/rc.local on all nodes
                sudo chmod a+r /boot/vmlinuz-* # used by guestmount
                nice --19 /home/salvus/salvus/salvus/data/local/sbin/tincd
          - [x] get vm.py to work on all nodes.
                   sudo apt-get install guestfish virtinst
                   sudo chmod a+r /boot/vmlinuz-*
                   Add salvus to kvm /etc/group

  - [x] setup rsync backup the 19 salvus/ directories to directories cloud1...cloud[n] in my backup target.  -- that will make rebuild/conf easier?
  - [x] storm: manually update compute_server table (for now)
            cqlsh:salvus> truncate compute_servers;
            cqlsh:salvus> select * from compute_servers
            cqlsh:salvus> update compute_servers set running=true, score=1 where host='10.2.5.4';
            cqlsh:salvus> update compute_servers set running=true, score=1 where host='10.2.6.4';
            cqlsh:salvus> select * from compute_servers;
            for i in [5,6,7] + [10..21]:
                print "update compute_servers set running=true, score=1 where host='10.2.%i.4';"%i



  - [x] expand storm to use all nodes:
      - updated tmuxlogin-storm-db script

  - [x] cloud: manually update compute_server table (for now)
            cqlsh:salvus> truncate compute_servers;
            cqlsh:salvus> select * from compute_servers
            cqlsh:salvus> update compute_servers set running=true, score=1 where host='10.2.5.4';
            cqlsh:salvus> update compute_servers set running=true, score=1 where host='10.2.6.4';
            cqlsh:salvus> select * from compute_servers;
            for i in [5,6,7] + [10..21]:
                print "update compute_servers set running=true, score=1 where host='10.2.%i.4';"%i


 - [x] update backup scripts to backup all nodes

 - [x] upgrade storm to cassandra 1.2.9
        - delete 'thrift_max_message_length_in_mb' in services file
        - login to all database nodes at once...
        - edit CASSANDRA_VERSION in build.py to 1.2.9
        nodetool snapshot; nodetool drain
        ./build.py --build_cassandra   # 10 second
        - restart cassandra from cloud1 admin console
        - check that everything starts
        nodetool status
        nodetool drain
        nodetool repair  # DO one one node at a time, so that whole website doesn't become unresponsive.  Each takes a long time.
        nodetool drain

nodetool --with-snapshot -pr repair

  - [x] upgrade cloud to cassandra 1.2.9
  - [x] use all the hub nodes
  - [x] usa all the new snapshot servers
1- [x] next release:
      - [x] pip install psutil # in sage
      - [x] pip install oct2py # in sage
      - [x] pip install munkres
      - [x] update salvus repo...

- [x] change helenus to use consistency quorum instead of the default of one for reads.
  This will massively improve things, I think.


  - [x] new release
     - [x] upgrade to sage-5.12
     - [x] upgrade to ipython-1.1
     - [x] upgrade node:
              ./build.py --build_node
     - [x] upgrade node packages:
              npm install node-cassandra-cql
              git clone https://github.com/nodejitsu/node-http-proxy.git -b caronte; npm install node-http-proxy/
              rm -rf node-http-proxy/

     - [x] update database tables:
              fs=require('fs'); a = new (require("cassandra").Salvus)(keyspace:'salvus', hosts:['10.1.1.2','10.1.2.2'], username:'wstein', password:fs.readFileSync('data/secrets/cassandra/wstein').toString().trim(), cb:console.log)
              a = new (require("cassandra").Salvus)(keyspace:'salvus', hosts:['localhost:8403'], username:'salvus', password:fs.readFileSync('data/secrets/cassandra/salvus').toString().trim(), cb:console.log)
              a.update_email_address_to_account_id_table(console.log)
              a.update_project_count(console.log)
              a.migrate_from_deprecated_project_users_table(console.log)

     - [x] update database schema:

            grant all on keyspace salvus to wstein;

            alter table projects add owner set<uuid>;
            alter table projects add collaborator set<uuid>;
            alter table projects add viewer set<uuid>;
            alter table projects add invited_collaborator set<uuid>;
            alter table projects add invited_viewer set<uuid>;

            alter table accounts add owner set<uuid>;
            alter table accounts add collaborator set<uuid>;
            alter table accounts add viewer set<uuid>;
            alter table accounts add invited_collaborator set<uuid>;
            alter table accounts add invited_viewer set<uuid>;

            drop table hub_servers;
            drop table snap_servers;
            drop table compute_servers;

            CREATE TABLE hub_servers (
                dummy      boolean,    /* so fast */
                host       varchar,    /* hostname (ip address) */
                port       int,
                clients    int,        /* total number of connected clients right now */
                PRIMARY KEY(dummy, host, port)
            );
            CREATE TABLE snap_servers (
                dummy      boolean,
                id         uuid,
                host       varchar,    /* hostname */
                port       int,        /* integer port */
                key        varchar,    /* random string needed to unlock tcp connection */
                size       int,        /* total size in kilobytes of the objects/pack directory of the archive */
                PRIMARY KEY(dummy, id)
            );

            CREATE TABLE compute_servers (
                dummy boolean,
                host varchar,
                running boolean,
                score int,
                PRIMARY KEY(dummy, host)
            );
            CREATE INDEX ON compute_servers (running);

            update compute_servers set running=true, score=1 where dummy=true and host='10.1.5.4';
            update compute_servers set running=true, score=1 where dummy=true and host='10.1.6.4';
            update compute_servers set running=true, score=1 where dummy=true and host='10.1.7.4';
            update compute_servers set running=true, score=1 where dummy=true and host='10.1.10.4';
            update compute_servers set running=true, score=1 where dummy=true and host='10.1.11.4';
            update compute_servers set running=true, score=1 where dummy=true and host='10.1.12.4';
            update compute_servers set running=true, score=1 where dummy=true and host='10.1.13.4';
            update compute_servers set running=true, score=1 where dummy=true and host='10.1.14.4';
            update compute_servers set running=true, score=1 where dummy=true and host='10.1.15.4';
            update compute_servers set running=true, score=1 where dummy=true and host='10.1.16.4';
            update compute_servers set running=true, score=1 where dummy=true and host='10.1.17.4';
            update compute_servers set running=true, score=1 where dummy=true and host='10.1.18.4';
            update compute_servers set running=true, score=1 where dummy=true and host='10.1.19.4';
            update compute_servers set running=true, score=1 where dummy=true and host='10.1.20.4';
            update compute_servers set running=true, score=1 where dummy=true and host='10.1.21.4';

            CREATE TABLE stats_cache (
                dummy    boolean PRIMARY KEY,
                timestamp varchar,
                accounts int,
                projects int,
                active_projects int,
                last_day_projects int,
                last_week_projects int,
                last_month_projects int,
                snap_servers int,
                hub_servers varchar /* JSON */
            );

            CREATE TABLE counts (
                table_name varchar PRIMARY KEY,
                count counter
            );

            CREATE TABLE email_address_to_account_id (
                email_address varchar PRIMARY KEY,
                account_id    uuid
            );



     - [x] revoke wstein user.

- [x] change database to not use index on email_address, etc., so can use consistency level quorum.

- [x] database timeouts lead to CRAP -- must handle this!
      if this happens once -- with hub or snap -- it never recovers.  STUPID.
      On the other hand, with 19 nodes, this is not likely to ever happen.
      Also, since I'll likely switch drivers, this is again maybe premature..

        HelenusTimedOutException
            at onReturn (/home/salvus/salvus/salvus/node_modules/helenus/lib/connection.js:377:23)
            at exports.Connection.connection.addListener.self.transport.receiver.client._reqs.(anonymous function) (/home/salvus/salvus/salvus/node_modules/helenus/node_modules/helenus-thrift/lib/thrift/connection.js:80:11)
            at Object.CassandraClient.recv_execute_cql3_query (/home/salvus/salvus/salvus/node_modules/helenus/lib/cassandra/Cassandra.js:8337:12)
            at exports.Connection (/home/salvus/salvus/salvus/node_modules/helenus/node_modules/helenus-thrift/lib/thrift/connection.js:83:37)
            at Socket.TFramedTransport.receiver (/home/salvus/salvus/salvus/node_modules/helenus/node_modules/helenus-thrift/lib/thrift/transport.js:70:9)
            at Socket.EventEmitter.emit (events.js:96:17)
            at TCP.onread (net.js:397:14)
        debug: Failed to connect to database! -- HelenusNoAvailableNodesException: Could Not Connect To Any Nodes

  - [x] increase replication factor:
        ALTER KEYSPACE "salvus" WITH REPLICATION = {'class' : 'NetworkTopologyStrategy', 'DCO':4, 'DC1':3};



- [x] upgrade to sage-5.12: http://boxen.math.washington.edu/home/release/sage-5.12/

  - [x] fix snap servers not registering with database...
  - [x] don't delete pdf file when clicking "delete aux files"
- [x] base vm
    - [x] install fields package into R:
             umask 022 && R
             install.packages("fields", repos='http://cran.us.r-project.org')
    - [x] install Stein-watkins full database:
             umask 022; sage -i stein-watkins-ecdb; rm /usr/local/sage/current/spkg/optional/*.spkg

- [x] Massive bug: that filename/not matching directory path in UI thing leads to a huge bug where users will loose work, etc.
        see the log in bugs/
        the problem I think is that sessions are keyed by session id and filename, and filename is wrong...
1