
- Major projects:
    - make use of new hardware
    - fix bugs
    - better containment through containers, LXC (?), cgroup?, and project rebalancing.

# Current: increase capacity of cluster by adding cloud5-7, cloud10-21!

- change helenus to use consistency quorum instead of the default of one for reads.
  This will massively improve things, I think.

(NOT SO SIMPLE!)
--> - [ ] storm: expand cassandra to 19 nodes (but with 4GB/VM -- minimize waste)
        - rewrite the repair script to work on all nodes.
        - new base vm image with cassandra 1.2.9
        - modify services file
        - start new vm's -- has to make new disks, etc.
        - restart cassandra host vm's:
             [storm.restart('vm',hostname='storm-cassandra%s'%i,wait=False) for i in range(1,8) + range(10,22)]
             storm.wait_until_up()
        - start all cassandra's:
             storm.start('cassandra', parallel=True, wait=False)
        - nodetool repair on all

        - change params of how many nodes have each piece of data
        - nodetool repair on all

- [ ] cloud: expand cassandra to 19 nodes - I guess I'll just add one node at a time for the next few weeks...


- [ ] real progress bar when opening new projects

- [ ] collaborators: make collabs listed along with projects in project listing
- [ ] collaborators: make search have a button, instead of constant update which is confusing
- [ ] collaborators: invite by email


# Insanely High priority issues

- [ ] database timeouts lead to CRAP -- must handle this!
      if this happens once -- with hub or snap -- it never recovers.  STUPID.
      On the other hand, with 19 nodes, this is not likely to ever happen.
      Also, since I'll likely switch drivers, this is again maybe premature..

        HelenusTimedOutException
            at onReturn (/home/salvus/salvus/salvus/node_modules/helenus/lib/connection.js:377:23)
            at exports.Connection.connection.addListener.self.transport.receiver.client._reqs.(anonymous function) (/home/salvus/salvus/salvus/node_modules/helenus/node_modules/helenus-thrift/lib/thrift/connection.js:80:11)
            at Object.CassandraClient.recv_execute_cql3_query (/home/salvus/salvus/salvus/node_modules/helenus/lib/cassandra/Cassandra.js:8337:12)
            at exports.Connection (/home/salvus/salvus/salvus/node_modules/helenus/node_modules/helenus-thrift/lib/thrift/connection.js:83:37)
            at Socket.TFramedTransport.receiver (/home/salvus/salvus/salvus/node_modules/helenus/node_modules/helenus-thrift/lib/thrift/transport.js:70:9)
            at Socket.EventEmitter.emit (events.js:96:17)
            at TCP.onread (net.js:397:14)
        debug: Failed to connect to database! -- HelenusNoAvailableNodesException: Could Not Connect To Any Nodes


# Very High Priority

- [ ] sagews: try to implement dynamic line numbers for error messages... if possible. (?)  Interesting challenge... or put the errors where they occur as annotations to the input (as widgets!?)

- [ ] make sure to update database to only make new projects on new machines: change admin.py to set entries in compute_servers cassandra table, with option in services file to specify compute server status: off, on but no new projects, on and allowing new projects.

- [ ] file upload for brand new accounts before logout/log-back-in might be broken: https://mail.google.com/mail/u/0/?shva=1#inbox/1418037f49439328

# High priority

- [ ] backup server: moved the rsync-based backup from only 10.1.1.3 to a general service (either separate or part of hub) that has locking and runs on *all* nodes, and has various constraints, targets, rules, etc.     This needs a lot of thought... but right now backups have a single point of failure which is bad.

- [ ] project status; can be displayed in project listing, and will make the slowdown when loading *much* more bareable.

- [ ] if opening a projects takes > 1 second, show a 15-second progress bar and say "starting project server".... it's stupidly purely psychological, but would help people to know that this is expected behavior.  Better -- server could actually respond with a message so this is really meaningful.

- [ ] the SMC in SMC project wouldn't start -- I did "touch .sagemathcloud/installed" and it started instantly.  So...

- [ ] snap servers usually stop working after a while --  they serve requests but just don't make new snapshots.  Since there are so many servers and they all do the same thing, this hasn't actually been a problem yet.  However, it is very dangerous.

- [ ] open image on handheld = no way to close it!

- [ ] better analytics for connect versus reconnect: https://mail.google.com/mail/u/0/?shva=1#search/from%3Aharald/14178e870f62ceac

- [ ] sagetex bug -- it doesn't automatically re-run if input changes...: https://mail.google.com/mail/ca/u/0/#inbox/1418019f0fb715a1

- [ ] SMC in SMC/cassandra -- strangely, in Cassandra, it seems that nodetool doesn't require a username/password, even after auth is enabled.  Maybe upgrading cassandra will help.  WEIRD.; This is not much of a security issues since we firewall cassandra.  But still, it is not good either!

- [ ] If this message comes back from hub, instead of ignoring it, auto-re-sign the user in!  To clarify, make it a new message type rather thna just error, so we don't have to match on output.  This just bit me.
    debug: hub --> client (client=c0cc94b5-22e4-420b-99ab-6564bd5b0843): {"event":"error","id":"fd420696-11d3-46f3-8d30-57b3e844b16c","error":"user must be signed in before accessing pr
ojects"}
    NOTE: fix_connection is not enough... ?

# REVISIT:

HOLD ON!?  Hellenus (the node.js cassandra driver I'm using) doesn't support cassandra2!
I'm looking into https://github.com/jorgebay/node-cassandra-cql, which looks very exciting -- it's the new async binary native
protocol... but actually looks mature.

This might be a fork that would work: https://github.com/gwicke/helenus

  - [ ] Update database to cassandra 2.0.1, then expand to all nodes

  - [ ] upgrade storm to cassandra 2.0.1:
        - see http://www.datastax.com/documentation/cassandra/2.0/webhelp/index.html#upgrade/upgradeC_c.html#concept_ds_yqj_5xr_ck
        - upgrade to latest salvus repo
        nodetool snapshot; nodetool drain # as above
        - build ccassandra version 2.0.1 (as above)
        sudo update-alternatives --config java  # select 0 = oracle java 7

  - [ ] upgrade cloud db to cassandra 2.01

  - [ ] update repair scripts to repair all cassandras -- nodetool_repair

  - [ ] rewrite this to handle all nodes -- update_version_all



# DONE:

- [x] (0:33) it seems like the green spinner when computing in a worksheet... is just gone?  It's not working for me.  I have no idea how this broke, but this should confuse the hell out of everyone!   FIX THIS for next release for sure.

- [x] unicode issues when running sws2sagews.py: https://mail.google.com/mail/u/0/?shva=1#inbox/14183fbe5ba59141 where I changed the unicode calls around line 106 to:
    BETTER: use encoding='utf8') !
- [x] (0:51) implement harald's auto-kill-old-stuff approach -- decided not to deploy it
- [x] add a --timeout option to the `local_hub`, where it will pkill everything running as that user after a given amount of time.


- [x] setup so harald can do dev.

- [x] **CRITICAL** raw file download/browsing seems broken now.  CRAP.  Probably the result of an nginx build/config issue.  Fixing this is top priority.

- [x] next release:
    x- apt-get upgrade
    x- new deb packages: haskell-platform haskell-platform-doc haskell-platform-prof  mono-devel mono-tools-devel ocaml ocaml-doc tuareg-mode ocaml-mode libgdbm-dev mlton sshfs sparkleshare dropbox
    x- install SML (requested on google+)
    x- install ocaml
    x- install some haskell thing that's maybe mentioned on github issues (??): haskell-platform
    x - make it so on startup the ssh keys in the /home/salvus/ directory are deleted, since they aren't needed on the compute
        machine, and are dangerous, since they provide access to other things...
    x - rebuild nginx on base machine, and ensure that "-V" doesn't show rewrite module missing!
    x - manually login to each compute node and change salvus account password, since
        that password is stored in the external image for each compute machine...
    x- update vm again; push out
    x- update repos on cloud1-4
    x- update services files in cloud/ and storm/

    x- restart storm system
    x- quickly paste in code to add slavus/hub/snap users: see file box/DANGER.txt in laptop
    x- if stuff doesn't just start working:
          storm.restart('hub'); storm.restart('snap'); storm.restart('haproxy')
    x- test storm very hard due to all the `base_url` stuff -- make sure that doesn't break stuff.

    x- update main cloud same way as above.


- NEXT goal: make it so I can 100% do cloud development in cloud, i.e., make it so my "cloud.sagemath" project
  is a complete running copy of cloud, using some port forward.  Why:
      x- snapshotting
      x- aleviate my limited-memory laptop issues
      x- collab.

    - [x] Ports -- make it possible to customize all of these via services file, in one line at top
        HAPROXY_PORT = 8000
        NGINX_PORT   = 8080
        HUB_PORT       = 5000
        HUB_PROXY_PORT = 5001
        CASSANDRA_CLIENT_PORT = 9160
        CASSANDRA_INTERNODE_PORTS = [7000, 7001]



    - [x] cassandra is "wide open" -- must implement auth; first do each of these in my devel db,
          then for laptop, then storm, then cloud during next release:

          1. Add this on the [cassandra] line of services (in all of the services files):
   'authenticator':'org.apache.cassandra.auth.PasswordAuthenticator', 'authorizer':'org.apache.cassandra.auth.CassandraAuthorizer'

           - increase replication level of system_auth keyspace (only for cloud and storm) -- run "nodetool repair" on all nodes
             after doing this (done on cloud and storm, and nodetool repair started):

                ALTER KEYSPACE system_auth WITH REPLICATION = {'class' : 'NetworkTopologyStrategy', 'DC0' : 2, 'DC1' : 2};

           - create 'salvus' superuser, and 'hub', 'snap' users:
           - remove cassandra superuser:
           - grant rights to 'hub' and 'snap' users...

                CREATE USER salvus WITH PASSWORD '<random 16 characters>' SUPERUSER;
                CREATE USER hub WITH PASSWORD '<random 16 characters>';
                CREATE USER snap WITH PASSWORD '<random 16 characters>';
                DROP USER cassandra
                GRANT ALL ON KEYSPACE salvus TO hub;
                GRANT ALL ON KEYSPACE salvus to snap;   /* make more restrictive later once this is done and working */

                cqlsh:salvus> list users;
                 name   | super
                --------+-------
                 salvus |  True
                   snap | False
                    hub | False


           - make 'hub' use its user
           - make 'snap' use its user

       x- Make sure clear docs in cassandra.py about exactly how to set these things up.
       x- Make passwords random 16-character strings in secrets/ directory, and NOT in github.
       x- Make one set for local dev, and a different set for deployed cloud.



    - [x] update hosts so cloud10-21 hostnames just work...

    - [x] tmux: way to easily parallel ssh to specific list of cloud[n]'s all at once....

    - [x] add every cloud[n] to the vpn as 10.1.n.1.
          - [x] Since it didn't build due to liblzo2-dev:
                apt-get install liblzo2-dev
                cd salvus/salvus; . salvus-env; ./build.py --build_tinc

          - [x] generate public/private 2048-bit keys
                    cd salvus/salvus/; . salvus-env; cd data/local/etc/tinc/; rm rsa_key.priv; vi tinc.conf; vi tinc-up; tincd -K;  vi hosts/`hostname`
                    sudo /home/salvus/salvus/salvus/data/local/sbin/tincd
                    ...On cloud1...
                    scp cloud12:salvus/salvus/data/local/etc/tinc/hosts/cloud12 salvus/salvus/conf/tinc_hosts/
                    ...then test.
          - [x] check-in hosts to repo on cloud1
          - [x] push repo to all nodes
cd salvus/salvus/conf/tinc_hosts/; rm pixel sage_admin; git pull cloud1:salvus/
          - [x] on all nodes, change the tinc.conf so that ConnectTo points to all other machines.
          - [x] update *all* repos with newest salvus repo with this checkin
          - [x] on all nodes, restart tincd, so that ConnectTo takes effect, so all traffic doesn't go through 1-4 only.
          - [x] Add this to /etc/rc.local on all nodes
                sudo chmod a+r /boot/vmlinuz-* # used by guestmount
                nice --19 /home/salvus/salvus/salvus/data/local/sbin/tincd
          - [x] get vm.py to work on all nodes.
                   sudo apt-get install guestfish virtinst
                   sudo chmod a+r /boot/vmlinuz-*
                   Add salvus to kvm /etc/group

  - [x] setup rsync backup the 19 salvus/ directories to directories cloud1...cloud[n] in my backup target.  -- that will make rebuild/conf easier?
  - [x] storm: manually update compute_server table (for now)
            cqlsh:salvus> truncate compute_servers;
            cqlsh:salvus> select * from compute_servers
            cqlsh:salvus> update compute_servers set running=true, score=1 where host='10.2.5.4';
            cqlsh:salvus> update compute_servers set running=true, score=1 where host='10.2.6.4';
            cqlsh:salvus> select * from compute_servers;
            for i in [5,6,7] + [10..21]:
                print "update compute_servers set running=true, score=1 where host='10.2.%i.4';"%i



  - [x] expand storm to use all nodes:
      - updated tmuxlogin-storm-db script

  - [x] cloud: manually update compute_server table (for now)
            cqlsh:salvus> truncate compute_servers;
            cqlsh:salvus> select * from compute_servers
            cqlsh:salvus> update compute_servers set running=true, score=1 where host='10.2.5.4';
            cqlsh:salvus> update compute_servers set running=true, score=1 where host='10.2.6.4';
            cqlsh:salvus> select * from compute_servers;
            for i in [5,6,7] + [10..21]:
                print "update compute_servers set running=true, score=1 where host='10.2.%i.4';"%i


 - [x] update backup scripts to backup all nodes

 - [x] upgrade storm to cassandra 1.2.9
        - delete 'thrift_max_message_length_in_mb' in services file
        - login to all database nodes at once...
        - edit CASSANDRA_VERSION in build.py to 1.2.9
        nodetool snapshot; nodetool drain
        ./build.py --build_cassandra   # 10 second
        - restart cassandra from cloud1 admin console
        - check that everything starts
        nodetool status
        nodetool drain
        nodetool repair  # DO one one node at a time, so that whole website doesn't become unresponsive.  Each takes a long time.
        nodetool drain

nodetool --with-snapshot -pr repair

  - [x] upgrade cloud to cassandra 1.2.9
  - [x] use all the hub nodes
  - [x] usa all the new snapshot servers
1- [x] next release:
      - [x] pip install psutil # in sage
      - [x] pip install oct2py # in sage
      - [x] pip install munkres
      - [x] update salvus repo...
