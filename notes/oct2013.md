- [ ] next release:
      pip install psutil # both in sage and system-wide and add to build.py
      pip install oct2py # in sage and in build.py!!!

- [x] (0:51) implement harald's auto-kill-old-stuff approach

- [ ] add a --timeout option to the `local_hub`, where it will pkill everything running as that user after a given amount of time.

- [ ] increase capacity of cluster:

    x - update hosts so cloud10-21 hostnames just work...

    - add 10.1.10-21.1 to vpn   (cloud10-21):

      - Since it didn't build due to liblzo2-dev:

            apt-get install liblzo2-dev
            cd salvus/salvus; . salvus-env; ./build.py --build_tinc

      - Add this to /etc/rc.local

            sudo chmod a+r /boot/vmlinuz-* # used by guestmount
            nice --19 /home/salvus/salvus/salvus/data/local/sbin/tincd


# Very High priority issues

- [ ] If this message comes back from hub, instead of ignoring it, auto-re-sign the user in!  To clarify, make it a new message type rather thna just error, so we don't have to match on output.  This just bit me.  
    debug: hub --> client (client=c0cc94b5-22e4-420b-99ab-6564bd5b0843): {"event":"error","id":"fd420696-11d3-46f3-8d30-57b3e844b16c","error":"user must be signed in before accessing pr
ojects"}
    NOTE: fix_connection is not enough... ?

- [x] unicode issues when running sws2sagews.py: https://mail.google.com/mail/u/0/?shva=1#inbox/14183fbe5ba59141 where I changed the unicode calls around line 106 to:
    BETTER: use encoding='utf8') !

# High priority

- [ ] file upload for brand new accounts before logout/log-back-in might be broken: https://mail.google.com/mail/u/0/?shva=1#inbox/1418037f49439328

- [ ] if opening a projects takes > 1 second, show a 15-second progress bar and say "starting project server".... it's stupidly purely psychological, but would help people to know that this is expected behavior.  Better -- server could actually respond with a message so this is really meaningful.

- [ ] the SMC in SMC project wouldn't start -- I did "touch .sagemathcloud/installed" and it started instantly.  So...

- [ ] snap servers usually stop working after a day or two --  they serve requests but just don't make new snapshots.  Since there are so many servers and they all do the same thing, this hasn't actually been a problem yet.  However, it is very dangerous.

- [ ] open image on handheld = non way to close it!

- [ ] better analytics for connect versus reconnect: https://mail.google.com/mail/u/0/?shva=1#search/from%3Aharald/14178e870f62ceac

- [ ] sagetex bug -- it doesn't automatically re-run if input changes...: https://mail.google.com/mail/ca/u/0/#inbox/1418019f0fb715a1

- [ ] SMC in SMC/cassandra -- strangely, in Cassandra, it seems that nodetool doesn't require a username/password, even after auth is enabled.  Maybe upgrading cassandra will help.  WEIRD.; This is not much of a security issues since we firewall cassandra.  But still, it is not good either!



# DONE:


- [x] setup so harald can do dev.



- [x] **CRITICAL** raw file download/browsing seems broken now.  CRAP.  Probably the result of an nginx build/config issue.  Fixing this is top priority.



- [x] next release:
    x- apt-get upgrade
    x- new deb packages: haskell-platform haskell-platform-doc haskell-platform-prof  mono-devel mono-tools-devel ocaml ocaml-doc tuareg-mode ocaml-mode libgdbm-dev mlton sshfs sparkleshare dropbox
    x- install SML (requested on google+)
    x- install ocaml
    x- install some haskell thing that's maybe mentioned on github issues (??): haskell-platform
    x - make it so on startup the ssh keys in the /home/salvus/ directory are deleted, since they aren't needed on the compute
        machine, and are dangerous, since they provide access to other things...
    x - rebuild nginx on base machine, and ensure that "-V" doesn't show rewrite module missing!
    x - manually login to each compute node and change salvus account password, since
        that password is stored in the external image for each compute machine...
    x- update vm again; push out
    x- update repos on cloud1-4
    x- update services files in cloud/ and storm/

    x- restart storm system
    x- quickly paste in code to add slavus/hub/snap users: see file box/DANGER.txt in laptop
    x- if stuff doesn't just start working:
          storm.restart('hub'); storm.restart('snap'); storm.restart('haproxy')
    x- test storm very hard due to all the `base_url` stuff -- make sure that doesn't break stuff.

    x- update main cloud same way as above.


- NEXT goal: make it so I can 100% do cloud development in cloud, i.e., make it so my "cloud.sagemath" project
  is a complete running copy of cloud, using some port forward.  Why:
      x- snapshotting
      x- aleviate my limited-memory laptop issues
      x- collab.

    - [x] Ports -- make it possible to customize all of these via services file, in one line at top
        HAPROXY_PORT = 8000
        NGINX_PORT   = 8080
        HUB_PORT       = 5000
        HUB_PROXY_PORT = 5001
        CASSANDRA_CLIENT_PORT = 9160
        CASSANDRA_INTERNODE_PORTS = [7000, 7001]



    - [x] cassandra is "wide open" -- must implement auth; first do each of these in my devel db,
          then for laptop, then storm, then cloud during next release:

          1. Add this on the [cassandra] line of services (in all of the services files):
   'authenticator':'org.apache.cassandra.auth.PasswordAuthenticator', 'authorizer':'org.apache.cassandra.auth.CassandraAuthorizer'

           - increase replication level of system_auth keyspace (only for cloud and storm) -- run "nodetool repair" on all nodes
             after doing this (done on cloud and storm, and nodetool repair started):

                ALTER KEYSPACE system_auth WITH REPLICATION = {'class' : 'NetworkTopologyStrategy', 'DC0' : 2, 'DC1' : 2};

           - create 'salvus' superuser, and 'hub', 'snap' users:
           - remove cassandra superuser:
           - grant rights to 'hub' and 'snap' users...

                CREATE USER salvus WITH PASSWORD '<random 16 characters>' SUPERUSER;
                CREATE USER hub WITH PASSWORD '<random 16 characters>';
                CREATE USER snap WITH PASSWORD '<random 16 characters>';
                DROP USER cassandra
                GRANT ALL ON KEYSPACE salvus TO hub;
                GRANT ALL ON KEYSPACE salvus to snap;   /* make more restrictive later once this is done and working */

                cqlsh:salvus> list users;
                 name   | super
                --------+-------
                 salvus |  True
                   snap | False
                    hub | False


           - make 'hub' use its user
           - make 'snap' use its user

       x- Make sure clear docs in cassandra.py about exactly how to set these things up.
       x- Make passwords random 16-character strings in secrets/ directory, and NOT in github.
       x- Make one set for local dev, and a different set for deployed cloud.
