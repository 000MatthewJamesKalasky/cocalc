# Goals for the Thanksgiving weekend: Nov 29,30,Dec 1 --

--> - [ ] (6:00? started at 6am...) deploy on Google Compute Engine
    - [x] create debian 7 image
    - [x] put it on the tinc vpn: 10.3.x.x for google hosts
           10.3.1.1
           10.3.2.1
    - [x] do all the steps in the build guide.
            fails: julia (only!)

    Start with this minimal redundant setup, then double/triple as time elapses, or add data center:

        - [ ] 4-node cassandra site with replication factor of 3:
                   ...  n1-standard-1 instances (1 core, 3.75GB RAM)     $0.115/hour/machine (4 machines) = $331.20/month
                   cassandra1g   10.4.1.2
                   cassandra2g   10.4.2.2
                   cassandra3g   10.4.3.2
                   cassandra4g   10.4.4.2
        - [ ] 2-node web serving (hub, haproxy, stunnel, nginx, snap) nodes:
                   ....  n1-standard-1 instances (1 core, 3.75GB RAM)     $0.114/hour/machine = $166/month
                   each has a public ip address
                   web1g         10.4.1.3
                   web2g         10.4.2.3
        - [ ] Disk (snapshots, database images; etc.) at $0.085/GB = $272/month

              All these will use LVM:  sudo apt-get install lvm2
              Will grow later.

                   cassandra1g1  100GB
                   cassandra2g1  100GB
                   cassandra3g1  100GB
                   cassandra4g1  100GB
                   compute1g1    400GB
                   compute2g1    400GB
                   snap1g1       1000GB
                   snap2g1       1000GB

        - [ ] 2 4-core compute vm:
                   2 .... n1-standard-4-d instances (4 cores, 15GB RAM, diskless)  $0.461/hour/machine = $664/month (2 machines)
                   compute1g     10.4.1.4
                   compute2g     10.4.2.4

        Total cost: about $1200/month

    Add the public web server ip's to godaddy dns.



- [ ] (0:30?) make "go to line" dialog faster.
- [ ] (2:00?) fix some unicode file issues:
      - https://mail.google.com/mail/u/0/?shva=1#drafts/1429fc21daa100bff
      - https://cloud.sagemath.com/projects/182355db-6edd-4e4d-a821-7906cdc708eb/files/
- [ ] (5:00?) make project move "from last working" snapshot work robustly
    - [ ] consider changing usernames to be closely related to uuid
    - [ ] (1:00?) change the create_unix_user.py (sudo) command to take an account name as input.
    - [ ] (1:00?) maintain username on project move
    - [ ] (1:00?) better ui feedback
    - [ ] (1:00?) hub: needs to react to project move very quickly
- [ ] (1:00?) projects -- table that lists the timestamp of the last successful snapshot -- make sure snap user doesn't have to edit project table; show last snapshot in settings.
- [ ] (2:00?) limit user cpu using cgroups (see redhat guide)
- [ ] (2:00?) limit user memory using cgroups
- [ ] (2:00?) limit user disk io using cgroups
- [ ] (2:00?) disk quotas -- bring them back
- [ ] (1:00?) add file attachment to worksheet printing -- http://tex.stackexchange.com/questions/94811/attaching-file-into-a-pdf-with-pdflatex-will-crash-adobe-reader
- [ ] (0:30?) worksheet printing -- command line option to leave files around.
- [ ] (1:00?) fix json worksheet printing issue that Harald pointed out with his theano example
- [ ] (1:00?) bug in parsing try/except/else -- https://mail.google.com/mail/u/0/?shva=1#starred/1428eb398a87ed4e
        try:
            print('try')
        except:
            print('except')
        else:
            print('else')
- [ ] (1:00?) bug in parser with non-indented comments
- [ ] (1:30?) bup snapshotting issue when sshfs is stale.
   ',3702601d-9fbc-4e4e-b7ab-c10a79e34d3b,command '/usr/bin/bup' (args=on teaAuZ9M@10.1.2.4 index --one-file-system .) exited with nonzero code 1 -- stderr='/mnt/home/teaAuZ9M/lxc: [Errno 107] Transport endp
oint is not connected: 'lxc'
- [ ] (2:00?) print for other document types (use lstlisting or...?)













----

- [ ] rebooting host machine then trying to restart the vm's can't work, since they already exist.  It should... do something sensible.

- [ ] need to actually use the version number output by status for local_hub's... so that we never need to force reboot the vm's.

- [ ] ijulia: https://github.com/JuliaLang/IJulia.jl; [requested by Jiahao Chen on G+]



- [ ] (???) should I do this?   project replication: define and implement strategy:
          - [ ] must have ability to directly sync files between vm's (change firewall...):
                  maybe use lsyncd (?) -- https://github.com/axkibe/lsyncd/
                   and https://www.digitalocean.com/community/articles/how-to-mirror-local-and-remote-directories-on-a-vps-with-lsyncd
                  csync2 is also pretty neat... but not right for this app.  Lsyncd looks perfect.
                  I could have the localhub startup just start lsyncd daemons running with one target
                  for each data center.   This way lsyncd'ing happens precisely when the project is
                  running, and not otherwise.
                  Example:
                         lsyncd -nodaemon -rsyncssh tmp/ 10.1.1.3 tmp/
          - [ ] must store locations for replication in the database:
                  - field in the projects table
                  - part of the info.json file, since is needed by the lsyncd daemons.
          - [ ] failover strategy:
                  - button next to "restart project server" which is "move to another host"
                  - if project server won't start for k seconds, user gets dialog with button to "move to another host"
                  - move data center should also stop using the current master and switch to a new master in the
                    current data center. Then mark account for deletion in some table...

       Drawbacks:
          - it means MUCH more disk usage (double) -- right now we're using 2TB; with this it instantly goes to 4TB.
          - it might mean that I don't ensure snaps work "perfectly"; but that is a bogus argument.
          - it means increased network usage for replication, beyond the existing replication with bup
              (that said, I can change bup to only snapshot the replica that is in the same data center)
          -


=======


 ---

TESTING with "sage-support" project:

{"host":"10.1.4.4","username":"KyVNuXnb","port":22,"path":"."}

update projects set location='{"host":"10.10.10.4","username":"project","port":22,"path":"."}' where project_id=4a5f0542-5873-4eed-a85c-a18c706e8bcd;

WHAT do I *really* need?

   - each project to be stored in a big easy to deal with distributed "something".
   - replication: at least 2 copies of a each project, with one at most 1 minute (say) behind the other, with a replication *strategy/policy*
   - ability to "enhance" a project file store with *compute*, i.e. start an LXC container with the project files bound to it, efficiently.

New plan:

   - setup an ubuntu 13.10 KVM with lxc and have it mount an lvm volume from the *host* that contains projects.



- QUESTION: should I have all the lxc stuff run in a big kvm instance?

To make a container approach work for projects, I would need:

   - a new Ubuntu 13.10 kvm image

   sudo vmbuilder kvm ubuntu --rootsize=128000 --raw=/home/salvus/vm/images/base2/2013-11-16-1200.img --swapsize=0 --iso=/home/salvus/iso/ubuntu-13.10-server-amd64.iso --user=salvus --name=Salvus  --mem=8192 --cpus=4 --suite=saucy

   sudo vmbuilder kvm ubuntu --rootsize=128000 --raw=/tmp/a.img --swapsize=0 --iso=/home/salvus/iso/ubuntu-13.10-server-amd64.iso --user=salvus --name=Salvus  --mem=8192 --cpus=4 --suite=saucy

    sudo vmbuilder kvm ubuntu --rootsize=128000 --raw=/home/salvus/vm/images/base2/2013-11-16-1200.img --swapsize=0 --user=salvus --name=Salvus  --mem=8192 --cpus=4 --suite=saucy

Even with basically no options, the above just doesn't work!


virt-install approach


qemu-img create -f qcow2 /home/salvus/vm/images/base2/2013-11-16-1214.img 64G

virt-install --cpu host --network user,model=virtio --name 2013-11-16-1214 --vcpus=12 --ram 8192 --cdrom ~/iso/ubuntu-13.10-server-amd64.iso --disk ~/vm/images/base2/2013-11-16-1214.img,device=disk,bus=virtio,format=qcow2,cache=writeback --graphics vnc,port=12102

ssh -L 12102:localhost:12102 salvus@cloud1.math.washington.edu

- a base Ubuntu 13.10 root filesystem, with Sage, etc. installed
   - script to easily start an LXC-containers that uses that root filesystem, has its own /home/smc/, and its own tinc network address; also, way to specify the limits on that container.



    lxc-create -t ubuntu --r saucy -b salvus



Idea:

   Run *everything*: cassandra, web, snap, all in a *single* KVM image, but with several different lxc containers to provide resource isolation and security, and also several different mounted img files.

   If I want to move everything to another host -- even bare metal -- it's just an rsync of the base.


    lxc-start-ephemeral --orig base  --bdir /home/salvus/projects/4a5f0542-5873-4eed-a85c-a18c706e8bcd/ --user smc

Disabled aaparmor to test this... set in /var/lib/lxc/smc/config :

        lxc.aa_profile = unconfined

    mount -o uid=1001,gid=1001 --bind /home/salvus/projects/4a5f0542-5873-4eed-a85c-a18c706e8bcd/ /home/smc/


NO good.


    lxc-clone -s -B overlayfs -o smc -n project


IDEA -- what about having the projects and snaps in a folder on the *host*, but share that with the kvm guest?


virt-install --cpu host --network user,model=virtio --name 2013-11-16-1214 --vcpus=12 --ram 8192 --cdrom ~/iso/ubuntu-13.10-server-amd64.iso --disk ~/vm/images/base2/2013-11-16-1214.img,device=disk,bus=virtio,format=qcow2,cache=writeback --graphics vnc,port=12102

   -fsdev local,security_model=passthrough,id=fsdev0,path=/tmp/share -device virtio-9p-pci,id=fs0,fsdev=fsdev0,mount_tag=hostshare

---


I use the GUI to add this to the XML...

    <filesystem type='mount' accessmode='squash'>
      <source dir='/home/salvus/data/projects'/>
      <target dir='/projects'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/>
    </filesystem>

And this mount in the guest worked:

    mount -t 9p -o trans=virtio /projects /projects -oversion=9p2000.L,posixacl,cache=loose


MAJOR surprise -- this is really, really slow compared to just using a local filesystem.

OK, so much for that idea...

I can't see any advantage to directly mounting an LVM volume, except that it is easy to make bigger... and maybe *it* would be faster.

OK, I'm going to just make an ubuntu 13.10 VM that can host a bunch of containers, and have containers for each of these, which
get bind mounted, much as right now (except with projects):

  - base
    - cassandra       --  /mnt/cassandra
    - snap            --  /mnt/snap
    - project         --  /mnt/projects/project-id/
    - web (hub/nginx/)--  /home/salvus


 lxc-clone -s -B overlayfs -o base -n cassandra
 lxc-clone -s -B overlayfs -o base -n snap
 lxc-clone -s -B overlayfs -o base -n web
 lxc-clone -s -B overlayfs -o base -n project

In each case, we have to start the machine, make the directory that will be bound to, then stop.


- [x] put the host lxc machine on the tinc vpn:
            10.1.n.5  <-- on cloud-n

- [ ] put each of the above machines on tinc vpn easily somehow, via the host, starting with:
      - [ ] web
      - [ ] cassandra
      - [ ] snap




- [ ] seen just now in a local hub log:
debug: received control mesg {"event":"project_exec","project_id":"c88745bc-89ac-42c8-967e-0578ef27d929","path":"/","command":"rm","args":["-rf","/tmp/undefined"],"timeout":30,"bash":false,"err_on_exit":true,"id":"ce89207a-3bfa-499e-8fd7-185b75238c3d"}
debug: Handling '{"event":"project_exec","project_id":"c88745bc-89ac-42c8-967e-0578ef27d929","path":"/","command":"rm","args":["-rf","/tmp/undefined"],"timeout":30,"bash":false,"err_on_exit":true,"id":"ce89207a-3bfa-499e-8fd7-185b75238c3d"}'
debug: project_exec
debug: execute_code: "rm -rf /tmp/undefined"
debug: Spawn the command rm with given args -rf,/tmp/undefine

---


- [ ] spin up project container with given address, bind mount, etc.

    - lxv.py --project=/path/to/project_id [options]
         * options = cgroup limitations
         * set database entry indicating that this host own the project now.
         * regularly check the database to see if some other host owns the project or project should term (e.g., a ttl record that must be refreshed); if so, shut down.
         * get newest version of project files, either from another host or newest available snapshot
         * start project lxc running locally
         * regularly replicate the project's files using rsync according to rules, storing info about replicas in the database

First version; make it a lot like vm.py, but using LXC.

    lxc.py start --project_id=project_id --project=/path/to/project/files
    lxc.py start --project_id=project_id

will fail if /path/to/project/files does not exist.  If it does exist, it will:

     * generate a random ip address of the form 10.x.y.z, check database to see if used, and if not, set
       project location to smc@10.x.y.z.  Here 10<=x<64, 0<=y,z<256.  # so 3538944 possibilities.
     * create an lxc container with /mnt/projects/project_id bound to /home/smc/
     * make that container have the given ip address on the tinc vpn.

And that is it.

Then to migrate from my current thing, I just have to rsync out all the projects to these new /mnt/projects/project_id qcow2 images,
compress them, do the rsync again.




---



- [ ] printing sage worksheets: minimal UI

- [ ] delete historical qcow2 snapshots everywhere except in one archive (?) -- older than salvus-2013-11-18-0800.img can go



- [ ] rsync out to the other machines and add vm.py option to run it.



- [ ] ipython still not working --> change it completely to use a python script that gets all relevant info, then just start pulling the correct page until it works.

- [ ] (0:45?) terminal: have a special code to "start" the terminal on first connection, since "first burst" just doesn't work.  See the line " On first write we ignore any queued terminal attributes responses that result." in console.coffee

- [ ] (1:00?) code folding: https://github.com/sagemath/sagecell/commit/529bbe8c98737287f973a736d09b2bc5b62bd06a


# Major bugs

  - [ ] f'd up sync when editing documents -- this is a direct result of something going wrong with the HUB...  I had this problem, and restarting my hub, and suddenly things work fine again.   Conclusion: ?


  - [ ] go_backups needs a timeout

# Misc


- [ ] cremona reports (https://mail.google.com/mail/?fs=1&source=atom#all/1427b8cccce76080)
        > 1. I cannot copy and paste inside Sage code cells.  I highlight a
        > whole line (say) but right-clicking on the mouse does not show the
        > "Copy" option.
        I can replicate this on firefox (right click to copy works fine for me on Chrome).  You should at least find that control+c works to copy.

- [ ] cremona reports: random indentation issues (https://mail.google.com/mail/?fs=1&source=atom#all/1427b8cccce76080)


- [ ] (2:00?) opening read only files gets ugly on the server (and client) side:

    debug: CodeMirrorSession: _connect to file '/usr/local/sage/sage-5.12/local/lib/python2.7/site-packages/sage/misc/.mrange.py.sage-chat' on a local hub (time=1384797849.
    376)
    debug: local_hub --> hub: (connect) error -- true, {"errno":3,"code":"EACCES","path":"/usr/local/sage/sage-5.12/local/lib/python2.7/site-packages/sage/misc/.mrange.py.s
    age-chat"}, trying to connect to '/usr/local/sage/sage-5.12/local/lib/python2.7/site-packages/sage/misc/.mrange.py.sage-chat' in 4a5f0542-5873-4eed-a85c-a18c706e8bcd.
    debug: CodeMirrorSession (path=/usr/local/sage/sage-5.12/local/lib/python2.7/site-packages/sage/misc/.mrange.py.sage-chat) -- connect -- FAIL (time=26.411999940872192 s
    econds) -- true


- [ ] (0:30?) make it so on startup snap runs snap_fix_master.py on all repos.

- [ ] (0:45?) make it so snap_fix_master.py repeatedly moves back the head until the repo works, rather than just trying once.

- [ ] (0:15?) should I make snap servers less aggressive, e.g., each always waits 30 seconds before making another snapshot?

- [ ] (1:00?) worksheet: simple first "export to pdf" -- write python script: https://mail.google.com/mail/u/0/?shva=1#inbox/1424dfa44323d4aa

- [ ] (0:45?) latex pdf preview: need a status/progress indicator.

- [ ] (0:45?) dropbox faq entry/tutorial


- [ ] figure out how used before implementing!   database logging: in projects
          file_access_log  list<uuid>
      which will be a list of json objects [filename,username,account_id,timestamp].
      When prepending to list sometimes check size, and if it exceeds some number (k*1.3?)
      then rewrite the list with only the first k entries.

- [ ] figure out how used before implementing!   database logging: in accounts
          file_access_log  list<uuid>
      which will be a list of json objects [project_id, filename].
      This could be shown with links as a well at the top of the "Projects" page, labeled
      "Recent".   *OR* in the project listing... not sure.


# Containers (!?)

 - We really need checkpointing and maybe live migration of containers.
 - openvz containers?! http://openvz.org/Main_Page
 - live migration of lxc?!- lxc + criu http://criu.org/Main_Page

 - EXERCISE: create an LXC-in-a-vm-on-my-laptop-based project on SMC.

1. Create an LXC container based project in a vm on my laptop that is linked to SMC

    [ ] create an LXC-based container:

           $ apt-get install lxc tinc
           /etc/default$ diff lxc-net.orig lxc-net

            16c16
            < LXC_ADDR="10.0.3.1"
            ---
            > LXC_ADDR="172.16.3.1"
            18,19c18,19
            < LXC_NETWORK="10.0.3.0/24"
            < LXC_DHCP_RANGE="10.0.3.2,10.0.3.254"
            ---
            > LXC_NETWORK="172.16.3.0/24"
            > LXC_DHCP_RANGE="172.16.3.2,172.16.3.254"

            i.e., make it so the network block is

                LXC_ADDR="172.16.3.1"
                LXC_NETMASK="255.255.255.0"
                LXC_NETWORK="172.16.3.0/24"
                LXC_DHCP_RANGE="172.16.3.2,172.16.3.254"

           I have to reboot after this change.


           # time lxc-create -t ubuntu -n project_test
           # ssh ubuntu@10.0.3.198
           ubuntu@project_test:~$ sudo su
           root@project_test:/home/ubuntu# apt-get install tinc

    [ ] make tinc vpn creds for this ip: 10.3.1.2

            root@project_test:/etc/tinc# more /etc/tinc/tinc-up

            #!/bin/sh
            ifconfig $INTERFACE 10.3.1.2 netmask 255.192.0.0

            root@project_test:/etc/tinc# more tinc.conf
            Name = project_test
            ConnectTo = cloud1

            root@project_test:/etc/tinc# mkdir hosts
            root@project_test:/etc/tinc# vi hosts/cloud1
            root@project_test:/etc/tinc# more hosts/cloud1
            Address = 128.95.242.135
            Subnet = 10.3.1.2/32
            -----BEGIN RSA PUBLIC KEY-----
            MIIBCgKCAQEArxrDr6qoIugjtTE664gvZKRsV4r7oGfZzTAWbq88E16VRRQELQR4
            OZ3HwezGc23ql/bpo1MU/cme+O4+3a2TvzVn6sMBgFuXnZiQSw3mbJB00JsAKyaM
            Pc2cM5ahYEDiSsII26y5NBOhoD97w5LtzMK46Nn+MsT15nncUn4NAPl/BMvQsO5I
            IvESIGR2lVDo/Msw++IRhrFMrgLLQxTcarvuAVCZjZd7WL5yhPLop8sF1FB7STDd
            7Qt4/MuWcLwLwNkpRP3oVQGW4fZJOn7T44MwTq2oCTIx4WA1qQt2xW2SHTIqYC7t
            Wqfi+9G4xuZ4mStFR9jYS58NHvz5Wnxe8QIDAQAB
            -----END RSA PUBLIC KEY-----

            root@project_test:/etc/tinc# tincd -K
            Generating 2048 bits keys:
            ..................................................+++ p
            .....................................................+++ q
            Done.
            Please enter a file to save private RSA key to [/etc/tinc/rsa_key.priv]:
            Please enter a file to save public RSA key to [/etc/tinc/hosts/project_test]:

            root@project_test:/etc/tinc# vi hosts/project_test  # make the following the first line of the file
            Subnet = 10.1.1.3/32

            On cloud1:

            salvus@cloud1:~/salvus/salvus/data/local/etc/tinc/hosts$ vi project_test

            Back on project_test:
            root@project_test:/etc/tinc# tincd -D --config=/etc/tinc
            tincd 1.0.16 (Jul 27 2011 12:56:56) starting, debug level 0
            Could not open /dev/net/tun: No such file or directory
            Terminating

root@project_test:/etc/tinc# mkdir /dev/net
root@project_test:/etc/tinc# mknod /dev/net/tun c 10 200



    [ ] put tinc creds in and get container to be on the network
    [ ] copy over ~/.sagemathcloud template
    [ ] make a project that has location set to username@10.3.1.2



- [ ] (0:30?) restore: store all past locations
- [ ] (1:00?) restore: change to use same account name, if possible.
- [ ] (1:30?) restore: choose new machine with lowest load that is up.
- [ ] (1:00?) restore: if a snapshot fails for some reason, try next older snapshot until success.  Give progress message.
- [ ] (2:00?) restore: status/progress messages to client



-----

---

misc

- [ ] ?? the dns issue -- ideas from keith, namely to not use crappy SLIRP.

- [ ] better search and replace?   open source code here -- https://codio.com/s/blog/2013/11/search-replace/


- [ ] SMC looks like crap on IE 9...

- [ ] case sensitive email -- https://mail.google.com/mail/u/0/?shva=1#inbox/1424c97515d65ee0

- [ ] (2:30?) notion of "admin" accounts on SMC with access to system monitor data, and page to show it.


---
- [ ] (0:30?) block outgoing email: (Keith Clawson)
            iptables -A OUTPUT -p tcp -m tcp --dport 25 -j REJECT --reject-with icmp-port-unreachable
- [ ] (0:45?) make it so *old* sws's can be converted: https://mail.google.com/mail/u/0/?shva=1#inbox/142393e15d4a9784
- [ ] (0:45?) harald's number of projects query: https://mail.google.com/mail/u/0/?shva=1#inbox/14238c307ccee599
- [ ] (0:45?) redo base VM: current one not working (?)




## Impose limitations

 - [ ] use cgroups to limit memory of each project
 - [ ] use cgroups to limit cpu of each project
 - [ ] quotas again - say 10gb?
 - [ ] limit number of projects per user (but make big -- just to avoid crazy cases).
 - [ ] move the /mnt/snap backup scripts into salvus/salvus/scripts/
 - [ ] upgraded to three.js r62 -- https://github.com/sagemath/sagecell/commit/10e1fa9d2141cefe7886f4e50b4654160d6e877e
 - [ ] make password reset use different javascript/html that is less subject to issues...
 - [ ] (0:45?) fix storm cassandra database


# Startup optimization:
 - [ ] squash base vm: I think this is making startup very slow, esp in parallel -- it adds a solid 12 seconds?
 - [ ] change the tinc/vpn info to be on a separate tiny .img that can be created instantly.


# Other

- interact challenge: https://mail.google.com/mail/u/0/?shva=1#inbox/14253949c1899e87, https://cloud.sagemath.com/projects/4a5f0542-5873-4eed-a85c-a18c706e8bcd/files/vector_field.sagews







done
====================


- [x] CPU lock maybe this bug -- https://bugzilla.redhat.com/show_bug.cgi?id=879801:

    sync && echo 3 > /proc/sys/vm/drop_caches

    #echo 0 > /sys/kernel/mm/transparent_hugepage/khugepaged/defrag
    #echo never > /sys/kernel/mm/transparent_hugepage/defrag
    echo never > /sys/kernel/mm/transparent_hugepage/enabled

I've done this on all hosts:

    echo never > /sys/kernel/mm/transparent_hugepage/enabled

and in crontab:

    @reboot echo never | tee /sys/kernel/mm/transparent_hugepage/defrag  # in cron


- [x] add a GCE node -- turn off in the morning?  This is $6/day...
Created a single 64GB image in us east with debian
    apt-get install tinc

    fdisk /dev/sda
    # create new primary partition, write it...
    partprobe
    mknod /dev/sda2 b 8 2
    mkfs.ext4 /dev/sda2
    # add this line to /etc/fstab
    dev/sda2 /usr/local/ ext4 defaults 1 1

Configure tinc:

sshd: in /etc/ssh/sshd_config; "PasswordAuthentication yes".


# Snap: idea for optimization
      - [x] (0:45?) (0:05) restore: test to see if restoring via sshfs is better than restoring locally, then using rsync -- no idea.

              restoring via sshfs (1) does not work at all, and (2) doing it locally, then rsync'ing via sshfs is vastly slower than just rsyncing directly.


- [x] (0:20) add a reconnect button to connections dialog.

- [x] (0:20?) (0:23) add command like this to scripts to ssh into vm if vpn not working
           virsh -c qemu:///session qemu-monitor-command --hmp <filename> 'hostfwd_add ::2222-:22'

- [x] (0:25?) (0:23) prominent http://ask.sagemath.org/questions/ link and http://boxen.math.washington.edu/home/schilly/salvus/stats/stats.html

- [x] fix bug in salvus.file [reported by Ширшов Андрей] -- https://mail.google.com/mail/u/0/?shva=1#inbox/1425539872c4bc5f

 - [x] (0:29) zoom on firefox (etc.) in latex editor is broken [reported by Pedro A. Tonelli] -- https://mail.google.com/mail/u/0/?shva=1#inbox/14245c94ae0fba49
 - [x] (0:18) admin: push out update (fixing zoom issue):
             pull_from_devel_repo on all web machines and update version
             and restart nginx then hub on all machines in serial (not parallel):
                 import time; ver = int(time.time())
                 cloud._hosts('hub', 'cd salvus/salvus; . salvus-env; ./pull_from_dev_project; echo "exports.version=%s" > node_modules/salvus_version.js; ./make_coffee --all'%ver, parallel=True, timeout=30)
                 cloud.restart('nginx'); cloud.restart('hub')



- [x] (0:05?) add keith to GCE account
- [x] (0:15?) (1:15) harald animation embed thing: https://mail.google.com/mail/u/0/?shva=1#search/harald+animation/142351aee45ed9b7
- [x] (1:00?) SMC email
- [x] (0:15?) (0:10) monitoring: write minimal functionality needed for the "move button" -- i.e., in admin.py make the monitor put the last entry into another table too:
    select day, hour, minute, compute from monitor_last where dummy=true;


 - [x] I need a new backups strategy:
        2.9T  2.7T   73G  98% /home/salvus



# Options for final HA solution

## ceph

## cgroups

## lxc




---

# TODO:

- [ ] CGROUPS?  Goal -- limit cpu usage of specific projects...?

    http://tuxion.com/2009/10/13/ubuntu-resource-managment-simple-example.html

Crucial: All the examples online are missing the "-t" option, but is *crucial* in modern ubuntu.

    root@ubuntu:/home/wstein# cgcreate -t wstein:wstein -a wstein:wstein -g memory,cpu:wstein3

- [ ] (2:00?) snap: many repos are corrupt, due to master pointing to something that isn't in repo.
        these can all (?) be repaired by simply pointing master to the previous id as listed in logs/HEAD.
        I must systematically run such repair on all the bup repos, since without this, the corresponding
        repos are completely useless.

- [ ] (1:00?) fix image load database issue: https://github.com/sagemath/cloud/issues/61

- [ ] (0:45?) SMC: message if you're using old browser: IE <= 8, etc. -- https://github.com/sagemath/cloud/issues/59
- [ ] (2:00?) SMC: mathematica in the cloud -- fix bug in sage (?) -- http://trac.sagemath.org/ticket/13892

- [ ] containers:

# LXC: <https://help.ubuntu.com/13.10/serverguide/lxc.html>

    # first try
    time sudo lxc-create -t ubuntu -n test1

    # bind wstein
    time sudo lxc-create -t ubuntu -n test3 -- -b wstein

# Google LMCTFY: <https://github.com/google/lmctfy>

    time sudo lxc-create -t ubuntu-cloud -n test4 -- -b wstein

# Google LMCTFY: <https://github.com/google/lmctfy>

# Google LMCTFY: <https://github.com/google/lmctfy>

- [ ] (0:15?) worksheet load spinner doesn't spin
- [ ] (1:30?) if document to open is large, provide options.
- [ ] (0:45?) block parser -- https://github.com/sagemath/cloud/issues/46





---

 - [x] broken blob getting from db.... (image loading)


- [x] ipython hangs.

Tried ipython 2 (tip from github) -- no better at the hang problem.  However, the new support for arbitrary directories and getting rid of id's is awesome.  I really must switch to this, since it will mean only one ipython across all, which saves some time.

Trying to just reconnect the websocket periodically.

    f = window.frames[$("iframe")[0].id]
    setInterval(function(){f.IPython.notebook.kernel.start_channels()}, 10000)




cd salvus/salvus/; sleep $(($RANDOM%5)); ./pull_from_dev_project; . salvus-env; ./make_coffee --all

DONE:

[x] new release on Nov 1:


   - open: There is a new open command, like in OS X.  Just type "open file1 file2 ..." in a full terminal to pop open those files in the editor; this is very preliminary, but may be useful.  (Note that it does not work under tmux yet, and is a little flaky.)

   - OS X friendly terminal changes:
          -- let browser have the command (=meta) key on OS X. (requested by Aron Ahmadia)
          -- make it so "Control+c" works even when text is selected on mac.  (requested by Aron Ahmadia)

   - Refresh buttons: add them to the project list and server stats (on help page).

   - Cassandra database: now uses Java 7 (instead of 6)

   - Snapshots: rewrote snapshot server code to be much more robust; also snapshot do not cross filesystem boundaries (needed to support sshfs remote mounting of filesystems)

   - HAProxy: increased a timeout parameter, which eliminates a bunch of 504 errors, which were causing sporadic trouble with ipython, file download, proxied sessions, etc.

   - IPython sync: numerous improvements and bug fixes related to startup, sync, etc.;  It might be usable again now.

   - Rewrote how most javascript/html gets loaded on upgrades (with a different filename), to decrease issues with browser caching.

   - Fix a leak that would result in a file becoming inaccessible if it is opened too many times (requiring a project server restart).

   - Upgrade to Codemirror Version 3.19

---


 - [x] code to automatically fix snapshot archives that loose the HEAD ref
 - [x] simple python script that goes through and fixes all existing archives:
                active broken on 7 and 12
 - [x] snap: remove non-fixable irrelevant repos:
           11,14,15,16
 - [x] (0:27) snap: if a repo exceeds 30GB, then make a new one. -- that seems like a good rule. -- and roll this out!
 - [x] upgrade host machines


--


 - [x] snap: fix so that repos don't break again (if indeed they ever do again -- wait for evidence)
      web5 and web12 broken last night
      x web13 is OLD?
           fatal: Unable to create '/mnt/snap/snap0/bup/ffeb32c5-1447-4e29-9857-305bde123fb1/refs/heads/master.lock': File exists.
           Problem was caused because refs/heads/master.lock didn't get removed as part of a rollback, since I didn't code that up!
      - [x] (0:15?) (0:36) snap: remove refs/heads/master.lock if it is there
      - [x] (0:45?) (0:45) snap: rollback -- check that bup ls works, and if not, rollback to the *previous* logged refs/heads/master, then check, etc.
      - [x] (0:20?) (0:45) store UTC time in commit database entries.

  alter table snap_commits add utc_seconds_epoch int;

 - [x] (0:05) snap: only broken repo left (?) -- fix or delete from db
      web5 38bcdad6-4c29-4185-a4bc-9292cdd7c795
 - [x] (0:24) get storm to start

 - [x] (0:16) fix storm database: -- it had replication 1, 1
         cqlsh> ALTER KEYSPACE "salvus" WITH REPLICATION = {'class' : 'NetworkTopologyStrategy', 'DC0':3, 'DC1':3};
         $ nodetool describering salvus>a   # confirms replication change

REMEMBER: setw synchronize-panes

 - [x] this account was running minerd: 3qHhHwPS@10.1.20.4 -- filip.stefanovski@aol.com, account_id=9bf8d35c-2569-4625-995a-6b83246213a0
 - [x] (0:10?) add "no bitcoin mining" to Terms of Usage
 - [x] (0:30?) add python function to get project id


- [x] database status monitoring:
     create user monitor with password '...';
     grant all on table monitor to monitor;
     grant all on table monitor_last to monitor;
 - [x] document my daily "status checks" on the SMC cluster:

          - [x] disk space on compute servers

ssh 10.1.1.4 "df -h /mnt/home"   # etc.
Filesystem      Size  Used Avail Use% Mounted on
/dev/vdb1      1008G  204G  754G  22% /mnt/home

          - [x] "uptime" on all project servers, web hosts, cassandra

salvus@cloud1:~$ ssh 10.1.1.4 "uptime"
06:38:41 up 8 days,  1:03,  0 users,  load average: 0.96, 0.76, 0.45

          - [x] dns check on compute nodes: host -v github.com

There is a script called "a" on the nodes which does this 30 times:

     host -v trac.sagemath.org >/dev/null && host -v www.sagemath.org >/dev/null; echo $?

It should complete with no errors in under 2 seconds.

          - [x] Make sure there all current snap repos are not corrupted:

# on each one, and this should take at most 2 seconds, and have size <= 35000000
ssh 10.1.11.3 'cd /mnt/snap/snap0/bup/`cat /mnt/snap/snap0/bup/active`&&time BUP_DIR=. bup ls && du -s .'

         - [x] nodetool status and make sure everything up

    # run on one node only!
    ssh 10.1.4.2 "cd salvus/salvus; . salvus-env; nodetool status"
    Datacenter: DC0
    ===============
    Status=Up/Down
    |/ State=Normal/Leaving/Joining/Moving
    --  Address    Load       Tokens  Owns   Host ID                               Rack
    UN  10.1.18.2  2.53 GB    256     6.1%   c1435b77-fe77-4964-916d-14cd36901da5  RAC0
    ...
    UN  10.1.20.2  338.5 MB   256     6.2%   d9020ad4-fd7e-4dc1-88cb-1119dda54b7a  RAC0
    Datacenter: DC1
    ===============
    Status=Up/Down
    |/ State=Normal/Leaving/Joining/Moving
    --  Address    Load       Tokens  Owns   Host ID                               Rack
    UN  10.1.4.2   896.16 MB  256     6.0%   9fb40b07-2e2f-43fe-b6dd-d0e0e7cced36  RAC1
    ...
    UN  10.1.7.2   7.54 GB    256     6.4%   a9425522-1c92-4ea2-96bd-f5fc16dbed9c  RAC1

          - good functional test query:  This should take a while, but *not* error out (e.g., saying "unavailable" and not saying "count").  It should take about 20-30 seconds...

                salvus@cloud1:~$ echo "consistency all; select count(*) from accounts;" | cqlsh_connect 10.1.4.2
                Consistency level set to ALL.
                 count
                -------
                  9789

          - disk space on backup server:

ssh disk.math.washington.edu "df -h /home/salvus"
Filesystem            Size  Used Avail Use% Mounted on
/dev/mapper/data-salvus
                      2.9T  2.4T  403G  86% /home/salvus


- [ ] NEW release
   - [x] modify /etc/and.priorities  # or whatever it is called
   - [x] apt-get install libav-tools
   - [x] sudo apt-get install nmon    # suggested by Harald Schilly
   - [x] add this to root's crontab:
               */1 * * * * killall minerd

# Nov 11: 10am - 10pm, except 1 hour for heating people and 1 hour for eating... so 10 hours.

    - [x] (1:00?) (9:00) re-fix base VM; test/debug on storm:
    - startup/restart of projects on storm is incredibly slow:
       - [x] 1. make sure that when (re-)starting a project server,
                no coffeescript needs to get compiled, by doing
                that when the hub starts (and also as part
                of the usual process)
       - [x] 2. instead of restart, do killall, rm the .port files, then just start.
       - [x] 3. benchmark why starting hubs so slow -- step by step
       - [x] don't watch file until successfully open the sage session connection, to avoid fs watch leak.
       - [x] issue -- missing coffee compiler in new project.
       - [x] issue -- shutting down all but one nginx servers... haproxy does *NOT* gracefully automatically fail over !
       - [x] make sure hub failover works well
       - [x] proxy failover -- that's not the ipython issue...
- [x] (0:10?) (0:10) upgrade hosts (seems safe)

- [x] (0:19) restart storm and make sure it works

- [x] (1:00?) (0:12) move tinc conf to a different image, so that vm's startup much, much more quickly:
         - [ ] do tests to make sure this is really a good idea
                    time guestfish -N fs:ext4:1M quit

                    # real    0m24.446s; but we could then always just instantly copy it (after the first time)

                    time qemu-img create -f qcow2 -b ~/vm/images/base/salvus-2013-11-11-1909.img test2.img
                    time qemu-img create -f qcow2 -b ~/vm/images/base/salvus-2013-05-19-1833.img test3.img

                    mkdir mnt1; time guestmount -a test1.img -m/dev/vda1 --rw mnt1

                    # real    0m14.742s    - WOAH!
                    # real    0m6.484s --

                    mkdir mnt2; time guestmount -a test2.img -m/dev/vda1 --rw mnt2

                    # real    0m7.487s

                    mkdir mnt3; time guestmount -a test3.img -m/dev/vda1 --rw mnt3

                    fusermount -u mnt1
                    fusermount -u mnt2
                    fusermount -u mnt3
            Hmm, so it seems this whole idea is without any merit!
            - [ ] make a small conf image file that is available as /dev/vdb
            - [ ] on bootup, copy info out of there about other disks, etc.

- [x] (0:20?) (0:51) make connection info display a modal on click instead of hover text

- [x] (1:00?) (0:43+) snap: restarting host of the snap server corrupts some pack files with high probability, breaking everything.  What happens is that there are empty pack files on resume, and for some reason the rollback file doesn't deal with this.  In all cases, removing the empty pack file and moving the head back one commit worked file.

    - I basically made one trivial change that probably won't make any difference.  Hmm...

- [x] (0:05) disable move again (?)

- [x] (1:00?) rewrite admin code that can do rolling restart
      - restart each stunnel, one at a time.
      - restart each haproxy, one at a time.
      - for each web machine, one at time.
           - stop snap
           - stop nginx
           - stop hub
           - restart the vm
           - start hub
           - start nginx
           - start snap
   (*)  - broadcast message to all clients "system maintenance -- The project servers are restarting and will be unavailable for up to 1 minute."...
     - restart all compute machine simultaneously (if projects moved then we would do differently)
     - for each cassandra machine, one a time:
           - stop cassandra
           - restart the vm
           - start cassandra

- [x] (0:45?) release a new version, but experiment with doing a rolling restart:


Or I can spend 10+ (?) hours and switch to using LXC, possibly not on a VM at all, and with different networking.


- [ ] create a simple ceph cluster (4 node) on my laptop, and try out cephfs:

Tried this, but it turns out ceph is already in saucy, and the below doesn't provide for saucy:

        ceph@ceph-deploy:~$ wget -q -O- 'https://ceph.com/git/?p=ceph.git;a=blob_plain;f=keys/release.asc' | sudo apt-key add -
        OK
        ceph@ceph-deploy:~$ echo deb http://ceph.com/debian/ $(lsb_release -sc) main | sudo tee /etc/apt/sources.list.d/ceph.list
        deb http://ceph.com/debian/ saucy main



And shit just doesn't work!?  Weird.
I'm going to try building the latest version from source...


    git clone --recursive https://github.com/ceph/ceph.git

    sudo apt-get install autotools-dev autoconf automake cdbs gcc g++ git libboost-dev libedit-dev libssl-dev libtool libfcgi libfcgi-dev libfuse-dev linux-kernel-headers libcrypto++-dev libcrypto++ libexpat1-dev uuid-dev libkeyutils-dev libgoogle-perftools-dev libatomic-ops-dev libaio-dev libgdata-common libgdata13 libsnappy-dev libleveldb-dev sudo apt-get install default-jdk javahelper junit4 libboost-program-options-dev libboost-thread-dev libboost-system-dev libcurl4-gnutls-dev libnss3-dev libxml2-dev python-nose yasm

... total fail -- due to internal compiler errors.

Getting it to work:

        sudo apt-get install ceph # on all nodes!

        ceph-deploy new ceph-node1
        ceph-deploy install ceph-node1 ceph-node2 ceph-node3
        # I had to manually do "sudo mkdir /etc/ceph" on all nodes.
        ceph-deploy mon create ceph-node1
        ceph-deploy gatherkeys ceph-node1

Do this on all nodes:

        sudo mkdir -p /var/lib/ceph/bootstrap-osd/
        sudo mkdir -p /var/lib/ceph/tmp

Then
        ceph-deploy osd prepare ceph-node2:/tmp/osd0 ceph-node3:/tmp/osd1
        ceph-deploy osd activate ceph-node2:/tmp/osd0 ceph-node3:/tmp/osd1
        ceph-deploy admin ceph-deploy ceph-node1 ceph-node2 ceph-node3


I'm giving up on 13.10 for ceph; time to try 12.04.

Ceph seems very complicated and hard to use.  Error prone. Difficult.

--

xtreemfs, on the other hand, seems pretty clean and clever.  Interesting.

It's looking like something like either lxc-clone or lxc-start-ephemeral on *Ubuntu 13.10* is good (12.04's LXC SUCKS).
One of these.

   lxc-clone -s -B overlayfs -o project_test -n project_test2
   lxc-start-ephemeral -b project_test -o project_test -u ubuntu -d

PLAN:

   - [ ] configure xtreemfs servers running on bare metal on all machines and firewall access so only
     the vpn machines mentioned below can connect.
        (see https://code.google.com/p/xtreemfs/wiki/ContrailSummerSchoolHandsOn2013)

     TEST THIS:
          - [x] set up xtreemfs on cloud1, 2, 3, 4, 5
          - [ ] do some benchmarking and robustness testing
                  CONCLUSION: woah, xtreemfs is completely useless!?  Truly terrible. Ugh.
                  HORRIBLE.  OMFG.  Even for a completely local install.

          - [ ] set up xtreemfs on cloud1-21.
          - [ ] configure a volume with replication of 4 (?) and data center topology awareness
          - [ ] set up firewall so that only vpn hosts and web vm's can mount, etc. -- get all the PKI for free via tinc
          - [ ] start syncing home directories into it.





   - [ ] make an ubuntu 13.10 VM host with lxc and xtreemfs-client installed, and not much else.
   - [ ] make a filesystem backed container with everything installed into it (described in build.py),
     so this is about 35-40GB, and has latex, sage, etc.  This might sit on a btrfs volume, so
     it is easy to use COW for new lxc containers, and also not loose any past releases.  This would be
     inside the ubuntu 13.10 vm.
   - [ ] there is a large xtreemfs volume with the data for all user projects in directiries, one for each
     project and.  all user access to project data happens in lxc clones like above
     with a particular path into that volume bound into the project.  (This will make it possible to
     make multiple project data available in the same container, when users want that.)
     Also, configure tinc for that container.
   - [ ] bup snapshotting change: snap can just directly mount the large xtreemfs volume and make snapshots
     directly, instead of having to use ssh to remote machines, etc.   Also, the bup snapshots could themselves
     be stored in a different xtreemfs volume, which would make them HA.
   - [ ] raw file server optimization: can be replaced by something that doesn't use lxc or projects at all -- it's
     just a distributed nginx or nodejs serving files directly from the xtreemfs volume, but respecting auth.
     This makes all project data available efficiently and easily at all times without having to spin up
     any containers, VM's, networks, etc.

CONCERNS:

   - [ ] lack of quotas means I'll have to have a service that simply scans and takes action...


- [x] email domains:
     echo "select email_address from accounts limit 12000;" | cqlsh_connect 10.1.3.2 > a
     open('b','w').write('\n'.join(sorted([x.split('@')[1].strip() for x in open('a').readlines() if len(x.split('@')) > 1])))

I can spend 5 hours on this:
    - [ ] (1:30?) VM startup -- make for every VM a *fixed* conf-?.img with the tinc vpn info, etc., so don't have to do that every time.  Do not remove hosts ssh file anymore from host.   This will greatly speedup startup, and make it so tinc doesn't have to constantly deal with vanishing hosts & new hosts.


- [x] rewrite hub to make direct connection instead of port forward?

NO -- I'm testing now and the problem is totally gone when the network is less loaded.
Making this change will make things way less secure and less *flexible*, and I have no proof that it is necessary.

I think what I should do is just push forward with moving to the LXC container, the new network, etc., ASAP.

What if the problem has nothing to do with the network?  What if something just causes the hub to block for
a while, and heavy sagews traffic makes this happen?

- [x] upgrade all kvm hosts: new libvirt packages.
   - [x] rsync out vm images
   - [x] update salvus repo on all hosts
   - [x] fix apparmor config as above on all hosts
   - [x] upgrade tinc on hosts
   - [x] restart tinc on hosts
   - [x] test to see if perms need to be changed on cloud21; if so change on all hosts
   - [x] restart storm from scratch to see what needs to be fixed; fix it.
   - [x] restart cloud using the gentle rolling restart of everything.



- [x] qcow2 compression?  -- a one-time deal (see http://www.noah.org/wiki/KVM#Compress_a_QCOW2_image)
      Basically, this takes a LONG time, but reduces the size of an significantly!

salvus@cloud1:~/vm/images/base2$ time kvm-img convert -c -f qcow2 -O qcow2 -o cluster_size=2M 2013-11-16-1214.img new_disk.qcow2

real    27m57.878s
user    20m5.015s
sys     0m31.146s
salvus@cloud1:~/vm/images/base2$ ls -lh
total 33G
-rw-r--r-- 1 salvus salvus  24G Nov 18 07:35 2013-11-16-1214.img
-rw-r--r-- 1 salvus salvus 9.7G Nov 18 07:16 new_disk.qcow2

Now, I'm trying this, which might both compress everything *and* rebase it all into one single file :-)

    salvus@cloud1:~/vm/images/base$ time kvm-img convert -c -f qcow2 -O qcow2 -o cluster_size=2M salvus-2013-11-16-1205.img COMPRESSED-2013-11-16-1205.qcow2

I could see using this somehow... but not sure how yet.




- [x] recompress the lxc base image, and restart with default network

       time kvm-img convert -c -f qcow2 -O qcow2 -o cluster_size=2M 2013-11-16-1214.img lxc-2013-11-19-1037.img
       # This took 33 minutes and compressed from 30GB to 10GB.

       qemu-img create -b lxc-2013-11-19-1037.img -f qcow2 lxc-2013-11-19-1706.img

       virt-install --connect qemu:///system --cpu host --network network:default,model=virtio --name lxc1 --vcpus=12 --ram 8192 --import --disk lxc-2013-11-19-1706.img,device=disk,bus=virtio,format=qcow2,cache=writeback --noautoconsole --graphics vnc,port=12101


       virt-install  --connect qemu:///system --cpu host --network network:default,model=virtio --name lxc1 --vcpus=12 --ram 8192 --import  --disk lxc-2013-11-19-1706.img,device=disk,bus=virtio,format=qcow2,cache=writeback --noautoconsole --graphics vnc,port=12101


[x] change all the kvm images to use qemu:///system and "default" network, across the system

data/local/bin/python vm.py --ip_address 10.1.1.2 --pidfile data/pids/vm-10.1.1.2.pid --logfile data/logs/vm-10.1.1.2.log --vcpus 2 --ram 8 --vnc 0 --vm_type kvm --base salvus-2013-11-18-0800 --disk cassandra:256 --hostname cassandra1

    qemu-img create -b /home/salvus/vm/images/base/salvus-2013-11-18-0800.img -f qcow2 temporary/cassandra1.img
    virt-install --connect qemu:///system --cpu host --network network:default,model=virtio --name cassandra1 --vcpus 2 --ram 8192 --import --disk /home/salvus/vm/images/temporary/cassandra1.img,device=disk,bus=virtio,format=qcow2,cache=writeback --noautoconsole --disk /home/salvus/vm/images/persistent/cassandra1-cassandra.img,bus=virtio,cache=writeback

    virt-install --connect qemu:///system --cpu host --network network:default,model=virtio --name cassandra1 --vcpus 2 --ram 8192 --import --disk /home/salvus/vm/images/temporary/cassandra1.img,device=disk,bus=virtio,format=qcow2,cache=writeback --noautoconsole --disk /home/salvus/vm/images/persistent/cassandra1-cassandra.img,bus=virtio,cache=writeback


virt-install --connect qemu:///system --cpu host --network network:default,model=virtio --name cassandra1 --vcpus 2 --ram 8192 --import --disk /home/salvus/vm/images/temporary/cassandra1.img,device=disk,bus=virtio,format=qcow2,cache=writeback --noautoconsole --disk /home/salvus/vm/images/persistent/cassandra1-cassandra.img,bus=virtio,cache=writeback

virt-install --connect qemu:///system --cpu host --network network:default,model=virtio --name cassandra1 --vcpus 2 --ram 8192 --import --disk /home/salvus/vm/images/temporary/cassandra1.img,device=disk,bus=virtio,format=qcow2,cache=writeback --noautoconsole

virt-install --connect qemu:///system --cpu host --network network:default,model=virtio --name cassandra1 --vcpus 2 --ram 8192 --import --disk /home/salvus/vm/images/temporary/cassandra1.img,device=disk,bus=virtio,format=qcow2,cache=writeback --noautoconsole --disk /home/salvus/vm/images/persistent/cassandra1-cassandra.img,bus=virtio,cache=writeback



qemu-img create -b /home/salvus/vm/images/base/salvus-2013-11-18-0800.img -f qcow2 /home/salvus/vm/images/temporary/c.img
chgrp kvm /home/salvus/vm/images/temporary/x.img; chmod g+rw /home/salvus/vm/images/temporary/x.img

virt-install --connect qemu:///system --cpu host --network network:default,model=virtio --name x --vcpus=12 --ram 8192 --import --disk /home/salvus/vm/images/base/x.img,device=disk,bus=virtio,format=qcow2,cache=writeback --noautoconsole --graphics vnc,port=12101

----

qemu-img create -b /home/salvus/vm/images/base/salvus-2013-11-20-0617.img -f qcow2 a.img; chgrp kvm a.img; chmod g+rw a.img
virt-install --connect qemu:///system --cpu host --network network:default,model=virtio --name a --vcpus 2 --ram 8192 --import --disk a.img,device=disk,bus=virtio,format=qcow2,cache=writeback --noautoconsole
virsh_sys destroy a; virsh_sys undefine a



qemu-img create -b /home/salvus/vm/images/base/salvus-2013-11-18-0800.img -f qcow2 /home/salvus/vm/images/base/salvus-2013-11-20-0610.img
chgrp kvm /home/salvus/vm/images/base/salvus-2013-11-20-0610.img; chmod g+rw /home/salvus/vm/images/base/salvus-2013-11-20-0610.img
virt-install --connect qemu:///system --cpu host --network network:default,model=virtio --name salvus-2013-11-20-0610 --vcpus=12 --ram 8192 --import --disk /home/salvus/vm/images/base/salvus-2013-11-20-0610.img,device=disk,bus=virtio,format=qcow2,cache=writeback --noautoconsole --graphics vnc,port=12101

virt-install --connect qemu:///system --cpu host --network network:default,model=virtio --name salvus-2013-11-20-0615 --vcpus=12 --ram 8192 --import --disk /home/salvus/vm/images/base/salvus-2013-11-20-0615.img,device=disk,bus=virtio,format=qcow2,cache=writeback --noautoconsole


rm a.img; qemu-img create -b /home/salvus/vm/images/base/salvus-2013-11-20-0617.img -f qcow2 a.img; chgrp kvm a.img; chmod g+rw a.img
virt-install --connect qemu:///system --cpu host --network network:default,model=virtio --name a --vcpus 2 --ram 8192 --import --disk a.img,device=disk,bus=virtio,format=qcow2,cache=writeback --noautoconsole
virsh_sys destroy a; virsh_sys undefine a


virt-install --connect qemu:///system --cpu host --network network:default,model=virtio --name a --vcpus 2 --ram 8192 --import --disk b.img,device=disk,bus=virtio,format=qcow2,cache=writeback --noautoconsole


virt-install --connect qemu:///system --cpu host --network network:default,model=virtio --name salvus-2013-11-20-0758 --vcpus=12 --ram 8192 --import --disk /home/salvus/vm/images/base/salvus-2013-11-20-0758.img,device=disk,bus=virtio,format=qcow2,cache=writeback --noautoconsole --graphics vnc,port=12101

virt-install --connect qemu:///system --cpu host --network network:default,model=virtio --name salvus-2013-11-20-1230 --vcpus=12 --ram 8192 --import --disk /home/salvus/vm/images/base/salvus-2013-11-20-1230.img,device=disk,bus=virtio,format=qcow2,cache=writeback

----

- [x] KVM bug in ubuntu with multiple images.

       https://bugs.launchpad.net/ubuntu/+source/libvirt/+bug/696318

add this at the end of /etc/apparmor.d/abstractions/libvirt-qemu

  # Work around for bug 696318 in KVM
  /var/lib/libvirt/images/** r,
  /home/salvus/vm/images/** r,
  /home/salvus/vm/images/temporary/** r,
  /home/salvus/vm/images/persistent/** r,
  /home/salvus/vm/images/base/** r,

Then "sudo service apparmor  restart"


## NOTE:

I just found that you can *directly* make a qcow2 image available:
         https://www.kumari.net/index.php/system-adminstration/49-mounting-a-qemu-image
         http://alexeytorkhov.blogspot.com/2009/09/mounting-raw-and-qcow2-vm-disk-images.html

    sudo modprobe nbd max_part=8
    qemu-nbd --connect=/dev/nbd0 ~/vm/images/temporary/cassandra1.img
    sudo mkdir /mnt/kvm
    sudo vgscan
    sudo vgchange -ay
    sudo lvdisplay |more
    sudo mount /dev/salvus-base/root /mnt/kvm
    sudo umount /mnt/kvm
    sudo vgchange -an salvus-base
    sudo killall qemu-nbd

But the shocking thing is that qemu-nbd and nbd-client make it possible to mount raw qcow2 images over the network?!  Interesting.

    sudo apt-get install nbd-client



- [x] BAD IDEA: each project could be stored in a single qcow2 image file, which gets regularly compressed.
      This could make it easy to store them...

- [x] figure out how to get lxc base image to run on the system "system" instead of the user one, and with the "default" network instead of slirp


- [x] the pauses when editing the 581f book are due to storing pages in the database when sending them to the user.
      If I use in-memory instead of database in the hub, it goes away.
      It goes to hell (hub = 100% cpu) any time editing the double ctnt.tex file in devel project.
      Partial solution for now: rewrite salvus_client.read_file_from_project

- [x] rewrite salvus_client.read_file_from_project to *NOT* use the database, and instead use the raw http server.  This will be faster and dramatically reduce the load on the database.   And it is what I planned to do anyway.

    - [ ] (1 day project?) redesign previews so that they are next to the file, shared, generated when needed, etc., instead of using tmp, and generate all pages in parallel using individual calls to gs
    - [x] when running gs on file.pdf, should run on copy, since if it changes, then file breaks and previews are never created.
    - [x] (seems to just work) need to improve raw file server to always work even if file *just* created
    - [x] deal with this in editor.coffee -- no see above.
               # TODO: need to somehow ttl that directory...
               #remove_tmp_dir(@project_id, "/", tmp)
    - [x] need to set .sagemathcloud/root symlink or something like that

- [x] setup 10.9.1.1 VM w/ full disk encryption and no ssh key access for offsite backup initiated from that machine, and start backup going using bup for cassandra (so get de-dup snapshots even *when* we automate this to happen periodically, so a corrupted DB doesn't wreck backups).  With snap backups, after we get a full set, we'll switch to *only* copying over the newest data, so snap backups can't get wrecked.

 - [x] upgrade to codemirror-3.20


- [x] (1:00?) logging: make a new table that logs to the database each time a syncdoc (or other file) is opened by a client; this should be extremely useful for analytics, to see *what* people are using.

- [x] (0:45?) add search box for project log

# Next Release

    ssh 192.168.122.22

 - [x] delete this in client.coffee once restart project servers:
         "TODO: this is temporary hacked code *only* until the project servers get restarted"
 - [x] install Julia system-wide:
         http://ubuntugeeknerd.blogspot.com/2012/10/how-to-install-julia-programming.html
 - [x] apt-get install tig libtool

---

- [x] make neuron generally available: http://pub.ist.ac.at/~jguzman/doc/neuron/tutorial/Tutorial0.html#to-build-the-neuron-shared-library-for-python
      https://mail.google.com/mail/?fs=1&source=atom#inbox/14293e8c8f2686be

        # Install Neuron
         sage -sh; cd /tmp; hg clone http://www.neuron.yale.edu/hg/neuron/iv; hg clone http://www.neuron.yale.edu/hg/neuron/nrn


- [ ] switch to LXC
  - [ ] finish lxc.py script
     - [x] lxc: killing the python lxc.py process doesn't kill/destroy the corresponding lxc container.
     - [x] new_lxc_image.py script: like new_vm_image.py
     - [x] lxc: choose latest base automatically
     - [x] lxc: option to decide whether or not salvus gets mounted and to decide if a project gets mounted
     - [x] lxc: automatic hostname based on project id
     - [x] lxc: "sage" command to run sage; lxc: latexing
     - [x] lxc: install new ipython into sage
     - [x] lxc: install_scripts in /usr/local/bin/
     - [x] lxc: go through everything listed in build.py

  - [ ] lxc machine -- switch /var/lib/lxc to btrfs, somehow.... e.g., by simply creating a new qcow2 image, mounting it, and rsyncing the data over.

  - [ ] lxc: limit RAM
  - [ ] lxc: limit cpu
  - [ ] lxc: limit disk


  - [x] worksheet printing -- minimal gui and release something:
    - [x] (0:30?) (0:28) sagewsprint: make it so sagews2pdf.py can be used from the command line.
    - [x] (0:45?) provide a UI button/popup with title that prints




