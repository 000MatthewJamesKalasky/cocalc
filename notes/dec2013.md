# Next update:
SMC:
 - [x] add lz4c: sudo add-apt-repository ppa:gezakovacs/lz4; sudo apt-get update; sudo apt-get install liblz4-tool

Google:
 - [ ] add lz4c: cd /tmp/; wget https://dl.dropboxusercontent.com/u/59565338/LZ4/lz4-r109.tar.gz; tar xvf lz4-r109.tar.gz; cd lz4-r109; make install; rm -rf /tmp/lz4-r109
 - [ ] sage -sh; umask 022; pip install pyinotify
 - [ ] add to /etc/sysctl.conf: # fs.inotify.max_user_watches=10000000
 - [ ] apt-get install strace iotop
 - [ ] sudo vi /etc/ssh/sshd_config; sudo /etc/init.d/ssh restart  # allow passwords; also MaxStartups 120


# Replication of project data -- High Availability "End Game"

   - [ ] i just started a zpool scrub on 10.3.1.4 to see what happens.  It looks like it would take over a day per terabyte...
root@compute1dc2:/home/salvus# zpool list
NAME       SIZE  ALLOC   FREE    CAP  DEDUP  HEALTH  ALTROOT
projects   508G   183G   325G    35%  4.61x  ONLINE  -
root@compute1dc2:/home/salvus# zpool scrub projects
root@compute1dc2:/home/salvus# zpool status projects
  pool: projects
 state: ONLINE
  scan: scrub in progress since Thu Dec 26 13:44:23 2013
    450M scanned out of 183G at 9.38M/s, 5h31m to go
    0 repaired, 0.24% done


   - [x] (0:45?) (1:47) storage2: open a project somewhere

   - [x] (0:30) storage2: fix some issues with replicate and ssh host keys

   - [ ] (2:00?) hub: make responsive to project location from database.
          - always check on start

   - [ ] (0:30?) make hub do open_project as step 1 of opening a local hub project (?)

   - [ ] (1:00?) option to the snapshot function to only snapshot if "interesting" files have changed.

   - [ ] (2:00?) make snapshots browse .zfs/snapshot when new-style project
        - need to divide up by day
        - need to add a "restore" button
        - need to add a read-only open for files (need that forever)

   - [ ] (2:00?) restore file from snapshot.

   - [ ] (3:00?) viewing of read-only files

   - [ ] (1:30?) implement migrate again in storage.coffee, so that it can interact with database and be much more parallel.
         - if project_id doesn't exist on this machine, create it
         - open that project
         - rsync over; look at the rsync output and ignore lines like this; as long as no other errors, good
               rsync: readlink_stat("/mnt/home/teaAuZ9M/x/mnt") failed: Permission denied (13)
               rsync error: some files/attrs were not transferred (see previous errors) (code 23) at main.c(1070) [sender=3.0.9]
         - if any files get transfered due to changes, then make another snapshot, and replicate it out.
         - set project location to empty so next time it is opened, it uses new storage system

  - [ ] storage2 optimization: name the zfs send files in a *canonical* way and delete all that are greater than TIMEOUT old periodically.  touch when use.
        also, when creating, make another file that says that it is being created; delete that file when creation is done.
        This approach will massively reduce the cpu and disk-io wasted, since in the typical case it all happens exactly once.  VERY NICE.

  - [ ] storage2 optimization: "send to another dc, then distribute across that dc" should be a single atomic task, rather than doing all the send-to-other dc's first.

  - [ ] (2:00?) allow user to manually make a snapshot with a tag; these never get deleted;

  - [ ] (1:30?) hub: make it so hub checks location of projects periodically in database (when used), and appropriately reconnects when this changes


---

  - [ ] important bug fix -- the salvus_version file should be under revision control.

  - [ ] (1:00?) delete several snapshot server disks to save space.


  - [ ] (1:00?) change usage in settings to provide quota information, etc., computed

  - [ ] (2:00?) project settings: datacenter preference

  - [ ] (1:00?) storage: I'm not convinced that the activity watcher properly adds new directories 100% yet.  Maybe just have it rescan periodically?  note that the architecture I have would allow for the hub to also touch the appropriate file in storage, so we can use both activity sources.

  - [ ] ensure that zpool import is fast by using cachefile: http://utcc.utoronto.ca/~cks/space/blog/solaris/ZFSCachefiles without this it could be 10-15 minutes just to boot.  See also https://blogs.oracle.com/apatoki/entry/using_zfs_to_build_a

  - [ ] optimization -- storage: in replicate, (1) send one copy to each data center, then (2) tell each copy in the other data centers to send to the other node in that data center by ssh'ing over and calling storage.py.  This would take a little longer but will be more efficient network-wise.

  - [ ] optimization -- delete old automatic snapshot automatically to save space and clutter.


  - [ ] optimization -- tuning dedup ram tradeoff: https://github.com/zfsonlinux/zfs/issues/1369

  - [ ] optimization -- use nc (?) instead of scp to send data -- no encryption, since link is already encrypted.

  - [ ] migrate compute node status idea:  make a map collection time in the compute_server table and
        whenever the storage.coffee module successfully uses a node for something (?), update the mapping, which will be
           hub_address-compute_address:status
        e.g.,
           '10.1.1.3-10.3.1.4':{state:'up',timestamp:?}
        it is the status of 10.3.1.4 from the pov of of 10.1.1.3, at the given moment in time.
        When deciding on where to allocate a project, we can use all this info.  ???

  - [ ] project *reserved* space: ZFS has a notion of reserved space for a filesystem.  This will be a for pay feature.

  - [ ] (1:00?) storage2: node_status -- function that (1) caches result in database for 30 seconds, (2) computes some info about a compute node and returns it if not using cache.     What to compute -- something like this:
          - whether or not up
          - is projects zpool active
          - load numbers
          - ram numbers
          - number of processes
          - uptime


=======================================


NOTE: great dedup article: http://constantin.glez.de/blog/2011/07/zfs-dedupe-or-not-dedupe

# Next update


# Issue/bug

- [ ] If "ssh project@host" fails, we need a way to get in and fix it anyways, i.e., to repush the keys, i.e., a sudo script to fix access.  Like GCE seems to do.


# BUGS

- [ ] (1:30?) case sensitivity of email addresses causes confusion!

- [ ] This is a traceback from what is causing dropped client connections right now:

        error: Query cql('SELECT owner,collaborator FROM projects WHERE project_id = ?  ',params=bdc96a2b-2670-49c7-b8c9-7d2c1b665230) caused a CQL error:
        ResponseError: Operation timed out - received only 5 responses.

        events.js:72
                throw er; // Unhandled 'error' event
                      ^
        ResponseError: Operation timed out - received only 5 responses.
            at FrameReader.readError (/home/salvus/salvus/salvus/node_modules/node-cassandra-cql/lib/readers.js:364:10)
            at FrameParser.parse (/home/salvus/salvus/salvus/node_modules/node-cassandra-cql/lib/readers.js:492:29)
            at FrameParser.transformPartialFrame (/home/salvus/salvus/salvus/node_modules/node-cassandra-cql/lib/readers.js:480:20)
            at FrameParser._transform (/home/salvus/salvus/salvus/node_modules/node-cassandra-cql/lib/readers.js:454:10)
            at FrameParser.Transform._read (_stream_transform.js:179:10)
            at FrameParser.Transform._write (_stream_transform.js:167:12)
            at doWrite (_stream_writable.js:221:10)
            at writeOrBuffer (_stream_writable.js:211:5)
            at FrameParser.Writable.write (_stream_writable.js:180:11)
            at write (_stream_readable.js:583:24)


## Next

- [ ] regina: https://mail.google.com/mail/u/0/?shva=1#inbox/14306cf912d3dd57

- [ ] LXC: need to change MTU for networking to work -- https://mail.google.com/mail/u/0/?shva=1#inbox/142ea20a9fb033ea

- [ ] important observation: the current cassandra driver I'm using does a TERRIBLE job dealing with database nodes not being accessible if they are given in the pool!  Watch out.

- [ ] idea for sharing project data with other projects -- use ZFS NFS support -- http://docs.oracle.com/cd/E23824_01/html/821-1448/gayne.html

- [ ] (1:30?) create a global gluster volume called "data"; the bricks could be zfs so we get compression and deduplication.
- [ ] (1:30?) create a global gluster volume called "scratch"
      note: can quota a directory even before it is created -- http://gluster.org/community/documentation/index.php/Gluster_3.2:_Setting_or_Replacing_Disk_Limit

- [ ] (1:00?) change so that *hub* closes project after a certain period, not projects themselves, and use project.py unmount
      To do this, we could simply scan the projects modified in the last week that were not modified in the last day, and ensure
      those are all closed.

- [ ] (2:00?) add UI, etc. to increase quota

- [ ] (2:00?) snapshots: switch to using ZFS for them.
      there's a debian package called "zfs-auto-snapshot".

- [ ] migrate all projects...




## Multi-data center HA
- [ ] (0:20?) get rid of existing glusterfs on bare metal
- [ ] (0:30?) change project.py script to create several ZFS img files that are 256MB each
- [ ] (0:45?) change project.py script when migrating to create several ZFS img files are a min of 1GB and a max of the size of the project (up to a limit)
- [ ] (2:00?) create a gluster service in admin.py which uses inotify to sync between clusters
- [ ] start building a debian image for GCE that's just my standard image (no lxc)



This will mount things as needed, start the gluster daemon, and also my inotify-based geo-sync daemons.  Also, the info in this config will be used by the compute vm's.

- [ ] modify compute vm to use global gluster store for projects
    - [ ] on startup the compute vm would mount glusterfs (given by some configuration info)
      (this would mean rooting a compute vm gives that person access to all projects unless I
       somehow encrypt them, but zfs doesn't do encryption: https://github.com/zfsonlinux/zfs/issues/494)
      But encrypting isn't supported, would likely kill the sparseness anyways.  Ugh.
      Solution: make it very, very easy for users to use encfs *inside* their projects if they elect to -- flip a switch.
      This solves a lot of problems.
    - [ ] to launch a project, hub will just connect in as "salvus" and run a sudo script that
      imports the relevant zfs pool, which will make /home/projectid mounted and available,
      possibly creating the relevant user and chown'ing.  Also, set database entry with location.
    - [ ] to stop a project, hub connect in, kill all projectid procs, then export the zfs pool.





# Idea -- how to make a global shared fs...?

On some node:

    zfs create projects/global

Use per-user quotas on that dataset, since zfs does that.

    apt-get install nfs-kernel-server   # critical that this is running
    zfs sharenfs=on projects/global
    mkdir -p /mnt/global; mount -t nfs 10.1.2.4:/projects/global /mnt/global

Then behind the scenes we would replicate.   *This is too slow to be really useful.*
The only thing that would be acceptable is some sort of sync with local filesystems.
I can see what's so cool about dropbox now.  It's a networked filesystem with a local cache.

Another idea -- make a dataset for each node:

       projects/share/compute1a
       projects/share/compute2a
       projects/share/compute3a
       ...
       local --> hostname

with a relatively restrictive per-user quota.
Users can put data there, and it gets replicated out to *all* compute nodes eventually.
Data users place there is by default world readable, but only writeable by them (umask 022).
The local dataset is read/write, but *all* others are read only.

Anyway, this seems like an excellent and fast way, which would just use the existing infrastructure.



## Other

- [ ] this is neat: http://cloudmonitor.ca.com/

- [ ] modify compute vm to use cgroups to limit users


- [ ] combine stunnel and haproxy: https://mail.google.com/mail/u/0/?shva=1#inbox/142ec7a3b987273c

- [ ] setup k-node glusterfs cluster at google us-central-1, and sync it with my existing glusterfs filesystem.  Use another filesystem instead of ext4 (?)

- [ ] geosync: implement locking system and randomized waiting, so I can run daemons on all nodes

- [ ] geosync: when copying, if it takes k seconds to copy a file, don't recopy it again for 2k seconds (?), so that a single file doesn't hog resources.

- [ ] design and implement a backup system for the global glusterfs data store, to recover from catastrophic point-in-time failure.
      Idea:
          - create a Linux VM with a large ZFS filesystem.
          - remotely mount it over nfs or just make it another glusterfs target
          - http://www.gluster.org/community/documentation/index.php/GlusterOnZFS

- [ ] experiment with performance impact of disabling ZIL for zfs mounted projects: `zfs set sync=disabled sp1`

- [ ] lxc container project that can mount project and run it with a given ip address:  lxc container
     - a base image, which is easy to move forward
     - mounts the zfs /home/project directory
     - datasets volume (sync it somehow to other dc's when adding new data)


- [ ] write script that for each project, sets something in database ('temporary maintenance'),
      updates the image (via rsync), then sets a field in projects database that says that
      project has been migrated to LXC+glusterfs+ZFS.

- [ ] rewrite snapshot interface to use zfs snapshots instead of bup

- [ ] get LXC container system to also work with GCE

- [ ] when connection gets created for new_local_hub, it will always query database for location and use that.

- [ ] if new_local_hub queries and gets empty location, it will call function to deploy project, then make connection

- [ ] (2:00?) project should get location info it needs from database (or just move that to localhub).

- [ ] (1:00?) project move: better ui feedback on move

- [ ] (3:00?) once snapshots are under control, e.g., using zfs and user-deletable: tell Rupert Hölzl <r@hoelzl.fr>


# Other

- [ ] (1:00?) project listing search bug: put "crem" in search box, then change to a project then change back to project list, and the cremona project vanishes.

- [ ] (1:00?) somebody improved printing: https://mail.google.com/mail/u/0/?shva=1#inbox/1424dfa44323d4aa

- [ ] (1:00?) somebody else cedric also improved printing: https://mail.google.com/mail/u/0/?shva=1#inbox/142eb9b3780afd83

- [ ] (1:00?) delete contents of ~/.forever on start_smc (?) -- it's 700+ MB for my devel project!!

- [ ] (1:30?) proper file rename/move for project files, finally!

- [ ] (0:30?) file operations thing on right is too far over for directories (compared to files, which are correct)

- [ ] (2:00?) limit user cpu using cgroups (see redhat guide)
- [ ] (2:00?) limit user memory using cgroups
- [ ] (2:00?) limit user disk io using cgroups

- [ ] (1:00?) change snapshot display to use timeago for the time they happened, when time since utc is available; otherwise, don't.

- [ ] (1:00?) bug in parsing try/except/else -- https://mail.google.com/mail/u/0/?shva=1#starred/1428eb398a87ed4e
        try:
            print('try')
        except:
            print('except')
        else:
            print('else')

- [ ] (1:00?) bug in parser with non-indented comments
- [ ] (1:30?) bug with wrong pill being highlighted.
- [ ] (2:00?) print for other document types (use lstlisting or...?)
- [ ] (1:00?) terminal: preserve history in file...
- [ ] (1:00?) fix password reset to be more robust against browser cache -- just make it some simple html
- [ ] (1:30?) fix terminal hangs

- [ ] (1:00?) can't use # in tex mode in %md cells, i.e., $$ \# $$ doesn't work.
- [ ] (1:00?) typeset_mode(True); EllipticCurve('11a').lseries()   # yuck!
- [ ] (1:30?) make latex timeout configurable: then tell Rupert Hölzl <r@hoelzl.fr>


# OLD/DONE ---------------------


## deploy on Google Compute Engine

Note -- all prices went down on Dec 2!

    - [ ] get quota raised (requested on Nov 29, 2013 at 11:30am; again twice on Dec 2.)
    - [ ] (0:30?) Create disks (snapshots, database images; etc.) at $0.085/GB = $272/month
          All these will use LVM:  sudo apt-get install lvm2
          Will grow later.
               cassandra1g1  100GB
               cassandra2g1  100GB
               cassandra3g1  100GB
               cassandra4g1  100GB
               compute1g1    400GB
               compute2g1    400GB
               snap1g1       1000GB
               snap2g1       1000GB
    - [ ] (1:00?) 4-node cassandra site with replication factor of 3:
               ...  n1-standard-1 instances (1 core, 3.75GB RAM)     $0.115/hour/machine (4 machines) = $331.20/month
               cassandra1g   10.4.1.2
               cassandra2g   10.4.2.2
               cassandra3g   10.4.3.2
               cassandra4g   10.4.4.2
    - [ ] (1:00?) 2-node web serving (hub, haproxy, stunnel, nginx, snap) nodes:
               ....  n1-standard-1 instances (1 core, 3.75GB RAM)     $0.114/hour/machine = $166/month
               each has a public ip address
               web1g         10.4.1.3
               web2g         10.4.2.3
    - [ ] (1:00?) 2 4-core compute vm's with /mnt/home:
               2 .... n1-standard-4-d instances (4 cores, 15GB RAM, diskless)  $0.461/hour/machine = $664/month (2 machines)
               compute1g     10.4.1.4
               compute2g     10.4.2.4

    Total cost: about $1200/month

    - [ ] (0:30?) Add the public web server ip's to godaddy dns.

========================

- [x] (1:30?) (0:58+) bup snapshotting issue when sshfs is stale.
   ',3702601d-9fbc-4e4e-b7ab-c10a79e34d3b,command '/usr/bin/bup' (args=on teaAuZ9M@10.1.2.4 index --one-file-system .) exited with nonzero code 1 -- stderr='/mnt/home/teaAuZ9M/lxc: [Errno 107] Transport endpoint is not connected: 'lxc'

   Should be fixed -- wait 15 minutes to see if cloud.dev project gets snapshotted; if so, good!
   If so, update bup on all compute machines.


- [x] (0:15?) enable crontabs on 10.9.1.2, once the two backups finished.

- [x] (1:00?) (0:40) change the create_unix_user.py (sudo) command to optionally take an account name as input.
- [x] (1:00?) (0:18) fix json worksheet printing issue that Harald pointed out with his theano example; also "tmp/linear regression.sagews" -- worksheet that doesn't print from Gustav Delius

- [x] (0:30?) (0:09) worksheet printing -- command line option to leave files around.

- [x] (1:00?) FAIL to add file attachment to worksheet printing -- http://tex.stackexchange.com/questions/94811/attaching-file-into-a-pdf-with-pdflatex-will-crash-adobe-reader

- [x] (0:10?) (0:09) send updated "file usage" data.

- [x] (0:45?) (0:47) snap -- save last successful snapshot in own table (make sure snap user doesn't have to edit project table).

    CREATE TABLE last_snapshot (
        project_id uuid,
        server_id  uuid,
        repo_id    uuid,
        timestamp  varchar,
        utc_seconds_epoch int,
        PRIMARY KEY(project_id, server_id)
    );


- [x] (1:00?) (2:00) show most recent available snapshot time, with a link to the snapshots listings.
- [x] (0:30?) last snapshot -- doesn't update properly -- why?!

- [x] (1:00?) (0:30) change smc favicon to be sage in color... and/or maybe use harald's?: https://mail.google.com/mail/u/0/?shva=1#inbox/142aead2719273d3

- [x] (1:00?) (1:08) project move: change username to project-id on project move and also new project creation; this is more logical and avoids all possible issues of conflict in future, especially enabling having a hot standby.f
- [x] (2:00?) (0:37) project move: go through each snapshot until success.
- [x] this repo on 10.1.10.3 is broken:
        202630a1-b0a5-422f-8e4f-b48e4998b371
    echo "select timestamp, project_id from snap_commits where repo_id=202630a1-b0a5-422f-8e4f-b48e4998b371; " | cqlsh_connect 10.1.3.2 |sort> a
    I had to go back 14 commits with snap_fix... script!?



   - [x] apt-get autoremove; apt-get clean
   - [x] install gluster 3.4.1: http://download.gluster.org/pub/gluster/glusterfs/3.4/3.4.1/Ubuntu/Ubuntu.README
          add-apt-repository ppa:semiosis/ubuntu-glusterfs-3.4
          apt-get update
          apt-get install glusterfs-client
   - [x] zfs support
          apt-add-repository --yes ppa:zfs-native/stable
          apt-get update
          time apt-get install ubuntu-zfs  # takes a long time - 12 minutes (?)
   - [x] update salvus repo
   - [x] update system-wide bup from git repo.
   - [x] update create_unix_user.py
   - [x] standard upgrades



- [x] copy /etc/hosts over from cloud1
- [x] apt-get install nfs-client
- [x] try out glusterfs geo-replication.  I could do the following (?):
         - cluster1 has a read/write glusterfs volume of projects it owns
         - cluster2 and cluster3 have geo-replicated versions of cluster1's projects in a *separate* store, for redundancy purposes
         - when cluster1 goes down or a user moves project to cluster2, *then* copy the project image from the
           geo-replicated slave to the cluster2 master volume and later remove it from the master on cluster1.
      Good idea?  Yes if:
           - geo-replication is actually efficient with sparse files
           - copying a sparse file is efficient.




- [ ] wild idea: have a slow distributed redundant "cloud filesystem" of newest version of all projects, which is distributed among all machines, with redundancy.  It is *only* used for project move and a "nearly live latest backup", but not run off of.  Active projects get rsync'd to it.  Either this exists already (in a form that doesn't completely suck), or I can write something that accomplishes the same thing, which is *organized* on top of cassandra (?).  Lay out the rules and try.

  - Researching this.  Looking at *gluster*.

  sudo apt-get install glusterfs-server
  sudo gluster peer probe cloud2.math.washington.edu
  sudo gluster peer probe cloud1.math.washington.edu  # on cloud2
  sudo gluster volume create testvol replica 2 transport tcp cloud1.math.washington.edu:/tmp/data cloud2.math.washington.edu:/tmp/data
  sudo mkdir /mnt/glusterfs  # on both machines
  mount -t glusterfs cloud1.math.washington.edu:/testvol /mnt/glusterfs
  How to setup so that each datacenter has two copies of files: http://gluster.org/pipermail/gluster-users/2011-March/029810.html

- [ ] make a big peer network of all my host vm's using /home/salvus/vm/images/gluster

        sudo gluster peer probe 10.1.2.1; sudo gluster peer probe 10.1.3.1; sudo gluster peer probe 10.1.4.1; sudo gluster peer probe 10.1.5.1; sudo gluster peer probe 10.1.6.1; sudo gluster peer probe 10.1.7.1; sudo gluster peer probe 10.1.10.1; sudo gluster peer probe 10.1.11.1; sudo gluster peer probe 10.1.12.1; sudo gluster peer probe 10.1.13.1; sudo gluster peer probe 10.1.14.1; sudo gluster peer probe 10.1.15.1; sudo gluster peer probe 10.1.16.1; sudo gluster peer probe 10.1.17.1; sudo gluster peer probe 10.1.18.1; sudo gluster peer probe 10.1.19.1; sudo gluster peer probe 10.1.20.1; sudo gluster peer probe 10.1.21.1

### Projects

' '.join(['10.1.%i.1:/home/salvus/vm/images/gluster/projects'%i for i in [1,10,  2,11,  3,12,  4,13,  5,14,  6,15,  7,16]])

        sudo gluster volume create projects replica 2 transport tcp 10.1.1.1:/home/salvus/vm/images/gluster/projects 10.1.10.1:/home/salvus/vm/images/gluster/projects 10.1.2.1:/home/salvus/vm/images/gluster/projects 10.1.11.1:/home/salvus/vm/images/gluster/projects 10.1.3.1:/home/salvus/vm/images/gluster/projects 10.1.12.1:/home/salvus/vm/images/gluster/projects 10.1.4.1:/home/salvus/vm/images/gluster/projects 10.1.13.1:/home/salvus/vm/images/gluster/projects 10.1.5.1:/home/salvus/vm/images/gluster/projects 10.1.14.1:/home/salvus/vm/images/gluster/projects 10.1.6.1:/home/salvus/vm/images/gluster/projects 10.1.15.1:/home/salvus/vm/images/gluster/projects 10.1.7.1:/home/salvus/vm/images/gluster/projects 10.1.16.1:/home/salvus/vm/images/gluster/projects

        sudo gluster volume set projects auth.allow 10.*

It turns out that the above is really slow?!

// on a client vm:

root@compute1a:/mnt/projects# sync; dd bs=1M count=128 if=/dev/zero of=test conv=fdatasync; sync
128+0 records in
128+0 records out
134217728 bytes (134 MB) copied, 11.334 s, 11.8 MB/s
root@compute1a:/mnt/projects# time ls teaAuZ9M/>/dev/null
real    0m0.190s

That's exactly my network speed, given TINC.

On bare metal:

salvus@cloud1:~/vm/images/gluster$ sync; dd bs=1M count=128 if=/dev/zero of=test conv=fdatasync; sync
128+0 records in
128+0 records out
134217728 bytes (134 MB) copied, 2.00069 s, 67.1 MB/s



Try again, but with no encryption in simplest setup:

sudo gluster volume create testvol replica 2 transport tcp cloud1.math.washington.edu:/home/salvus/vm/images/gluster/testvol cloud2.math.washington.edu:/home/salvus/vm/images/gluster/testvol

sudo gluster volume create testvol replica 2 transport tcp 10.1.1.1:/home/salvus/vm/images/gluster/img 10.1.1.2:/home/salvus/vm/images/gluster/img

Extracting sage with a mounted client on a vm took this long:  11m43.856s

The dd test on the compute vm is FAST -- almost same as local disk: 134217728 bytes (134 MB) copied, 2.21882 s, 60.5 MB/s
So the time diff might be latency.


 * Info about using ssl encryption with glusterfs: http://nongnu.13855.n7.nabble.com/Glusterfs-SSL-capability-td168156.html
 * Make xfs .img loopback device: http://www.mail-archive.com/gluster-users@gluster.org/msg08360.html

Something to test:

   - create a loopback device on top of glusterfs: how does it perform?

   dd of=test.img seek=1024 bs=1M count=0
   losetup /dev/loop1 test.img
   losetup -a | grep loop1
   mkfs.ext4 /dev/loop1
   mount -o loop /dev/loop1 /mnt/test

Wow, this is *amazing* -- it completely solves the latency issue and provides exactly what I really need.

Problem: Do the above, but with a qcow2 image.

    modprobe nbd max_part=63
    qemu-img create -f qcow2 test.qcow2 10G

    qemu-nbd -c /dev/nbd0 `pwd`/test.qcow2  # exact path is critical!
    fdisk /dev/nbd0
    mkfs.ext4 /dev/nbd0p1

    mount /dev/nbd0 /mnt/test2

Later

    qemu-nbd -d /dev/nbd0

This seems very slow and flakie.

### this works very nicely, and doesn't waste space.  GOOD.

    #dd if=/dev/zero of=sparse.img bs=1 count=0 seek=10G
    truncate -s 10G sparse.img
    losetup /dev/loop2 sparse.img

#### To re-sparsify

    time cp --sparse=always sparse.img sparse2.img


### Problem: Format sparse image using zfs -- try to compressed and de-duplicate

(best ZFS docs: https://pthree.org/2012/12/19/zfs-administration-part-xii-snapshots-and-clones/)

    apt-add-repository --yes ppa:zfs-native/stable
    apt-get update; apt-get install ubuntu-zfs  # takes a long time!

    truncate -s 10G sparse-zfs.img
    losetup /dev/loop1 sparse-zfs.img

    mkdir /mnt/testzfs
    zpool create -m /mnt/testzfs rpool /dev/loop1
    zfs set compression=gzip-9 rpool
    zfs set dedup=on rpool
    zfs get compressratio rpool; zpool get dedupratio rpool
    zfs set quota=5G rpool

    zfs snapshot rpool@s2

    zpool destroy rpool

Proposal:

   - (LATER!?) I create a big glusterfs like I did this morning, which is distributed across the tinc VPN, with
     say two copies of each file per datacenter (with 3 data centers: 4545, padelford, gce).

   - For each project, create a sparse zfs image file
               project-id.img
     setup as above with ZFS on it, and rsync in that project.
     The name of the pool will be the project id.


   - When a project runs somewhere, it mounts the project-id.img using a loopback device.
     (NOTE on loopback device limits -- http://old.slax.org/documentation_loop_mount.php)


### TEST

        truncate -s 128G 3702601d-9fbc-4e4e-b7ab-c10a79e34d3b.img
        losetup /dev/loop1 3702601d-9fbc-4e4e-b7ab-c10a79e34d3b.img
        mkdir /mnt/3702601d-9fbc-4e4e-b7ab-c10a79e34d3b
        zpool create -m /mnt/3702601d-9fbc-4e4e-b7ab-c10a79e34d3b  pool-3702601d-9fbc-4e4e-b7ab-c10a79e34d3b /dev/loop1
        zfs set compression=gzip pool-3702601d-9fbc-4e4e-b7ab-c10a79e34d3b
        zfs set dedup=on pool-3702601d-9fbc-4e4e-b7ab-c10a79e34d3b
        zfs set quota=4G pool-3702601d-9fbc-4e4e-b7ab-c10a79e34d3b
        zfs get compressratio pool-3702601d-9fbc-4e4e-b7ab-c10a79e34d3b; zpool get dedupratio pool-3702601d-9fbc-4e4e-b7ab-c10a79e34d3b


Wait, ZFS is too clever -- no need for loopback devices craziness!

    truncate -s 4G zfs-file.img
    cd /mnt/glusterfs/x/
    zpool create -m /mnt/test2 test `pwd`/zfs-file.img

    zpool set feature@lz4_compress=enabled test; zfs set compression=lz4 test
    zfs set dedup=on test
    zfs get compressratio test; zpool get dedupratio test

    zfs umount /mnt/test2
    zfs mount -a

    zpool export test

    zpool import test -d /mnt/glusterfs/x/

    # Add new images files to pool?  Yep.  So I can easily expand the available space for a project.  So no *need* to use zfs quota.
    truncate -s 4G zfs-file-2.img
    zpool add test /mnt/glusterfs/x/zfs-file-2.img
    # and instantly there is more space.  NICE.

Note -- if user deletes a lot of data off img, it does *NOT* get smaller.  However, when offline, one can try the following to shrink it, but it DOES NOT WORK:

    time cp --sparse=always 0.img 1.img

Another approach: add a new file, then remove the old one?  Nope.  Worry about this later, since there is always the obvious *and optimal* solution of just making a new img, since that goes through and recompresses and dedups everything...

    SUCKs.

    # this is very fast locally
    time cp -rv --sparse=always projects/* /mnt/projects/

    # So, if I can somehow mount -- say via nfs and a tunnel or something !? -- the glusterfs volume, then we can probably do it fine.



- [ ] convert some of the 18262 projects (maybe the 2356 on compute1a?) to img files in the following format, with the user having uid/gid 1001

        glusterfs volume "projects"
            projects/
                project-id/
                   0.img
                   ...  # reserved for "other stuff" -- not sure what -- later
                project-id/
                   ..

      - [ ] write python script that automates making the project-id image thing above.

                 project2zfs.py  /mnt/home/foo  /path/to/projects/

          - [x] first version: assumes /path/to/projects/project-id doesn't exist; makes it, copies over files, sets uid and gid
          - [x] next version: if /path/to/projects/project-id exists, mounts it, then rsync updates
          - [x] version that works on multiple input paths

- [x] (0:37) write script project_storage.py subcommand that uses db and goes through and adds the info.json files to *all* project home on a given vm, if they aren't there already.

- [x] run above conversion script on all projects.


- [ ] when things look good, figure out how to make this global.

   - OPTION 1 -- global glusterfs:
     create glusterfs with 1 replicas in each DC and 12 bricks in each of these data centers (so replication factor=3)

           padelford              1   2   3   4   5   6   7  1b  2b  3b  4b  5b
           4545                  10  11  12  13  14  15  16  17  18  19  20  21
           gce-us-central-1     u1a u2a u1b u2b u1c u2c u1d u2d u1e u2e u1f g2f

         ##  gce-europe           e1a e2a e1b e2b e1c e2c e1d e2d e1e e2e e1f e2f   <--- too slow (?)

        I wonder -- will this make filesystem way slower (?) -- if so, we have a major problem with the whole design,
        and would have to pivot yet again :-(.

        Let's do a test:

            sudo gluster volume create projects0 replica 2 transport tcp 10.1.1.1:/home/salvus/vm/images/gluster/projects0 10.1.10.1:/home/salvus/vm/images/gluster/projects0

            sudo gluster volume set projects0 auth.allow 10.1.*

            That works *fine*.

   - OPTION 2 -- glusterfs inside each DC

     - create one glusterfs with no encryption/vpn inside each datacenter; replication factor = 2 (?)

     - write something to sync securely over the internet via rsync, where we sync back and forth with the newest file winning.  Since the files are these images, there is just one file, and conflict resolution is super easy.

Try this out right now on cloud3, cloud4:

sudo gluster volume create projects replica 2 transport tcp cloud3:/home/salvus/vm/images/gluster/projects/ cloud4:/home/salvus/vm/images/gluster/projects/

    OK, this is dramatically better.  Super, super good.  Wow.  WOW!


So here's the plan:

        - setup a non-encrypted replication factor=2 glusterfs in each data center, *including* tower.
          (worry about firewall and auth guest restriction.)

1,2
3,4,5,6,7
10,11,12,13,14,15,16,17,18,19,20,21

Do it:


        # on cloud1
        gluster peer probe cloud2
        gluster volume create projects replica 2 transport tcp cloud1:/home/salvus/vm/images/gluster/projects/ cloud2:/home/salvus/vm/images/gluster/projects/

        # on cloud3
        gluster peer probe cloud4; gluster peer probe cloud5; gluster peer probe cloud6; gluster peer probe cloud7

        gluster volume create projects replica 2 transport tcp cloud3:/home/salvus/vm/images/gluster/projects/ cloud4:/home/salvus/vm/images/gluster/projects/ cloud5:/home/salvus/vm/images/gluster/projects/ cloud6:/home/salvus/vm/images/gluster/projects/ cloud7:/home/salvus/vm/images/gluster/projects/ cloud3:/home/salvus/vm/images/gluster/projects1/

        gluster volume create projects replica 2 transport tcp 10.1.3.1:/home/salvus/vm/images/gluster/projects/ 10.1.4.1:/home/salvus/vm/images/gluster/projects/ 10.1.5.1:/home/salvus/vm/images/gluster/projects/ 10.1.6.1:/home/salvus/vm/images/gluster/projects/ 10.1.7.1:/home/salvus/vm/images/gluster/projects/ 10.1.3.1:/home/salvus/vm/images/gluster/projects1/

        # cloud10
        gluster peer probe cloud11;gluster peer probe cloud12;gluster peer probe cloud13;gluster peer probe cloud14;gluster peer probe cloud15;gluster peer probe cloud16;gluster peer probe cloud17;gluster peer probe cloud18;gluster peer probe cloud19;gluster peer probe cloud20;gluster peer probe cloud21

        gluster volume create projects replica 2 transport tcp cloud10:/home/salvus/vm/images/gluster/projects/ cloud11:/home/salvus/vm/images/gluster/projects/ cloud12:/home/salvus/vm/images/gluster/projects/ cloud13:/home/salvus/vm/images/gluster/projects/ cloud14:/home/salvus/vm/images/gluster/projects/ cloud15:/home/salvus/vm/images/gluster/projects/ cloud16:/home/salvus/vm/images/gluster/projects/ cloud17:/home/salvus/vm/images/gluster/projects/ cloud18:/home/salvus/vm/images/gluster/projects/ cloud19:/home/salvus/vm/images/gluster/projects/ cloud20:/home/salvus/vm/images/gluster/projects/ cloud21:/home/salvus/vm/images/gluster/projects/


    # For each, do
    sudo gluster volume set projects auth.allow '192.168.*'

        - start copy over project data from guests to their own centers...


        - also work on how to merge it all together across data centers regularly and automatically using a sync system; maybe csync2, or maybe just rsync with the -u option:
                -u, --update                skip files that are newer on the receiver

          Annoying issue: rsync of a sparse file is dog slow (?)
            # on cloud1
            time rsync -axvHu --sparse /mnt/projects/ cloud3:/mnt/projects/

          See this about inplace and sparse -- http://gergap.wordpress.com/2013/08/10/rsync-and-sparse-files/

          when done, the 3-7 and 10-21 DC's will have the union of all images.

        - mount and run images using an lxc container + dynamic vpn network.


read/write test:

    root@cloud3:/mnt/projects# dd if=/dev/zero of=test bs=512k count=1000
    root@cloud3:/mnt/projects# dd if=test of=/dev/null bs=512k count=1000

----

- [ ] setup new gluster cluster on cloud10, 11, 12 in VPN.

gluster volume create projects replica 4 transport tcp 10.1.10.1:/home/salvus/vm/images/gluster/projects/ 10.1.11.1:/home/salvus/vm/images/gluster/projects/ 10.1.12.1:/home/salvus/vm/images/gluster/projects/ 10.1.13.1:/home/salvus/vm/images/gluster/projects/

    - test speed (both between nodes and with client):
             dd writes at about 20MB/s between hosts.
             sparse support sucks -- but good in ubuntu13.10 locally ?!
             client gets about 10-15MB/s


 sudo gluster volume create projects transport tcp 10.1.1.5:/gluster/test

    - test security (that isn't listening to outside connections)


### A global shared data volume for database of interest to people

' '.join(['10.1.%i.1:/home/salvus/vm/images/gluster/data'%i for i in [1,10,  2,11,  3,12,  4,13,  5,14,  6,15,  7,16]])

sudo gluster volume create data replica 2 transport tcp  10.1.1.1:/home/salvus/vm/images/gluster/data 10.1.10.1:/home/salvus/vm/images/gluster/data 10.1.2.1:/home/salvus/vm/images/gluster/data 10.1.11.1:/home/salvus/vm/images/gluster/data 10.1.3.1:/home/salvus/vm/images/gluster/data 10.1.12.1:/home/salvus/vm/images/gluster/data 10.1.4.1:/home/salvus/vm/images/gluster/data 10.1.13.1:/home/salvus/vm/images/gluster/data 10.1.5.1:/home/salvus/vm/images/gluster/data 10.1.14.1:/home/salvus/vm/images/gluster/data 10.1.6.1:/home/salvus/vm/images/gluster/data 10.1.15.1:/home/salvus/vm/images/gluster/data 10.1.7.1:/home/salvus/vm/images/gluster/data 10.1.16.1:/home/salvus/vm/images/gluster/data

    sudo gluster volume set data  auth.allow 10.1.*

### A global shared scratch -- free for all that any user can access from any project; easy data sharing.

sudo gluster volume create scratch replica 2 transport tcp  10.1.1.1:/home/salvus/vm/images/gluster/scratch 10.1.10.1:/home/salvus/vm/images/gluster/scratch 10.1.2.1:/home/salvus/vm/images/gluster/scratch 10.1.11.1:/home/salvus/vm/images/gluster/scratch 10.1.3.1:/home/salvus/vm/images/gluster/scratch 10.1.12.1:/home/salvus/vm/images/gluster/scratch 10.1.4.1:/home/salvus/vm/images/gluster/scratch 10.1.13.1:/home/salvus/vm/images/gluster/scratch 10.1.5.1:/home/salvus/vm/images/gluster/scratch 10.1.14.1:/home/salvus/vm/images/gluster/scratch 10.1.6.1:/home/salvus/vm/images/gluster/scratch 10.1.15.1:/home/salvus/vm/images/gluster/scratch 10.1.7.1:/home/salvus/vm/images/gluster/scratch 10.1.16.1:/home/salvus/vm/images/gluster/scratch

    sudo gluster volume set scratch  auth.allow 10.*


Eliminating SPOF on mount: http://www.jamescoyle.net/how-to/439-mount-a-glusterfs-volume


---

PLAN:

 - glusterfs data and scratch volumes, which are shared across the system -- convenient for users.
 - have a large glusterfs volume with replication of (at least 2, for now) where canonical version of projects are stored, and project is periodically rsync'd back when running.

     Testing this: it takes hours (?) just to rsync my 2.5GB devel project over.  Thus to see if this is a usable approach, I will need to try various optimizations.  The big issue is encryption, probably.

     Idea: - completely unencrypted in data center
           - client vm connects to host only to mount (not over vpn)
           - use an ssh tunnel *between* data centers, where encryption is required


     Idea: - completely unencrypted in data center
           - client vm connects to host only to mount (not over vpn)
           - use an ssh tunnel *between* data centers, where encryption is required


 - when user opens or moves a project, if not deployed, it gets rsync from glusterfs volume as first choice.
 -




   - [ ] (1:00?) (0:20?) project move: *all* hubs using that project need to react to project move correctly... argh.  This is hard because there is no inter-hub message system... and I don't want one if I can avoid it!

  - If the project is working, then it can broadcast that it is going to be moved to all connected hubs, and they can take appropriate action.
  - If the project is not working, e.g., because the machine it is deployed on is down, then there are no connected hubs.

... so it seems like all I need is a way for a local hub to tell all connected global hubs to close *all* connections to it.
OR, even more simply, when a global hub looses the connection to the local hub, it checks in the database that the location
is the same before trying to reconnect.

So I just need to modify Project and LocalHub so that instead of caching the VM they are connected to, whenever they need that information they get it fresh from the database.  Then everything else takes care of itself.

---->  - [x] (1:30) new_local_hub should *only* take project_id as input and only cached based on that.

       - [ ] be sure to test this codepath in hub.coffee: "This deals with VERY RARE case where user of project somehow deleted"
       - [x] test this killall change: "pkill -9 -u `whoami`"


[x] CRAZY idea:  NO!
    Just have one massive gluster across the world, which is "dog slow".
    However, when actually *using* images, just copy them out of gluster
    to the local disk, and every time they change copy back, letting that
    take as long as it takes.   This would basically solve all problems,
    except it would mean that opening a projct not in the local "cache"
    takes longer.  That is acceptable.

    This would be *very* easy to implement.  Also, I could totally consider using
    cassandra for this.... but that would require tar'ing every time, which is not realistic.

    Test mounting a zpool locally inside vm and extracting sage tarball:

        truncate -s 2G test.img
        zpool create test -d /tmp/test.img
        time wget http://sage.math.washington.edu/home/release/sage-5.13.rc0/sage-5.13.rc0-boxen-x86_64-Linux.tar.gz
        # about 35 seconds
        zfs set compression=gzip test
        zfs set dedup=on test

    This is stupid.  It just throws away valuable information, and isn't much faster.  It also guarantees we'll loose
    data when things go wrong.



- [x] decommission cloud13:
   - [x] disable in dns: https://dns.godaddy.com/ZoneFile.aspx?zone=SAGEMATHCLOUD.COM&zoneType=0&sa=isc%3dbb1455jd90&refer=dcc&marketID=en-US
   - [x] move compute13a:
         # The following would take 7 hours... -- too slow!
         rsync -axvH --sparse cassandra13-cassandra.img compute13a-home.img cloud14:tmp/

         # Fortunately, I've been obsessed with how to transfer sparse files lately.  Try this instead
         # (takes about 40 minutes to make, then 40 to transfer)
         time bsdtar cvf sparse.tar compute13a-home.img
   - [x] move cassandra13
   - [x] remove compute13a from database compute servers table:
           select * from compute_servers where dummy=true;
           DELETE from compute_servers where dummy=true and host='10.1.13.4';
   - [ ] move web13 somewhere...


# LXC -- attempted to combine all services into a single LXC machine with projects running in containers

...and it fails for small subtle reasons, e.g., problems with GCE, lack of support for key features in ZFS, too much change to the overall system

This makes me want a replication factor of 3 for gluster instead of 2, because when one nodes down, then suddenly
there is a single point of failure.
I also wonder if I should virtualize the gluster nodes even on cloud-n?
Pros:
   - very easily use a different filesystem (e.g., xfs or even zfs)
   - physically move them
   - reduce attack surface
   - use ubuntu 13.10 instead of 12.04
   - simplify selling the private version
Cons:
   - slower

I'm also thinking of making it so the ZFS raw image files are smaller, which makes geo-sync potentially more efficient.

I could:

  - configure a single ubuntu 13.10 VM:
      - tinc vpn
      - runs gluster (with replication=3)
      - runs cassandra (with replication=3)
      - runs nginx
      - runs haproxy
      - runs stunnel
      - runs some hubs in containers
      - runs projects in containers

  - move my entire existing system into the above, so I'm just running such identical vm's everywhere.

  - [x] create brand new ubuntu 13.10 machine on cloud10

        qemu-img create -f qcow2 base.img 64G          # ext4  -- the base vm linux install
        qemu-img create -f qcow2 lxc.img 128G          # btrfs -- /var/lib/lxc: for lxc containers
        qemu-img create -f qcow2 cassandra.img 256G    # xfs -- for cassandra
        qemu-img create -f qcow2 bup-0.img 256G        # ext4 -- for bup backups of this or other machines
        qemu-img create -f qcow2 gluster-0.img 256G    # xfs gluster brick0 -- we'll grow these as needed

        virt-install --connect qemu:///system --cpu host --network user,model=virtio --name smc --vcpus=8 --ram 8192 --cdrom ~/vm/images/iso/ubuntu-13.10-server-amd64.iso --disk ~/vm/images/smc/base.img,device=disk,bus=virtio,format=qcow2,cache=writeback --disk ~/vm/images/smc/lxc.img,device=disk,bus=virtio,format=qcow2,cache=writeback --disk ~/vm/images/smc/gluster-0.img,device=disk,bus=virtio,format=qcow2,cache=writeback --graphics vnc,port=12102

        virsh autostart  # see https://wiki.archlinux.org/index.php/libvirt

        ssh -L 12102:localhost:12102 salvus@cloud1.math.washington.edu

     - [x] install base os and update/upgrade
     - [x] put on the tinc vpn as 10.1.10.5 (for now)
     - [x] disable the graphical console on the virtual machine (security risk) -- "virsh edit smc"
     - [x] (0:04) install lxc: "apt-get install lxc"
     - [x] (0:14)
           setup /var/lib/lxc filesystem (btrfs):
                apt-get install btrfs-tools
                # use fdisk on /dev/vdb
                mkfs.btrfs /dev/vdb1

           setup /var/lib/glusterd/bricks/projects0 brick (and format xfs)
                apt-get install xfsprogs
                # use fdisk on /dev/vdc
                mkfs.xfs /dev/vdc1

           setup cassandra (and format xfs)
                # use fdisk on /dev/vdd
                mkfs.xfs /dev/vdd1
                mkdir -p /var/lib/cassandra; chown salvus. /var/lib/cassandra

           setup local bup archive
                # use fdisk on /dev/vde
                mkfs.ext4 /dev/vde1
                mkdir -p /bup
                # after mount
                mkdir /bup/root/; mkdir /bup/salvus/; chown salvus. /bup/salvus/

                #In root's .bashrc:
                export BUP_DIR=/bup/root/
                # In salvus's .bashrc
                export BUP_DIR=/bup/salvus/
                # and do "bup init" for both.

           get uuid of disks:
                ls -l /dev/disk/by-uuid

           /etc/fstab entries:

                UUID=5d582c14-1d14-4d39-b06d-a136ba1b2b6d /var/lib/lxc    btrfs   defaults 0 2
                UUID=60b5dd47-32f7-41e6-a95a-7e7b0129d049 /var/lib/glusterd/bricks/projects0 xfs defaults 0 2
                UUID=44c0b200-2db2-4402-9a86-09273e8b1149 /var/lib/cassandra xfs defaults 0 2
                UUID=0a515e08-d424-495b-8787-d4e2d9817196 /bup            ext4  defaults 0 2

     - [x] install gluster
     - [x] install git and salvus repo in ~/salvus
     - [x] change lxc containers to use 172 local subnet

           /etc/default$ diff lxc-net.orig lxc-net

            16c16
            < LXC_ADDR="10.0.3.1"
            ---
            > LXC_ADDR="172.16.3.1"
            18,19c18,19
            < LXC_NETWORK="10.0.3.0/24"
            < LXC_DHCP_RANGE="10.0.3.2,10.0.3.254"
            ---
            > LXC_NETWORK="172.16.3.0/24"
            > LXC_DHCP_RANGE="172.16.3.2,172.16.3.254"

            i.e., make it so the network block is

                LXC_ADDR="172.16.3.1"
                LXC_NETMASK="255.255.255.0"
                LXC_NETWORK="172.16.3.0/24"
                LXC_DHCP_RANGE="172.16.3.2,172.16.3.254"

           I have to reboot after this change.


     - [x] setup a container that will be used for running hubs, nginx, haproxy, stunnel:
              time sudo lxc-create -t ubuntu -n salvus -- -b salvus

     - [x] setup a container that will be used for running projects:
              sudo adduser project # should be user 1001 on host!; use a large random password
              time sudo lxc-create -t ubuntu -n project -- -b project

     - [x] setup cassandra xfs filesystem.

     - [x] test lxc container system working

     - [x] backup vm work so far; quick tarball just in case and also bup every 30 minutes of important stuff to a separate drive:
             also setup external crontab bup of the vm images to another directory on same machine, and another less frequent
             bup of the images to cloud11 (from cloud10).  This is a lot of backups, but I think there is a high probability of
             messing up bigtime while doing the rest of the steps to get this new system up and running, and I feel error prone.

    - [x] (0:11) setup gluster fs: one local node with 3 local bricks for dev and *testing*:

            root@smc:/var/lib/glusterd/bricks/projects0# mkdir test1; mkdir test2; mkdir test3; mkdir test4

            gluster volume create test replica 2 transport tcp 10.1.10.5://var/lib/glusterd/bricks/projects0/test1 10.1.10.5://var/lib/glusterd/bricks/projects0/test2 10.1.10.5://var/lib/glusterd/bricks/projects0/test3 10.1.10.5://var/lib/glusterd/bricks/projects0/test4
            gluster volume start test
            mkdir -p /var/lib/smc/projects

            # put this in /etc/fstab
            localhost:/test /var/lib/smc/projects glusterfs defaults 0 2

            # In general the plan is that the global project store will be by definition what is visible at /var/lib/smc/projects


     - [x] lxc projects container; fix sudo then start installing stuff

     - [x] finish setting up project container

          - [x] dpkg-reconfigure tzdata

     - [x] *attempt* (12 hours, but fails) something similar on GCE:
             - [x] resize the root fs as at http://askubuntu.com/questions/24027/how-can-i-resize-an-ext-root-partition-at-runtime
                      fdisk /dev/sda
                      # delete and re-make /dev/sda1
                      # REBOOT!
                      resize2fs /dev/sda1

             - [x] setup lxc ext4 disk, since btfrs + lxc + debian = BROKEN ( http://bugs.debian.org/cgi-bin/bugreport.cgi?bug=663274)

                      fdisk /dev/sdb
                      apt-get install btrfs-tools
                      mkfs.btrfs /dev/sdb1

             - [x] use this debian-wheezy template, which actually works (unlike the built-in stuff):

                      # https://github.com/simonvanderveldt/lxc-debian-wheezy-template
                      wget https://raw.github.com/simonvanderveldt/lxc-debian-wheezy-template/master/lxc-debian-wheezy-robvdhoeven
                      cp lxc-debian-wheezy-robvdhoeven /usr/share/lxc/templates/lxc-debian-wheezy
                      sudo chown root:root /usr/share/lxc/templates/lxc-debian-wheezy
                      sudo chmod +x /usr/share/lxc/templates/lxc-debian-wheezy
                      # then change the br0 line to "lxc.network.link = virbr0"

             - [x] setup bridged networking to start by default:

                      virsh net-autostart default
                      virsh net-autostart default --disable

             - [x] create salvus container:

                      lxc-create -n salvus -t debian-wheezy
                      cp /etc/apt/sources.list /var/lib/lxc/salvus/rootfs/etc/apt

             - [x] create project container:

                      sudo lxc-create -n project -t debian-wheezy
                      cp /etc/apt/sources.list /var/lib/lxc/project/rootfs/etc/apt

             - [ ] build.py -- build sage, etc.; strange surprise -- wget in container insanely slow but scp out of container  fast...?                              - had issue with spell package; had to uninstall, and also to reconfigure "locales" (no default)

             - [ ] NAT/bridging just doesn't work right in lxc with both virsh default and a custom setup.  Same problem.  WEIRD.
                   I don't know why. Too new to find out.  I'll try with tinc.  If that doesn't work, give up for a while?
                   Problems with tun device.
                   Add this to config for LXC container:

                       # tun
                       lxc.cgroup.devices.allow = c 10:200 rwm

                   Then in the container:

                       mkdir /dev/net; mknod /dev/net/tun c 10 200

                iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE
                /sbin/iptables -A FORWARD -i eth0 -o tun0 -m state --state RELATED,ESTABLISHED -j ACCEPT
                /sbin/iptables -A FORWARD -i tun0 -o eth0 -j ACCEPT

            - [ ] I can't get NAT to work!  Try again, from scratch with backport debian image.

                 apt-get install lxc
                 echo "cgroup  /sys/fs/cgroup  cgroup  defaults  0   0" >>/etc/fstab; mount -a
                 dpkg-reconfigure locales
                 wget https://raw.github.com/simonvanderveldt/lxc-debian-wheezy-template/master/lxc-debian-wheezy-robvdhoeven
                 cp lxc-debian-wheezy-robvdhoeven /usr/share/lxc/templates/lxc-debian-wheezy
                 sudo chown root:root /usr/share/lxc/templates/lxc-debian-wheezy
                 sudo chmod +x /usr/share/lxc/templates/lxc-debian-wheezy
                 lxc-create -n test -t debian-wheezy
                 apt-get install bridge-utils
                 # tried direct nat -- total fail
                 sudo apt-get install libvirt-bin


            - [x] GCE -- investigate using CoreOS instead
              <http://coreos.com/docs/google-compute-engine/>
                gcutil --project=sagemathcloud addimage --description="CoreOS 153.0.0" coreos-production-v153 gs://storage.core-os.net/coreos/amd64-generic/153.0.0/coreos_production_gce.tar.gz

              It fails in the same way!  So I'm sending emails to some authorities and *giving up*.

              Making my own image for GCE should be possible:
                  - http://blog.cohesiveft.com/2013/12/delivering-virtual-machine-images-into.html
                  - http://googlecloudplatform.blogspot.com/2013/12/suse-linux-enterprise-server-now-available-on-google-compute-engine.html


     - [ ] projects: "way to run project in an ephemeral container that bind mounts a zpool from /var/lib/smc/projects/project-id
                      and deal with some sort of locking.
                     (database... but also put info file in /var/lib/smc/projects/project-id/deploy.json?)"

              project.py start project-id

              project.py stop project-id

              - [ ] user could customize timezone "dpkg-reconfigure tzdata"  (?)

     - [ ] nginx
     - [ ] hub
     - [ ] cassandra
     - [ ] new ui for snapshots


Idea: simply make it so the machine that I'm building above replace the existing compute-n machines.  That's it.
And they handle all the storage in a self-contained way.

Next hour: just test mounting a project again.

    cd /var/lib/smc/projects/c1f1dc4a-dbf0-4fc6-9878-012020a0a829
    zpool create project-c1f1dc4a-dbf0-4fc6-9878-012020a0a829 -m /mnt/projects/c1f1dc4a-dbf0-4fc6-9878-012020a0a829 `pwd`/0.img
    zfs set compression=gzip project-c1f1dc4a-dbf0-4fc6-9878-012020a0a829
    zfs set dedup=on project-c1f1dc4a-dbf0-4fc6-9878-012020a0a829
    rsync -axvH DU6rFFKG@10.1.13.4:./ /mnt/projects/c1f1dc4a-dbf0-4fc6-9878-012020a0a829/
    chown project. -R /mnt/projects/c1f1dc4a-dbf0-4fc6-9878-012020a0a829/
    time lxc-clone -s project project-c1f1dc4a-dbf0-4fc6-9878-012020a0a829  # 0.17 seconds

    # edit /var/lib/lxc/project-c1f1dc4a-dbf0-4fc6-9878-012020a0a829/fstab
    /mnt/projects/c1f1dc4a-dbf0-4fc6-9878-012020a0a829 home/project none bind 0 0

    ssh project@ip-address # ...

    # making snapshot (from outside!)
    zfs snapshot project-c1f1dc4a-dbf0-4fc6-9878-012020a0a829@init
    ## PROBLEM: these snapshots are not visible inside the vm, though they are visible to the project user outside.
    ## So... install zfs in project and merely make the img files available via the bind mount?
    ## This would have to work, and might be better... ?
    ## This does not work.  It simply doesn't build.
    - Use NFS? https://github.com/zfsonlinux/zfs/issues/616
    - single snapshots can be mount bind'd, only one at a time.

    ### ** Known problem *** -- OK, so snapshots don't work in LXC containers.
        https://groups.google.com/a/zfsonlinux.org/forum/#!topic/zfs-discuss/tNU15-r7Z20
        this has been not available in ZFS for years, and won't get fixed anytime soon.  Dang.

    zpool create -f project-c1f1dc4a-dbf0-4fc6-9878-012020a0a829 -m /var/lib/lxc/project-test/delta0/home/project/ /var/lib/smc/projects/c1f1dc4a-dbf0-4fc6-9878-012020a0a829/0.img

    - [ ] Try NFS?

  -->  - [x] Try zfs-fuse purely inside the container:  This looks ugly... but should try.
          http://jtrancas.wordpress.com/2011/02/09/fuse-filesystems-lxc-container/

# Next release (base vm update)
  - [x] raise the simultaneous ssh connection limit to 50
  - [ x] pip install pyinotify



# Reboot -- how about just map zpool's to user home directories, and run a chown when necessary?


- [x] first_boot.py support /mnt/gluster

- [x] modify vm.py to use nbd (that qcow2 network block device daemon) -- no, this doesn't work?

         qemu-img create -f qcow2 gluster1-glusterd.img 1G
         qemu-img create -f qcow2 gluster1-gluster1.img 256G
         sudo su
         cd /home/salvus/vm/images/persistent
         modprobe nbd max_part=63
         qemu-nbd -c /dev/nbd1 `pwd`/gluster1-gluster1.img
         qemu-nbd -c /dev/nbd2 `pwd`/gluster1-glusterd.img
         # use fdisk to make one pig partition on each
         mkfs.xfs -f /dev/nbd1p1   # takes about 30 seconds
         mkfs.ext4 /dev/nbd2p1
         qemu-nbd -d /dev/nbd1
         qemu-nbd -d /dev/nbd2

      for testing the above -- modify services/hosts config to have one gluster node with a brick
      then start it and xfs format the brick.

- [x] (0:15) update all vm's and install the new salvus_nbd_format.py script across the cluster

- [x] create a gluster cluster with replication of 3 on cloud1-3,4-6 and another on cloud10-12,14-16.  Make projects volume.

        gluster peer probe 10.1.2.5; gluster peer probe 10.1.3.5; gluster peer probe 10.1.4.5; gluster peer probe  10.1.5.5;gluster peer probe  10.1.6.5; gluster peer probe  10.1.7.5
        gluster volume create projects replica 3 transport tcp 10.1.1.5:/mnt/brick1 10.1.2.5:/mnt/brick1 10.1.3.5:/mnt/brick1 10.1.4.5:/mnt/brick1 10.1.5.5:/mnt/brick1 10.1.6.5:/mnt/brick1 10.1.7.5:/mnt/brick1 10.1.1.5:/mnt/brick2 10.1.2.5:/mnt/brick2 10.1.3.5:/mnt/brick2 10.1.4.5:/mnt/brick2 10.1.5.5:/mnt/brick2
        gluster volume set projects nfs.disable on
        gluster volume start projects

        gluster peer probe 10.1.11.5; gluster peer probe 10.1.12.5; gluster peer probe 10.1.14.5; gluster peer probe  10.1.15.5;gluster peer probe  10.1.16.5; gluster peer probe 10.1.17.5; gluster peer probe 10.1.18.5; gluster peer probe  10.1.19.5;gluster peer probe  10.1.20.5; gluster peer probe  10.1.21.5;
        gluster volume create projects replica 3 transport tcp 10.1.10.5:/mnt/brick1 10.1.11.5:/mnt/brick1 10.1.12.5:/mnt/brick1 10.1.14.5:/mnt/brick1 10.1.15.5:/mnt/brick1 10.1.16.5:/mnt/brick1 10.1.17.5:/mnt/brick1 10.1.18.5:/mnt/brick1 10.1.19.5:/mnt/brick1 10.1.20.5:/mnt/brick1 10.1.21.5:/mnt/brick1 10.1.10.5:/mnt/brick2
        gluster volume set projects nfs.disable on
        gluster volume start projects

        mkdir -p /mnt/gluster/padelford/projects; mount -t glusterfs 10.1.1.5:/projects /mnt/gluster/padelford/projects
        mkdir -p /mnt/gluster/4545/projects; mount -t glusterfs 10.1.10.5:/projects /mnt/gluster/4545/projects

        # on a compute node...
        mkdir -p /mnt/projects; mount -t glusterfs 10.1.14.5:/projects /mnt/projects

        Thus make it so projects can be mounted from the projects pool, using zfs, and username=projectid

   mount -o bind,uid=1892,gid=1892 /mnt/projects/c1f1dc4a-dbf0-4fc6-9878-012020a0a829/ /mnt/home/DU6rFFKG/
   zfs allow -s @adminrole snapshot project-c1f1dc4a-dbf0-4fc6-9878-012020a0a829

- [x] (1:44) project.py -- make 0.img that is max of (1.5*current, 256MB); project.py -- *mount* the ZFS image at /home/projectid and create the account called "projectid"; chown.

- [x] (0:45?) (0:37) project.py -- can be called repeatedly without trouble and has useful exit codes.

- [x] install standard packages on a GCE node.

  - [x] define data centers:
      dc0 = 4545 data center
      dc1 = padelford
      dc2 = gce uc-central-1



  - [x] redo the bricks on my machines: gluster1-21 to each be 256GB, with 2 bricks only on gluster1.
  - [x] create two more gce gluster nodes: 10.3.3.5, 10.3.4.5
  - [x] create gce web node: will be used for developing other stuff below.
  - [x] create a compute node
  - [x] create two cassandra nodes with 128GB/each
  - [x] push all gce tinc keys to all cloud-n nodes.
  - [x] make a single massive gluster *cluster*:

  gluster peer probe 10.1.2.5;gluster peer probe 10.1.3.5;gluster peer probe 10.1.4.5;gluster peer probe 10.1.5.5;gluster peer probe 10.1.6.5;gluster peer probe 10.1.7.5;gluster peer probe 10.1.10.5;gluster peer probe 10.1.11.5;gluster peer probe 10.1.12.5;gluster peer probe 10.1.13.5;gluster peer probe 10.1.14.5;gluster peer probe 10.1.15.5;gluster peer probe 10.1.16.5;gluster peer probe 10.1.17.5;gluster peer probe 10.1.18.5;gluster peer probe 10.1.19.5;gluster peer probe 10.1.20.5;gluster peer probe 10.1.21.5;gluster peer probe 10.3.1.5;gluster peer probe 10.3.2.5;gluster peer probe 10.3.3.5;gluster peer probe 10.3.4.5

  - [x] setup gluster volumes:

        gluster volume create dc0-projects replica 2 transport tcp  10.1.1.5:/mnt/brick1 10.1.2.5:/mnt/brick1 10.1.3.5:/mnt/brick1 10.1.4.5:/mnt/brick1 10.1.5.5:/mnt/brick1 10.1.6.5:/mnt/brick1 10.1.7.5:/mnt/brick1 10.1.1.5:/mnt/brick2

        gluster volume create dc1-projects replica 2 transport tcp 10.1.10.5:/mnt/brick1 10.1.11.5:/mnt/brick1 10.1.12.5:/mnt/brick1 10.1.13.5:/mnt/brick1 10.1.14.5:/mnt/brick1 10.1.15.5:/mnt/brick1 10.1.16.5:/mnt/brick1 10.1.17.5:/mnt/brick1 10.1.18.5:/mnt/brick1 10.1.19.5:/mnt/brick1 10.1.20.5:/mnt/brick1 10.1.21.5:/mnt/brick1

        gluster volume create dc2-projects replica 2 transport tcp 10.3.1.5:/mnt/brick1 10.3.2.5:/mnt/brick1 10.3.3.5:/mnt/brick1 10.3.4.5:/mnt/brick1

        gluster volume set dc0-projects nfs.disable on
        gluster volume set dc1-projects nfs.disable on
        gluster volume set dc2-projects nfs.disable on

        gluster volume start dc0-projects
        gluster volume start dc1-projects
        gluster volume start dc2-projects

        # mounting
        mkdir -p /mnt/dc0-projects; mount -t glusterfs 10.1.1.5:/dc0-projects /mnt/dc0-projects
        mkdir -p /mnt/dc1-projects; mount -t glusterfs 10.1.1.5:/dc1-projects /mnt/dc1-projects
        mkdir -p /mnt/dc2-projects; mount -t glusterfs 10.1.1.5:/dc2-projects /mnt/dc2-projects


        time /mnt/dc0-projects/salvus/project.py --verbose sync --watch --min_sync_time=0 /mnt/brick1/  /mnt/dc1-projects/

### web1gce1

 - [x] start stunnel/haproxy/nginx/hub running on 10.3.1.3=web1gce1=108.59.84.126 but using same database, etc.



- [x] get rid of the old glusterfs on hosts

- [x] quick global *test* volume

   gluster volume create global-share replica 3 transport tcp 10.1.1.5:/tmp/global  10.1.10.5:/tmp/global 10.3.1.5:/tmp/global
   gluster volume start global-share
   mkdir -p /mnt/global-share; mount -t glusterfs 10.1.1.5:/global-share /mnt/global-share/

   12 seconds to simply read a little file!  Ugh.  Useless.


- [x] It turns out that sparse file support is totally busted on xfs! switch all gluster bricks to ext4 and remake the volumes.  UGH.

   - this is complicated since rebalancing sounds highly unlikely to "work".
   - just making a huge image doesn't work -- there seems to be a kvm/qcow2/etc (?) limits of 2TB.
   - ext4 + lvm is what I can use!  Then I can add new qcow2 images to just dynamically grow bricks
     as space is needed on a node.    I just can't add new machines.  OK.
   - and I absolutely must do this on the GCE nodes too.
   - STEPS:
       - make it so each gluster node has a 256GB ext4 image attached and working.
       - figure out how on the command line...
       - UGH -- my lvm stuff won't persist since it is in /
       - an advantage to using LVM is *snapshots* -- I could snapshot the whole gluster system, just in case (?)

   - [x] (0:20?) add option to vm.py to not mount or format a persistent image.
   - [x] (0:50?) (0:43) add custom fstab option to vm.py and admin.py and use to configure expandable lvm brick stuff on gluster1 and test

      vgcreate gluster /dev/vdc
      lvcreate -n /dev/gluster/brick1 -l 100%FREE  gluster
      lvdisplay gluster
      mkfs.ext4 /dev/gluster/brick1

          # this fstab line goes in conf file:   /dev/gluster/brick1 /mnt/brick1 ext4 nobootwait 0 2

   - [x] (0:30?) push out vm.py to all and start new gluster cluster running
   - [x] (0:10?) test sparse copy; verify works right.
   - [x] (1:00?) (0:40) get the gce machines working and with lvm setup.

      vgcreate gluster /dev/sdb2
      lvcreate -n /dev/gluster/brick1 -l 100%FREE  gluster
      lvdisplay gluster
      mkfs.ext4 /dev/gluster/brick1

- [x] sync -- any advantage to many small images?
        not every file is touched always, but mount is *much* slower!

- [x] (0:45?) (0:89) gluster: implement sync locking -- skip syncing a file if there is a file called .filename.gluster.lock
      at the destination, and it is at most 1 hour (?) old.


   Hmm. I just hit an interesting fact:

      - using bup with *fuse* (and only fuse!) to restore a sparse image locally via copy is fast and
        does result in a sparse image.  Whatever.

      - I can make efficient disaster recover backups as follows:

        (1) central OFFSITE-able:
          - use bup to backup to one place all the underlying qcow2 image that contains the brick.  a 256GB image locally would take
            about an hour and occupies the *same* space as reading string through the sparse image would use.
            And when the qcow2 image is not fully used, it takes proportionally less time.   Because it is qcow2 instead
            of sparse, we can recompress it, and extracting works well too, and would take about an hour in the above size.
            Also, the (asymptotic) speed is the same over the network, and things dedup very well, given the setting.
            This is awesome.

            We would do this for exactly *one* of the volumes (no need to do all three), since we can't do it for the google volumes.

            In fact, I should convert the cassandra images to qcow2 format and also start backing them up this way too,
            since it's the easiest possible safest way for recovery.   I could then likely backup everything to <1TB for
            quite some time.

        (2) use lvm snapshots on the LVM brick volumes, to provide an "undo" just in case. Obviously, an attacker could
            just reformat everything.  Hmm.

        mkdir -p /mnt/dc0-projects; mount -t glusterfs 10.1.1.5:/dc0-projects /mnt/dc0-projects
        mkdir -p /mnt/dc1-projects; mount -t glusterfs 10.1.1.5:/dc1-projects /mnt/dc1-projects
        mkdir -p /mnt/dc2-projects; mount -t glusterfs 10.1.1.5:/dc2-projects /mnt/dc2-projects
        /mnt/dc0-projects/salvus/project.py --verbose sync --watch /mnt/brick1/  /mnt/dc1-projects/ /mnt/dc2-projects/


- [x] inotify based sync: just run one on every node?
    I Implemented it and it sucks.


sync -- use "rsync --inplace" when the destination file already exists versus cp?  Threshold?

      - weird thing: if I make a large sparse file, then cp it to a glusterfs volume, the result is much less sparse?!
      - trying rsync --sparse on same file now.

       - it turns out rsync disables delta-xfer on "local" paths, so we have to *explicitly* force it back... but this
         might cause rsync to read the entire target file into memory anyways, which is "over the network" and way worse.  Hmm.
         We could also try to write to all bricks simultaneously.  That seems very bad.

             time rsync -axvH --inplace --no-whole-file 37/02/3702601d-9fbc-4e4e-b7ab-c10a79e34d3b/   /mnt/dc1-projects/37/02/3702601d-9fbc-4e4e-b7ab-c10a79e34d3b/

    what is the right thing to do?

    Idea:

      - run one sync on every other brick
      - run deamon on *every* other brick
      - sync a given file at most once every 10 minutes (?)
      -


 -- [x] automate determining the sources and targets for sync, if not explicitly given, based on parsing the output of "gluster volume info" on the node, along with getting the ipaddress.  Then I'll be able to just run this on each node:

     /mnt/dc0-projects/salvus/project.py --verbose sync_daemons

This will in turn run (say):

    /mnt/dc0-projects/salvus/project.py --verbose sync --daemon --pidfile=brick1.pid --logfile=brick1.log /mnt/brick1 /mnt/dc1-projects/ /mnt/dc2-projects/
    /mnt/dc0-projects/salvus/project.py --verbose sync --daemon --pidfile=brick2.pid --logfile=brick2.log /mnt/brick2 /mnt/dc1-projects/ /mnt/dc2-projects/

and when the above is killed, it will kill these two.

So:

   - [x] (0:45?) (0:34) project: make daemon mode for sync watcher
   - [x] (0:30?) (0:25) storage: rewrite to use proper logging

   - [x] (0:30?) storage: make it automatically figure out bricks and run their daemons
   - [x] (0:30?) storage: make it automatically figure out targets



- [x] storage sync: cache result of last sync for massive speedup

- [x] start migration, e.g.,

export T=1; mkdir -p /mnt/dc$T-projects; mount -t glusterfs 10.1.1.5:/dc$T-projects /mnt/dc$T-projects; /mnt/dc$T-projects/salvus/storage.py --loglevel=DEBUG migrate --new_only --storage=/mnt/dc$T-projects/projects /mnt/home/*

- [x] storage sync: daemon mode for non-inotify version

- [x] vm.py -- change to not use nobootwait, since that results in /etc/passwd not getting copied over properly for compute vms!

- [x] storage sync: daemon mode to just run sync repeatedly instead of using inotify

- [x] storage sync: don't re-mount gluster target volumes if they are already mounted

   cp /mnt/dc0-projects/salvus/storage.py /tmp; /tmp/storage.py --loglevel=DEBUG sync


- [x] fix /etc/passwd (etc) on 14, 17, 21 to account for loss from backup to now; or newly created projects.
    import os
    z = set([x.split(":")[0].strip() for x in open("/etc/passwd").readlines()])

    def fix(name):
       if not os.path.exists('/mnt/home/%s/.sagemathcloud'%name):
           print "not a user"
           return
       if name in z:
           print "skipping %s since in etc passwd"%name
           return
       print "creating '%s'"%name
       os.system("useradd -d /mnt/home/%s %s"%(name, name))
       os.system("chown -R %s. /mnt/home/%s/"%(name, name))
       return True

    def fixall():
       users = [x.strip() for x in os.popen('ls -1 /mnt/home').read().splitlines() if x.strip() != 'etc']
       i = 0
       for u in users:
           if fix(u):
               i += 1
       print "created %s accounts"%i




- [ ] storage sync: use lvm snapshot to ensure data reliability

        zpool create brick-zfs -m /mnt/brick-zfs /dev/salvus-base/tmp
        zfs set compression=gzip brick-zfs
        zfs set dedup=on brick-zfs
        gluster volume create zfs 10.1.2.5:/mnt/brick-zfs

NOTE: It *is* possible to find the node that contains a given file!

        apt-get install attr
        getfattr -n trusted.glusterfs.pathinfo storage.py

- [ ] write script to backup all the bricks to a bup repo "bricks" at 10.9.1.2

NOTE to self: there is an easy-to-install python program called bmaptool that can do sparse copy efficiently.
It is about 50% slower than just using "cp" though.
   <https://source.tizen.org/documentation/reference/bmaptool/usage/bmaptool-create>


Oh crap, I just *really* learned ZFS replication by reading the man page.  It's insanely powerful.  I also read straight through the
zfsonlinux docs again... and now I think straight ZFS could provide the ultimate possible solution.

Here's the idea.

Get rid of the gluster nodes.

Each compute node has a ZFS pool, which we add to as needed over time.  In that we have filesystems /mnt/projects/uuid, which
are the home directories for all projects on that VM.

Replication factors for projects will be defined by some rules that we choose.

Using "zfs send -R -i", i.e., *incremental* replication streams, we can -- as
efficiently as possible -- synchronized all the slave copies, including all
snapshots and clone filesystems!  This, even if /mnt/projects/uuid is quite large.

So we can eliminate all the bup snapshot stuff, and indeed end up with something much better.
We don't have to use gluster at all.
We get the speed of the local filesystem.
We get 100% robust point-of-time snapshots (no corruption).
We get transparent compression.

The drawback over gluster is that moves to a specific target VM would be slower,
since they require sending all the data.   But that's acceptable.

How would I implement this:

 - Phase 1
   - add to each compute machine a 256GB qcow2 image
   - create a ZFS pool using that image
   - migrate project accounts to it (start small, but in theory would do all)
   - switch to using this for snapshots
 - Phase 2
   - write a script that would:
      - take a project id as input
      - get lock
      - query database to find out where project is hosted
      - snapshot project's zfs dataset
      - query database to find out where each replica is
      - another script that:
         - determine most recent snapshot on replica
         - run command to send replica stream to each replicate zpool
         - on success, record updated replica information in database
   - HA:
      - when host stops working or is too heavily loaded, move to the most up-to-date
        least loaded replica that is available.  User could also initiate this,
        with option to go to least loaded node.

   - global data:
      - just replicate a dataset out to *all* nodes.

   - sharing:
      - nfs export the dataset for a particular project -- then other vm's can mount it, read/write it, etc.

TEST

    zpool create -f projects /dev/vdc
    zfs set compression=gzip projects

    zfs create projects/0df21bd2-3a26-4c59-ac0b-cdbb293c4036
    rsync -axvH --exclude .forever --exclude .bup --exclude .sagemathcloud /mnt/home/a0dLQbgp/ /projects/0df21bd2-3a26-4c59-ac0b-cdbb293c4036/


    zfs set quota=1G projects/0df21bd2-3a26-4c59-ac0b-cdbb293c4036
    zfs get quota zfs set quota=1G projects/0df21bd2-3a26-4c59-ac0b-cdbb293c4036

    zfs snapshot projects/0df21bd2-3a26-4c59-ac0b-cdbb293c4036@0
    zfs send -R projects/0df21bd2-3a26-4c59-ac0b-cdbb293c4036@0 >/tmp/0.img

    # on other node
    cat 0.img | zfs receive projects/0df21bd2-3a26-4c59-ac0b-cdbb293c4036

    # on master again, make a bunch more data.
    cd /projects/0df21bd2-3a26-4c59-ac0b-cdbb293c4036
    git clone /home/salvus/salvus
    zfs snapshot projects/0df21bd2-3a26-4c59-ac0b-cdbb293c4036@1
    zfs send -R -i projects/0df21bd2-3a26-4c59-ac0b-cdbb293c4036@0 projects/0df21bd2-3a26-4c59-ac0b-cdbb293c4036@1 >/tmp/1.img  # 2.5 s

    # need -F since I mucked with slave.
    cat 1.img | zfs receive -F projects/0df21bd2-3a26-4c59-ac0b-cdbb293c4036   # 4.5 s

    #hit quota and had to raise it:
    zfs set quota=1G projects/0df21bd2-3a26-4c59-ac0b-cdbb293c4036

    # back home
    zfs set quota=2G projects/0df21bd2-3a26-4c59-ac0b-cdbb293c4036
    zfs snapshot projects/0df21bd2-3a26-4c59-ac0b-cdbb293c4036@2
    zfs send -R -i projects/0df21bd2-3a26-4c59-ac0b-cdbb293c4036@1 projects/0df21bd2-3a26-4c59-ac0b-cdbb293c4036@2 >/tmp/2.img

    # WARNING: quota doesn't propogate with replication, so we will have to manually do that.


    # migrate me;

    time zfs create projects/3702601d-9fbc-4e4e-b7ab-c10a79e34d3b
    time rsync -axvH --exclude .forever --exclude .bup --exclude .sagemathcloud /mnt/home/teaAuZ9M// /projects/3702601d-9fbc-4e4e-b7ab-c10a79e34d3b/   # 56 seconds (twice as fast since I turned on compression)


---

Here's a replication script:  https://github.com/allanjude/zxfer
other crap:
https://github.com/leprechau/zfs-replicate
Another one: http://www.bolthole.com/solaris/zrep/


- [x] Test: manually migrate the one project DU6rFFKG@10.1.13.4    c1f1dc4a-dbf0-4fc6-9878-012020a0a829  to be hosted in a zpool on that machine.


        export project_id=c1f1dc4a-dbf0-4fc6-9878-012020a0a829; export projectid=c1f1dc4adbf04fc69878012020a0a829; export user=DU6rFFKG
        zfs create projects/$project_id
        rsync -axvH --exclude .forever --exclude .bup  /mnt/home/$user/ /projects/$project_id/
        useradd $projectid -d /projects/$project_id    # NO trailing slash!!
        chown $projectid. -R /projects/$project_id
        zfs set snapdir=visible projects/$project_id

        zfs snapshot projects/$project_id@`date +"%Y-%m-%dT%H:%M:%S"`

        # Now, in the database
        update projects set location='{"host":"10.1.13.4","username":"c1f1dc4adbf04fc69878012020a0a829","port":22,"path":"."}' where project_id=c1f1dc4a-dbf0-4fc6-9878-012020a0a829;

        # Sadly, have to restart the hub
        cloud.restart('hub', host='web18',id='3')


That worked, now try streaming replication to 10.1.2.4, as a test:


        export now=`date +"%Y-%m-%dT%H:%M:%S"`
        zfs snapshot projects/$project_id@$now
        time zfs send -R projects/$project_id@$now | ssh 10.1.2.4 zfs recv projects/$project_id

And this just works perfectly.

Now let's try the extract latest sage binary test:

        time wget http://sage.math.washington.edu/home/release/sage-5.13/sage-5.13-boxen-x86_64-Linux.tar.gz
        # takes 22 s

Now replicate:

        export now1=`date +"%Y-%m-%dT%H:%M:%S"`
        zfs snapshot projects/$project_id@$now1
        time zfs send -i $now -R projects/$project_id@$now1 | ssh 10.1.2.4 zfs recv projects/$project_id
        # takes 1m11.772s

Now extract tarball

        time tar xf sage-5.13-boxen-x86_64-Linux.tar.gz
        # takes "0m53.781s" --- that's incredible!  and the result takes 1.2GB due to compression.

Now replicate:

        export now2=`date +"%Y-%m-%dT%H:%M:%S"`
        zfs snapshot projects/$project_id@$now2
        time zfs send -i $now1 -R projects/$project_id@$now2 | ssh 10.1.2.4 zfs recv -F projects/$project_id
        # takes 4m44.103s

Delete original tarball and push over

        export now3=`date +"%Y-%m-%dT%H:%M:%S"`
        zfs snapshot projects/$project_id@$now3
        time zfs send -i $now2 -R projects/$project_id@$now3 | ssh 10.1.2.4 zfs recv -F projects/$project_id
        # 0m48.139s

        #Trivial move as user
        mv sage-5.13-boxen-x86_64-Linux/ sage-5.13

        export now4=`date +"%Y-%m-%dT%H:%M:%S"`
        zfs snapshot projects/$project_id@$now4
        time zfs send -i $now3 -R projects/$project_id@$now4 | ssh 10.1.2.4 zfs recv -F projects/$project_id
        # INSTANT


NOTE:
   bup + zfs = don't work together?!   OSError: [Errno 95] Operation not supported: '/projects/c1f1dc4a-dbf0-4fc6-9878-012020a0a829/tmp/'
   So I have to do both replication and switching project homes to zfs at the same time.




Thoughts on how to do replication:


- [x] Migration -- I might as well start it so I have some data to replicate.
     I have launched automated migration of *all* user projects right now, to zfs pools on each compute node.


- [x] switch to "zfs set compression=lz4 projects" based on what I've read.

- [x] wikipedia suggests using: "zpool set copies=2 projects" which would make it so the available disk space is halved, but data corruption of the disk image is automatically fixed.   I'm not going to do this; instead I'm going to maximize space per node, and physically replicate to other machines and data centers.  Make it so failure is graceful.

- [ ] hub support for new storage system

Then...

   Typical current location
      {"host":"10.1.11.4","username":"xdctYThX","port":22,"path":"."}
      {"host":"10.1.21.4","username":"bfa9ae97e9ff4978888940d082c84c49","port":22,"path":"."}

   HA gluster backwards compatible *enhancement*:

      {"gluster-zfs":true, 'datacenter':'padelford'}   # was last mounted at 10.1.11.4
      {"gluster-zfs":true, 'datacenter':'padelford', 'host':'10.1.11.4'}   # possibly mounted at 10.1.11.4 (but if hosts reboots this is false)


      - location=8 char username: behave as now

      - how to deploy:
          - look at projects/project_id path for each mounted gluster volume on the particular hub server.
          - take the one that is working, available, and has newest version


      - location=no username (but ip given): default the username to equal the project id with -'s removed.
      - location=none: lock, choose least-loaded vm in datacenter with newest available replica, run "project.py mount" there, set username
          wait -- add a last_location field in the database, so we know where the project was last successfully mounted.
      - what about project *creation*?


- [ ] (1:00?) ui -- button to stop a project, which unmounts it, etc..  This below "restart project server".
              don't have this for projects that have location=8 char username.

   - [x] script that creates/updates a remote replica

Testing on this from 2-->3:   ba7f993c-2313-4b0a-ac81-29fec5ff908d


- [ x] NO!!  zfs doesn't scale to a large number of datasets since mounting each one takes 1+ seconds, and if there are 10000 in a given machine... boom.  Perhaps I'm hitting this?   https://github.com/zfsonlinux/zfs/issues/845   It should be fixed in the version I'm using...?

Solutions:

   - do some tests using "gluster1" machine.
       - mount copy of compute1a image
       - see what happens on boot

       - zpool import -N projects
       - works but it SLOW

       - Try to use daily:
          change ppa, update

              apt-get remove ubuntu-zfs zfs-dkms
              # reboot!!!
              apt-get install ubuntu-zfs zfs-dkms

        Hmm, it fails to build.
        Trying the same on ubuntu 13.10 now


# ZFS

   - mount the projects on demand rather than mounting them all on reboot... since "mount -a" *will* easily take
     hours (right now 10 minutes=1000 projects), given the number of projects I plan to store per machine.

     I'm going to test gluster1 with my compute1a-projects volume and see what happens.

     For starters,

         time zfs -?

     is instant without any pool there.

  - [ ] startup:

          zpool import -Nf projects; mkdir -p /projects; chmod a+rx /projects

          Rebooting results in it taking (presumably) forever -- I killed it after 5-10 minutes -- while it is mounting
          all those datasets.  According to solaris docs:

               "To prevent a file system from being mounted, set the mountpoint property to none"

          Another thing to try is limiting the number of datasets per pool and having lots of pools.

    This sets the mountpoint and mounts it:

          zfs set mountpoint=/projects/00079e5f-98cf-4046-9dbd-f4acc3c340bb projects/00079e5f-98cf-4046-9dbd-f4acc3c340bb

    This makes it so it does *NOT* automount on boot and unmounts it

          zfs set mountpoint=none projects/00079e5f-98cf-4046-9dbd-f4acc3c340bb

  - Second, regarding replication, it seems by far the optimal strategy will be:

       - figure out the *oldest* snapshot that needs to be included in the rep stream, based on database and a local listing.
         note that including old versions in the stream is fine.
       - dump a gzip'd rep stream to /tmp/storage/project_id.img
            time zfs send -RD projects/3702601d-9fbc-4e4e-b7ab-c10a79e34d3b@2013-12-18T18:23:22 | gzip > /tmp/3702601d-9fbc-4e4e-b7ab-c10a79e34d3b.gz
       - scp it out to all the targets
       - run a "fire adn forget" zfs import on them

  - ZFS can tell exactly which files (and when) change between any given snapshots or live!

        time zfs diff -t projects/3702601d-9fbc-4e4e-b7ab-c10a79e34d3b@2013-12-18T18:23:22 projects/3702601d-9fbc-4e4e-b7ab-c10a79e34d3b

    This will be super-useful:

        - never make a snapshot and replicate, if nothing changed. this is valuable.

        - can have a new tab with "files that changed in the last 10 minutes, hour, day, week", based on snapshots and this. For this just find a fine might be faster.   Except find can do the same thing probably faster.  (http://stackoverflow.com/questions/16085958/scripts-find-the-files-have-been-changed-in-last-24-hours)



I just saw this claim: "To avoid this problem I'd suggest setting them up using the Linux automounter so everything gets mounted on demand. "


# (solved) ZFS non-scaling lameness -- https://github.com/zfsonlinux/zfs/issues/845

I'm going to try building debs from source.  If this works, I'll then maybe try hacking on the source.

    ubuntu-zfs zfs-dkms  zfsutils apt-get removespl-dkms

    apt-get install build-essential gawk alien fakeroot linux-headers-$(uname -r)
    apt-get install zlib1g-dev uuid-dev libblkid-dev libselinux-dev parted lsscsi wget

    wget http://archive.zfsonlinux.org/downloads/zfsonlinux/spl/spl-0.6.2.tar.gz
    wget http://archive.zfsonlinux.org/downloads/zfsonlinux/zfs/zfs-0.6.2.tar.gz

    tar -xzf spl-0.6.2.tar.gz
    tar -xzf zfs-0.6.2.tar.gz

    cd spl-0.6.2
    ./configure
    make deb-utils deb-kmod
    dpkg -i kmod-spl-devel_0.6.2-1_amd64.deb kmod-spl-devel-3.2.0-56-generic_0.6.2-1_amd64.deb

    cd ../zfs-0.6.2

    dpkg -i */*.deb
    ./configure
    make deb-utils deb-kmod
    cd ..

    dpkg -i */*.deb





# Install the spl development headers, they are required to build zfs.
$ sudo dpkg -i \
	kmod-spl-devel-x.y.z-r.arch.deb \
	kmod-spl-devel-kernel-x.y.z-r.arch.deb

$ cd ../zfs-x.y.z
$ ./configure
$ make deb-utils deb-kmod


---


- [x] (0:30?) (3:00) add hashring to storage.py: python implementation sucks... so I had to rewrite it to have same features as node.js one

    Python:
    r = hashring.HashRing({'10.1.1.4':{'vnodes':256}, '10.1.2.4':{'vnodes':128}, '10.1.3.4':{'vnodes':128}})

    Node:
    r = new HashRing({'10.1.1.4':{vnodes:256}, '10.1.2.4':{vnodes:128}, '10.1.3.4':{vnodes:128}});0;

 - [x] (0:30?) create topology file and push out to /home/storage/topology
 - [x] make a replicate --all option and run it everywhere...

  - [x] send on lots of projects and see how it goes...




# Next update of base vm image
- [x] change timezone to UTC
- [x] update zfs with my new images (i.e., build from source and comment out the line at https://github.com/zfsonlinux/zfs/issues/845)
- [x] create storage user from one existing setup
- [x] add visudo:
           storage ALL=(ALL) NOPASSWD: /sbin/zfs *
- [x] apt-get install tcl-dev tk-dev libcppunit-dev libpopt-dev
- [x] umask 022; sage -f python; sage -i http://snappy.computop.org/get/snappy-2.0.3.spkg
- [ ] confirm that /projects gets mounted at boot time and first_boot.py works correctly, disabling storage account on non-compute

# Next update google

- [x] apt-get install bsdtar attr tcl-dev tk-dev  libcppunit-dev libpopt-dev
- [x] change timezone to UTC:  dpkg-reconfigure tzdata
- [x] create storage user with
            storage ALL=(ALL) NOPASSWD: /sbin/zfs *
- [x] sage -i http://snappy.computop.org/get/snappy-2.0.3.spkg
- [x] umask 022; sage -f python

- [ ] setup zfs pool
        zpool create -f projects /dev/sdc ?????????
        zfs set compression=lz4 projects
        zfs set atime=off projects
        zfs set mountpoint=none projects
        # zfs set dedup=on projects ## NOT sure about this; haven't tried yet, will need to test...



## Discussion
I now have migrate.py send, which can update the remote data for a project.

(For failover and better robustness, I should add "migrate.py sync",
and have it resolve conflicts, merge, etc., in case something funny temporarily
happened and a project got opened on two nodes at the same time.   But that is
for later.)

Deciding *where* a project is replicated can be done with consistent hashing
and virtual nodes. This seems like the best approach, though the drawback is
that it isn't clear how to factor load balancing based on usage into this.
For the replicas load balancing doesn't matter.  For the master it does.

One idea would be that the *replicas* are all located using consistent hashing+vnodes,
but the live one is separate from that (though with a preference).

Failover -- this will require a gossip protocol and all kinds of stuff.  However, the
very *first* version could be done via a button the user explicitly pushes.

So we do this:

   - make a topology table

        dc0 compute_ip_address
        dc0 compute_ip_address
        ...

   - add a hashring implementation to storage.py (https://pypi.python.org/pypi/hash_ring) and hub (https://github.com/3rd-Eden/node-hashring)
     we use a separate hashring for each data center, of course.

   - labels: for a given project, define the identifiers for where the replicas are located as follows:

        project_id:0dc0
        project_id:1dc0
        project_id:2dc0
        ...
        project_id:0dc1
        project_id:1dc1
        project_id:0dc2
        project_id:1dc2

   - in the projects table of the database instead of the hostname we use [n]dc[i] to mean "the nth replica in data center i", which via
     hashring everybody can determine.

   - add code to storage.py that -- given a project -- will replicate it to project_id:[j]dc[i] for j=0,1 and i=0,1,2 (say).  We could
     start with just i=0,1 for now.

   - final migration and switch will involve locking out project temporarily, one last update of the project, one more migrate, and then
     changing database table to use new thing.

   - after everything is up and running, we'll go through and delete old project filesystems that aren't needed.
     This is a sort of repair operation.

   - a little ugly: in longterm if we want to have a project run on a particular node, we could just hash project_id:[n]dc0 for increasing n until the result is the requested node.

   - replacing a node will stream in data in parallel from everybody else, and should work well.

   - adding a new node: have to stream all the data too it before actually starting to use it.
     for each project id, find out its locations
     if one of them is on the new node, use the live version to send it to the new node.
     once this is done (and stable), switch the topology maps everywhere to use the new node list.
     later, (in repair as above), remove data from nodes that doesn't need to be there anymore,
     except if it is a live project.

----

Is consistent hashing worth it?  I'm not sure.  It has two advantages:

   - very low latency: this doesn't matter given the application.

   - no central directory service: very nice, since it means the compute nodes can easily replicate by themselves without needing central database access and without needing any external control.  This makes replication faster and more event driven.

Drawbacks:

   - inflexible load balancing.  projects are located randomly rather than spread out based on historical usage.
     and its hard for a user to put their project just anywhere.

   - must rebalance live projects.  that's pretty painful.

Hybrid:

   - how about if we do this.  There's a live location exactly like right now.
     Then there is replication to k = n other nodes in each data center
     based on consistent hashing.  Here we take
         project_id-0
         project_id-1
     until we find k nodes that are different than the live node, in the given data center.
     In particular, if a given node wants to replicate out a given project, it just
     computes the hash of  project_id-0,  project_id-1, etc., until the first time it
     finds a node that isn't itself, and pushes there.

     If whenever a project filesystem changes in some way (either by recv or user edits),
     a node then snapshots and pushes out to one replica *if possible* (prefering the local data center)
     then at the end everything gets replicated efficiently with mininimal traffic across
     data centers.

     Using "zpool history" the storage daemon can systematically parse through and find everything
     that has happened recently.  Also using "ps aux | awk '{ print $1 }' | sed '1 d' | sort | uniq"
     we can see which accounts are running code... or just look for running local_hub's.  And for
     those, check if interesting files have changed, and if so, make a snapshot (up to a minimum timeout).

     This way we have 2 or 3 copies in the live data center and 2 in the others.  That sounds good.
     It's basically a gossip replication protocol.
     Time is spent doing each sync, but this is mostly io bound, so we can do a lot in parallel.

     Clean up will mean going through systematically and deleting unused replicas from either old
     lives or from adding new nodes and rebalancing.

     Adding a new node:
         - all other nodes in same data center will get the new topology file,
           then for each project they host (according to the old topology file, so live or old data is excluded!)
           such that the sync dest is the new node,  they run that sync.
           When all nodes complete then each project sync happened exactly once, and the new node is fully loaded.
           Then we switch to the new topology file everywhere across all data centers and database.
           Live is independent of all this, so won't be messed up.

     Project move:
       - give user option of "instant" or their choice of node (with a time estimate).




Total usage right now across all zpools = 723GB.

    In [9]: %time v =[float(storage.cmd('ssh 10.1.%s.4 "sudo zpool list"'%i).splitlines()[1].split()[2][:-1]) for i in range(1,8) + range(10,22)]
    In [8]: sum(v)
    Out[8]: 723.1

so with my above strategy this will require between 1.5 and 2.2TB total storage at each data center.
At GCE prices, that's "Provisioned space	$0.04 GB / month" = $80/month for this.


Bups of just home:

    salvus@smc-backup-2:~$ du -sch bup-home
    180G    bup-home
    180G    total

Usage of /mnt/home:

    %time v =[float(storage.cmd('ssh 10.1.%s.4 "df -h /mnt/home"'%i).splitlines()[1].split()[2][:-1]) for i in range(1,8) + range(10,22)]
    sum(v)
    2183.0

---


   - [ ] define database schema with replication info, e.g., maybe just add something to projects table (?):
         - master location
         - slave replicas with for each replica, timestamp of last successful update

        http://www.datastax.com/dev/blog/cql3_collections

        ALTER TABLE projects ADD replicas map<text, timestamp>;

        UPDATE projects SET replicas = {'10.1.3.4':'2013-12-18T01:15:58+0000'} WHERE project_id = ba7f993c-2313-4b0a-ac81-29fec5ff908d;
        UPDATE projects SET replicas['10.1.10.4']='2013-12-18T01:15:58+0000' WHERE project_id = ba7f993c-2313-4b0a-ac81-29fec5ff908d;
        UPDATE projects SET replicas['10.1.11.4']='2013-12-18T01:15:58+0000' WHERE project_id = ba7f993c-2313-4b0a-ac81-29fec5ff908d;

        UPDATE projects SET replicas['10.1.11.4']='2013-12-17T23:02:49+0000' WHERE project_id=f7c11e0e-69ba-4bb3-ab63-e28d262220ae

        # Just in case I need to parse a date
        # datetime.datetime.strptime('2013-12-18T03:20:42', '%Y-%m-%dT%H:%M:%S')



  - [x] redo base salvus with patched zfs; test and snapshot it.

  - [x] (0:45?) (1:40) provision four single-core gce nodes with ZFS setup; 512GB each; -- these will be computes1-4.

  - [x] (0:30?) (0:03) initiate push of all the data to GCE.

  - [x] (1:00?) make a daemon started from admin.py (which pushes out the storage/topology file), and monitors zpool history and sends each project with newly *created* snapshots on this node (created by the zfs snapshot command).

  - [x] (1:00?) make daemon that periodically snapshots and replicates active projects

  - [x] storage: replicate many should send kill signal finally on all pids that are still running.


  - [x] GCE: switch to four machines with 4 cores each; [x] send a cpu quota increase form;
        restart the replication; rebuild atlas with 4-core support.

Local:
 - [x] sage -sh; umask 022; pip install pyinotify
 - [x] add to /etc/sysctl.conf: # fs.inotify.max_user_watches=10000000


 - [x] storage: sending replication via streaming over ssh is idiotic, dangerous and leads to deadlocks -- only recoverable via a reboot! -- when anything goes wrong with the ssh connections.   So change to scp the rep stream, apply, then delete it. It's the only safe way to go.
- [x] storage: sending replication via streaming over ssh is idiotic, dangerous and leads to deadlocks -- only recoverable via a reboot! -- when anything goes wrong with the ssh connections.   So change to scp the rep stream, apply, then delete it. It's the only safe way to go.


 - [x] global synchronization -- no matter what, we need functionality that computes the latest snapshots name for all projects... it's built into ZFS!

   With over 7000 projects, this took under 30 seconds, and it is a list of all snapshots.

         time sudo zfs list -r -t snapshot -o name -s creation projects > b

   I should have a global "repair" command that:

     - gets the list of all snapshots on all nodes in the cluster:
           probably this is 10 minutes...?  But "less than an hour" is fine.
           A slightly old cache is not a problem for this.
     - computes from this, using the consistent hash table, all operations
       between all nodes are needed to ensure all data is where it should be.
       This includes:
           - sends
           - destroys

  - [x] (0:58) write "./storage.py update_snapshot_cache", which will run above zfs list command,
       parse the output, and store it as a file "~/snapshot_cache.json".

  - [x] (3:04) storage: function that loads all the snapshot cache files and computes all work that needs to be done to bring everything into sync, and a function that does that work. First usable version.; Fully test out on compute13a. 100% completed with no problems after 1 hour.  WOW.

  - [x] (0:30) (1:18) storage: only snapshot if there are actual changes since the last snapshot.

  - [x] (0:55) storage: re-update/migrate a node and see how that goes --> wrote a "repair" command to make this easy.

  - [x] storage: create a quota command, which increased the quota on *all* copies of the project; then test it on e89567b4-02f6-4947-9809-ac24e64ebbd8 on 10.1.1.4 (and any other big ones?), which is a MRC project.

  - [x] storage: remove use of gzip for compressing the stream; it slows things down a lot and defeats the automatic compression in. The ssh page says compression "will only slow down things on fast networks".

  - [x] (0:40) storage: in repair, only ensure that there is at least one copy in each *other* data center, but all in self's data center.  Then, once we go around and run repair on all other nodes, we will only ever send a given update from one DC to another once during repair, but all will get replicated around.
  - [ ] get all data in current state properly replicated out to all machines: 'repair'
  - 'repair'

    Give this crazy script a try as I go to sleep...

        #!/usr/bin/env python

        import os

        v=[2,3,4,5,6,7,10,11,12,13,14,15,16,17,18,19,20,21]

        addr = ['10.1.%s.4'%i for i in range(2,8) + range(10,22)] + ['10.3.%s.4'%i for i in range(1,4)]
        print addr

        def cmd(s):
            print s
            os.system(s)

        for x in addr:
            cmd('ssh storage@%s "./storage.py --loglevel=debug repair"'%x)

      FAIL?

   Thoughts:
      - thoughts?  It seems that anything "global" involving ZFS is potentially really slow and non-scalable.
      - ZFS is not a database, so not query-able
      - If I switch to a node/cassandra based approach, I'll have to make sure that *all* operations on the fs
        go through that, and that state changes (new versions) are recorded only when they are really made.
      - Latency and load shouldn't be a big issues, since filesystem snapshotting isn't that frequent, really.
      - Need a global view of the system in order to delete old snapshots, allow for multiple node failures, etc.
      - Python sucks for this sort of stuff.

-------

   Can I write something that does everything I now know I need in a few hours using Cassandra + Node?

   Database that records:

      - health of each compute storage node

      - all snapshot versions available on each node for each project (so can generate strategy to sync, can systematically delete old snapshots, resolve conflicts, etc.)

      - quota of each project on each node (should be the same, but) -- use project's "quota" field.

      - any launched replications tasks, including from, to, pid, time started, completion status,
        so we can generate a global view of them all

  API:

      - get/set compute health: already in compute_server table
      - get available snapshots for a project on a given node:
      - get nodes that have a project, with latest snapshot version
      - get quota (including actual usage)
      - create snapshot
      - delete snapshot
      - replicate project
      - bootstrap a new node
      - remove a node
      - repair
      - rebuild snapshot list from filesystem periodically, just in case (?)


    ALTER TABLE projects ADD locations map<text, text>;   /* ip_address --> list of snapshots available there */
    ALTER TABLE projects ADD storage_tasks map<uuid,text>;   /* active storage tasks: task_id --> description */


    ALTER TABLE compute_servers ADD virtual_nodes int;
    ALTER TABLE compute_servers ADD data_center   int;    /* 0=padelford, 1=4545, 2=GCE-us-central-1, .... */



  - (1:30?) [x] (1:40) populate the locations and snapshots entries of the projects table of cassandra for all projects.

     *- approach 1: write a sequence of update table statements to a file, then read it in
          (do 1, since there is no point building on these cache/json files further, since will get rewritten)
      - approach 2: parse all the cache/*.json files in a node program and send queries to the db.
      - approach 3: parse all the cache/*.json files in a python program and send queries to the db.

I had to import on the command line this way, since piping in resulted in a timeout after 10 seconds:

        salvus@cloud3:~$ PW=`cat $HOME/salvus/salvus/data/secrets/cassandra/salvus`
        salvus@cloud3:~$ time cqlsh -k salvus -u salvus -p $PW 10.1.3.2 -f out.cql
        # <40 seconds

  - [x] (1:00?) (3:00) storage in coffescript:  snapshot a project; ended up also doing a lot more, e.g., deleting snapshots

  - [x] (1:00?) (5:49) storage in coffescript:  replicate a project; add topology, repair snapshots.

    UPDATE storage_topology set vnodes=256 where data_center='0' and host='10.1.1.4';
    UPDATE storage_topology set vnodes=256 where data_center='0' and host='10.1.2.4';
    UPDATE storage_topology set vnodes=256 where data_center='0' and host='10.1.3.4';
    UPDATE storage_topology set vnodes=256 where data_center='0' and host='10.1.4.4';
    UPDATE storage_topology set vnodes=256 where data_center='0' and host='10.1.5.4';
    UPDATE storage_topology set vnodes=256 where data_center='0' and host='10.1.6.4';
    UPDATE storage_topology set vnodes=256 where data_center='0' and host='10.1.7.4';
    UPDATE storage_topology set vnodes=256 where data_center='1' and host='10.1.10.4';
    UPDATE storage_topology set vnodes=256 where data_center='1' and host='10.1.11.4';
    UPDATE storage_topology set vnodes=256 where data_center='1' and host='10.1.12.4';
    UPDATE storage_topology set vnodes=256 where data_center='1' and host='10.1.13.4';
    UPDATE storage_topology set vnodes=256 where data_center='1' and host='10.1.14.4';
    UPDATE storage_topology set vnodes=256 where data_center='1' and host='10.1.15.4';
    UPDATE storage_topology set vnodes=256 where data_center='1' and host='10.1.16.4';
    UPDATE storage_topology set vnodes=256 where data_center='1' and host='10.1.17.4';
    UPDATE storage_topology set vnodes=256 where data_center='1' and host='10.1.18.4';
    UPDATE storage_topology set vnodes=256 where data_center='1' and host='10.1.19.4';
    UPDATE storage_topology set vnodes=256 where data_center='1' and host='10.1.20.4';
    UPDATE storage_topology set vnodes=256 where data_center='1' and host='10.1.21.4';
    UPDATE storage_topology set vnodes=256 where data_center='2' and host='10.3.1.4';
    UPDATE storage_topology set vnodes=256 where data_center='2' and host='10.3.2.4';
    UPDATE storage_topology set vnodes=256 where data_center='2' and host='10.3.3.4';
    UPDATE storage_topology set vnodes=256 where data_center='2' and host='10.3.4.4';

    -------

  - [x] (0:30?) (0:32) storage2: delete project from host

  - [x] (1:00?) (0:51) storage2: replicate all - implement version in storage2 and start running.
         - just query for all project id's, iterate over each, and run replicate.
           could do several in parallel or not.

  - [x] (0:30?) storage: add a field in the database to indicate new-style project

         ALTER TABLE projects ADD storage varchar;
         update projects set storage='zfs' where project_id=c1f1dc4a-dbf0-4fc6-9878-012020a0a829;


- [x] testing changing some timeout params on hub15 -- if terminals stop hanging, roll that out.


- [x]
   Wow, that was really stupid of me to use Python's "hash" as part of the user id function!

      n = hash(hashlib.sha512(uuid).digest()) % (4294967294-1000)    # 2^32-2=max uid, as keith determined by a program + experimentation.
      return n + 1001

   Better would be something simple that I can also implement in node.  But making this change will require
   mounting, chmoding, resyncing, etc. every single project! Ugh.  That would take a while and will also mean
   that the first snapshot is not user-accessible, which is a little weird.

   Options:
      *-* stick with having a python-only implementation; there is really no turning back!
      - change to a more straightforward definition and rerun on everything
      - implement python hashing in node -- can always do later: http://stackoverflow.com/questions/2070276/where-can-i-find-source-or-algorithm-of-pythons-hash-function



  - [x] (0:45?) (1:45) storage2: self-healing set/get quota... across all hosts for the project.

  - [x] (0:30?) (1:31 due to scope increase) storage2: get current usage and quota also store in database

            ALTER TABLE projects ADD usage_zfs varchar;
            ALTER TABLE projects ADD quota_zfs varchar;

  - [x] (1:00?) migrate: amazingly the node.js script to do everything ran no problem to completion :-)
        However, there were 35 errors.  Look at them. They are in the file "y" on 10.3.1.3.

I'm re-migrating fc204d2c-1fbb-4726-b625-1ff1a45ef2f4 on 10.1.6.4 since it hits 10G and I did it before I had enabled dedup.  I'm curious what dedup does.  It is the only one where the quota error got hit.  Re-dedup made little difference... not sure how dedup really works behind the scenes.

    # This should take *HOUR+*.
    r=require('storage'); x={};r.replicate(project_id:'fc204d2c-1fbb-4726-b625-1ff1a45ef2f4', cb:(e,r)->x.e=e;x.r=r;console.log("DONE",x))

        Most of the other issues are because the project isn't migrated yet.
        Four of them are mysterious "dataset" not available zfs locks that just don't go away.  ARGH.  This could prove very painful.
        After the big transfer finishes, reboot 10.3.1.4 and retry them; that will workill workill work.

  - [x] (1:00?) (0:47) by controlling the storage machines from the hubs, I can get rid of the ssh equivalence between storage nodes, which cuts the attack surface a lot...   I could just get rid of the storage account and visudo the salvus account to run "zfs".

       - [x] (0:15?) (0:16) setup sudo zfs and sudo create_project_user.py for salvus user:
       - [x] (0:10?) (0:04) mkdir ~/storage directory for salvus user -- where temporary files are stored
       - [x] (0:10?) modify storage.coffee to reflect these changes

   - [x] (0:30?) (0:49) UGH. I have to have the storage user for security reasons, since there has to be a way to directly send the ZFS streams
    between compute nodes.   Add it back.  WASTE OF TIME !
        - added back in base image.
        - now I'll restart one compute with it, make sure all the first boot stuff works, then push it out to all other machines.


  - [x] (0:20) create_project_user -- make it work

  - [x] (1:00?) (1:52) storage2: open a project --
          - mount filesystem
          - create user (with auto-heal of ~/.ssh)
          - fix .ssh/authorized_keys, if necessary
          - test login works
  - [x] (0:45?) (0:45) storage2: close a project --   (later this will involve setting cgroups, maybe lxc stuff, etc.)
          - kill all procs by that user
          - umount filesystem

  - [x] (0:45?) (2:27) storage2: create project --
          - create filesystem
          - copy over a template into it
          - add to database
