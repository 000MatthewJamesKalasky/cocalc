# Next release (base vm update)

   - [x] apt-get autoremove; apt-get clean
   - [ ] install gluster 3.4.1: http://download.gluster.org/pub/gluster/glusterfs/3.4/3.4.1/Ubuntu/Ubuntu.README
          add-apt-repository ppa:semiosis/ubuntu-glusterfs-3.4
          apt-get update
          apt-get install glusterfs-client
   - [ ] zfs support
          apt-add-repository --yes ppa:zfs-native/stable
          apt-get update; apt-get install ubuntu-zfs  # takes a long time!
   - [ ] update salvus repo
   - [ ] update system-wide bup from git repo.
   - [ ] update create_unix_user.py


# Top priority GOALS

- [ ] project listing search bug: put "crem" in search box, then change to a project then change back to project list, and the cremona project vanishes.

- [ ] delete contents of ~/.forever on start_smc (?) -- it's 700+ MB for my devel project!!

- [ ] wild idea: have a slow distributed redundant "cloud filesystem" of newest version of all projects, which is distributed among all machines, with redundancy.  It is *only* used for project move and a "nearly live latest backup", but not run off of.  Active projects get rsync'd to it.  Either this exists already (in a form that doesn't completely suck), or I can write something that accomplishes the same thing, which is *organized* on top of cassandra (?).  Lay out the rules and try.

  - Researching this.  Looking at *gluster*.

  sudo apt-get install glusterfs-server
  sudo gluster peer probe cloud2.math.washington.edu
  sudo gluster peer probe cloud1.math.washington.edu  # on cloud2
  sudo gluster volume create testvol replica 2 transport tcp cloud1.math.washington.edu:/tmp/data cloud2.math.washington.edu:/tmp/data
  sudo mkdir /mnt/glusterfs  # on both machines
  mount -t glusterfs cloud1.math.washington.edu:/testvol /mnt/glusterfs
  How to setup so that each datacenter has two copies of files: http://gluster.org/pipermail/gluster-users/2011-March/029810.html

- [ ] make a big peer network of all my host vm's using /home/salvus/vm/images/gluster

        sudo gluster peer probe 10.1.2.1; sudo gluster peer probe 10.1.3.1; sudo gluster peer probe 10.1.4.1; sudo gluster peer probe 10.1.5.1; sudo gluster peer probe 10.1.6.1; sudo gluster peer probe 10.1.7.1; sudo gluster peer probe 10.1.10.1; sudo gluster peer probe 10.1.11.1; sudo gluster peer probe 10.1.12.1; sudo gluster peer probe 10.1.13.1; sudo gluster peer probe 10.1.14.1; sudo gluster peer probe 10.1.15.1; sudo gluster peer probe 10.1.16.1; sudo gluster peer probe 10.1.17.1; sudo gluster peer probe 10.1.18.1; sudo gluster peer probe 10.1.19.1; sudo gluster peer probe 10.1.20.1; sudo gluster peer probe 10.1.21

### Projects

' '.join(['10.1.%i.1:/home/salvus/vm/images/gluster/projects'%i for i in [1,10,  2,11,  3,12,  4,13,  5,14,  6,15,  7,16]])

        sudo gluster volume create projects replica 2 transport tcp 10.1.1.1:/home/salvus/vm/images/gluster/projects 10.1.10.1:/home/salvus/vm/images/gluster/projects 10.1.2.1:/home/salvus/vm/images/gluster/projects 10.1.11.1:/home/salvus/vm/images/gluster/projects 10.1.3.1:/home/salvus/vm/images/gluster/projects 10.1.12.1:/home/salvus/vm/images/gluster/projects 10.1.4.1:/home/salvus/vm/images/gluster/projects 10.1.13.1:/home/salvus/vm/images/gluster/projects 10.1.5.1:/home/salvus/vm/images/gluster/projects 10.1.14.1:/home/salvus/vm/images/gluster/projects 10.1.6.1:/home/salvus/vm/images/gluster/projects 10.1.15.1:/home/salvus/vm/images/gluster/projects 10.1.7.1:/home/salvus/vm/images/gluster/projects 10.1.16.1:/home/salvus/vm/images/gluster/projects

        sudo gluster volume set projects auth.allow 10.*

It turns out that the above is really slow?!

// on a client vm:

root@compute1a:/mnt/projects# sync; dd bs=1M count=128 if=/dev/zero of=test conv=fdatasync; sync
128+0 records in
128+0 records out
134217728 bytes (134 MB) copied, 11.334 s, 11.8 MB/s
root@compute1a:/mnt/projects# time ls teaAuZ9M/>/dev/null
real    0m0.190s

That's exactly my network speed, given TINC.

On bare metal:

salvus@cloud1:~/vm/images/gluster$ sync; dd bs=1M count=128 if=/dev/zero of=test conv=fdatasync; sync
128+0 records in
128+0 records out
134217728 bytes (134 MB) copied, 2.00069 s, 67.1 MB/s



Try again, but with no encryption in simplest setup:

sudo gluster volume create testvol replica 2 transport tcp cloud1.math.washington.edu:/home/salvus/vm/images/gluster/testvol cloud2.math.washington.edu:/home/salvus/vm/images/gluster/testvol

sudo gluster volume create testvol replica 2 transport tcp 10.1.1.1:/home/salvus/vm/images/gluster/img 10.1.1.2:/home/salvus/vm/images/gluster/img

Extracting sage with a mounted client on a vm took this long:  11m43.856s

The dd test on the compute vm is FAST -- almost same as local disk: 134217728 bytes (134 MB) copied, 2.21882 s, 60.5 MB/s
So the time diff might be latency.


 * Info about using ssl encryption with glusterfs: http://nongnu.13855.n7.nabble.com/Glusterfs-SSL-capability-td168156.html
 * Make xfs .img loopback device: http://www.mail-archive.com/gluster-users@gluster.org/msg08360.html

Something to test:

   - create a loopback device on top of glusterfs: how does it perform?

   dd of=test.img seek=1024 bs=1M count=0
   losetup /dev/loop1 test.img
   losetup -a | grep loop1
   mkfs.ext4 /dev/loop1
   mount -o loop /dev/loop1 /mnt/test

Wow, this is *amazing* -- it completely solves the latency issue and provides exactly what I really need.

Problem: Do the above, but with a qcow2 image.

    modprobe nbd max_part=63
    qemu-img create -f qcow2 test.qcow2 10G

    qemu-nbd -c /dev/nbd0 `pwd`/test.qcow2  # exact path is critical!
    fdisk /dev/nbd0
    mkfs.ext4 /dev/nbd0p1

    mount /dev/nbd0 /mnt/test2

Later

    qemu-nbd -d /dev/nbd0

This seems very slow and flakie.

### this works very nicely, and doesn't waste space.  GOOD.

    #dd if=/dev/zero of=sparse.img bs=1 count=0 seek=10G
    truncate -s 10G sparse.img
    losetup /dev/loop2 sparse.img

#### To re-sparsify

    time cp --sparse=always sparse.img sparse2.img


### Problem: Format sparse image using zfs -- try to compressed and de-duplicate

(best ZFS docs: https://pthree.org/2012/12/19/zfs-administration-part-xii-snapshots-and-clones/)

    apt-add-repository --yes ppa:zfs-native/stable
    apt-get update; apt-get install ubuntu-zfs  # takes a long time!

    truncate -s 10G sparse-zfs.img
    losetup /dev/loop1 sparse-zfs.img

    mkdir /mnt/testzfs
    zpool create -m /mnt/testzfs rpool /dev/loop1
    zfs set compression=gzip-9 rpool
    zfs set dedup=on rpool
    zfs get compressratio rpool; zpool get dedupratio rpool
    zfs set quota=5G rpool

    zfs snapshot rpool@s2

    zpool destroy rpool

Proposal:

   - (LATER!?) I create a big glusterfs like I did this morning, which is distributed across the tinc VPN, with
     say two copies of each file per datacenter (with 3 data centers: 4545, padelford, gce).

   - For each project, create a sparse zfs image file
               project-id.img
     setup as above with ZFS on it, and rsync in that project.
     The name of the pool will be the project id.


   - When a project runs somewhere, it mounts the project-id.img using a loopback device.
     (NOTE on loopback device limits -- http://old.slax.org/documentation_loop_mount.php)


### TEST

        truncate -s 128G 3702601d-9fbc-4e4e-b7ab-c10a79e34d3b.img
        losetup /dev/loop1 3702601d-9fbc-4e4e-b7ab-c10a79e34d3b.img
        mkdir /mnt/3702601d-9fbc-4e4e-b7ab-c10a79e34d3b
        zpool create -m /mnt/3702601d-9fbc-4e4e-b7ab-c10a79e34d3b  pool-3702601d-9fbc-4e4e-b7ab-c10a79e34d3b /dev/loop1
        zfs set compression=gzip pool-3702601d-9fbc-4e4e-b7ab-c10a79e34d3b
        zfs set dedup=on pool-3702601d-9fbc-4e4e-b7ab-c10a79e34d3b
        zfs set quota=4G pool-3702601d-9fbc-4e4e-b7ab-c10a79e34d3b
        zfs get compressratio pool-3702601d-9fbc-4e4e-b7ab-c10a79e34d3b; zpool get dedupratio pool-3702601d-9fbc-4e4e-b7ab-c10a79e34d3b


Wait, ZFS is too clever -- no need for loopback devices craziness!

    truncate -s 4G zfs-file.img
    cd /mnt/glusterfs/x/
    zpool create -m /mnt/test2 test `pwd`/zfs-file.img

    zpool set feature@lz4_compress=enabled test; zfs set compression=lz4 test
    zfs set dedup=on test
    zfs get compressratio test; zpool get dedupratio test

    zfs umount /mnt/test2
    zfs mount -a

    zpool export test

    zpool import test -d /mnt/glusterfs/x/

    # Add new images files to pool?  Yep.  So I can easily expand the available space for a project.  So no *need* to use zfs quota.
    truncate -s 4G zfs-file-2.img
    zpool add test /mnt/glusterfs/x/zfs-file-2.img
    # and instantly there is more space.  NICE.

Note -- if user deletes a lot of data off img, it does *NOT* get smaller.  However, when offline, one can try the following to shrink it, but it DOES NOT WORK:

    time cp --sparse=always 0.img 1.img

Another approach: add a new file, then remove the old one?  Nope.  Worry about this later, since there is always the obvious *and optimal* solution of just making a new img, since that goes through and recompresses and dedups everything...

    SUCKs.

    # this is very fast locally
    time cp -rv --sparse=always projects/* /mnt/projects/

    # So, if I can somehow mount -- say via nfs and a tunnel or something !? -- the glusterfs volume, then we can probably do it fine.



- [ ] convert some of the 18262 projects (maybe the 2356 on compute1a?) to img files in the following format, with the user having uid/gid 1001

        glusterfs volume "projects"
            projects/
                project-id/
                   0.img
                   ...  # reserved for "other stuff" -- not sure what -- later
                project-id/
                   ..

      - [ ] write python script that automates making the project-id image thing above.

                 project2zfs.py  /mnt/home/foo  /path/to/projects/

          - [x] first version: assumes /path/to/projects/project-id doesn't exist; makes it, copies over files, sets uid and gid
          - [x] next version: if /path/to/projects/project-id exists, mounts it, then rsync updates
          - [x] version that works on multiple input paths

- [x] (0:37) write script project_storage.py subcommand that uses db and goes through and adds the info.json files to *all* project home on a given vm, if they aren't there already.

- [x] run above conversion script on all projects.

- [ ] test/finish my LXC container system in a way that can mount and run users using one of the
      above zfs images and doesn't randomly destroy all my work!  Hopefully this doesn't
      require changing everything...

- [ ] when things look good, figure out how to make this global.


   - OPTION 1 -- global glusterfs:
     create glusterfs with 1 replicas in each DC and 12 bricks in each of these data centers (so replication factor=3)

           padelford              1   2   3   4   5   6   7  1b  2b  3b  4b  5b
           4545                  10  11  12  13  14  15  16  17  18  19  20  21
           gce-us-central-1     u1a u2a u1b u2b u1c u2c u1d u2d u1e u2e u1f g2f

         ##  gce-europe           e1a e2a e1b e2b e1c e2c e1d e2d e1e e2e e1f e2f   <--- too slow (?)

        I wonder -- will this make filesystem way slower (?) -- if so, we have a major problem with the whole design,
        and would have to pivot yet again :-(.

        Let's do a test:

            sudo gluster volume create projects0 replica 2 transport tcp 10.1.1.1:/home/salvus/vm/images/gluster/projects0 10.1.10.1:/home/salvus/vm/images/gluster/projects0

            sudo gluster volume set projects0 auth.allow 10.1.*

            That works *fine*.

   - OPTION 2 -- glusterfs inside each DC

     - create one glusterfs with no encryption/vpn inside each datacenter; replication factor = 2 (?)

     - write something to sync securely over the internet via rsync, where we sync back and forth with the newest file winning.  Since the files are these images, there is just one file, and conflict resolution is super easy.

Try this out right now on cloud3, cloud4:

sudo gluster volume create projects replica 2 transport tcp cloud3:/home/salvus/vm/images/gluster/projects/ cloud4:/home/salvus/vm/images/gluster/projects/

    OK, this is dramatically better.  Super, super good.  Wow.  WOW!


So here's the plan:

        - setup a non-encrypted replication factor=2 glusterfs in each data center, *including* tower.
          (worry about firewall and auth guest restriction.)

1,2
3,4,5,6,7
10,11,12,13,14,15,16,17,18,19,20,21

Do it:


        # on cloud1
        gluster peer probe cloud2
        gluster volume create projects replica 2 transport tcp cloud1:/home/salvus/vm/images/gluster/projects/ cloud2:/home/salvus/vm/images/gluster/projects/

        # on cloud3
        gluster peer probe cloud4; gluster peer probe cloud5; gluster peer probe cloud6; gluster peer probe cloud7

        gluster volume create projects replica 2 transport tcp cloud3:/home/salvus/vm/images/gluster/projects/ cloud4:/home/salvus/vm/images/gluster/projects/ cloud5:/home/salvus/vm/images/gluster/projects/ cloud6:/home/salvus/vm/images/gluster/projects/ cloud7:/home/salvus/vm/images/gluster/projects/ cloud3:/home/salvus/vm/images/gluster/projects-1/

        # cloud10
        gluster peer probe cloud11;gluster peer probe cloud12;gluster peer probe cloud13;gluster peer probe cloud14;gluster peer probe cloud15;gluster peer probe cloud16;gluster peer probe cloud17;gluster peer probe cloud18;gluster peer probe cloud19;gluster peer probe cloud20;gluster peer probe cloud21

        gluster volume create projects replica 2 transport tcp cloud10:/home/salvus/vm/images/gluster/projects/ cloud11:/home/salvus/vm/images/gluster/projects/ cloud12:/home/salvus/vm/images/gluster/projects/ cloud13:/home/salvus/vm/images/gluster/projects/ cloud14:/home/salvus/vm/images/gluster/projects/ cloud15:/home/salvus/vm/images/gluster/projects/ cloud16:/home/salvus/vm/images/gluster/projects/ cloud17:/home/salvus/vm/images/gluster/projects/ cloud18:/home/salvus/vm/images/gluster/projects/ cloud19:/home/salvus/vm/images/gluster/projects/ cloud20:/home/salvus/vm/images/gluster/projects/ cloud21:/home/salvus/vm/images/gluster/projects/


        - start copy over project data from guests to their own centers...


        - also work on how to merge it all together across data centers regularly and automatically using a sync system; maybe csync2, or maybe just rsync with the -u option:
                -u, --update                skip files that are newer on the receiver

          Annoying issue: rsync of a sparse file is dog slow (?)
            # on cloud1
            time rsync -axvHu --sparse /mnt/projects/ cloud3:/mnt/projects/

          See this about inplace and sparse -- http://gergap.wordpress.com/2013/08/10/rsync-and-sparse-files/

          when done, the 3-7 and 10-21 DC's will have the union of all images.

        - mount and run images using an lxc container + dynamic vpn network.


- [ ] start stage 1 of migration process going in parallel on all compute nodes, since that will take several day(s).

- [ ] stage 2:
       - re-doing each rsync in case of newly created files (only needed on projects that have been used recently)
       - for each image that exceeds 4GB the quota has been exceeded, e.g., this one: eabf0ac2-95d3-4bc2-8e5a-1be7256234c9
         we have to do something -- maybe on a case-by-case basis.

- [ ] write script that for each project, sets something in database ('temporary maintenance'),
      updates the image (via rsync), then sets a field in projects database that says that
      project has been migrated to LXC+glusterfs+ZFS.

- [ ] rewrite snapshot interface to use zfs snapshots instead of bup

- [ ] get LXC container system to also work with GCE

- [ ] expand glusterfs to include GCE data centers

- [ ] offnet backup of all projects via gluster "geo-replication": https://access.redhat.com/site/documentation/en-US/Red_Hat_Storage_Software_Appliance/3.2/html/User_Guide/chap-User_Guide-Geo_Rep-Preparation.html


### A global shared data volume for database of interest to people

' '.join(['10.1.%i.1:/home/salvus/vm/images/gluster/data'%i for i in [1,10,  2,11,  3,12,  4,13,  5,14,  6,15,  7,16]])

sudo gluster volume create data replica 2 transport tcp  10.1.1.1:/home/salvus/vm/images/gluster/data 10.1.10.1:/home/salvus/vm/images/gluster/data 10.1.2.1:/home/salvus/vm/images/gluster/data 10.1.11.1:/home/salvus/vm/images/gluster/data 10.1.3.1:/home/salvus/vm/images/gluster/data 10.1.12.1:/home/salvus/vm/images/gluster/data 10.1.4.1:/home/salvus/vm/images/gluster/data 10.1.13.1:/home/salvus/vm/images/gluster/data 10.1.5.1:/home/salvus/vm/images/gluster/data 10.1.14.1:/home/salvus/vm/images/gluster/data 10.1.6.1:/home/salvus/vm/images/gluster/data 10.1.15.1:/home/salvus/vm/images/gluster/data 10.1.7.1:/home/salvus/vm/images/gluster/data 10.1.16.1:/home/salvus/vm/images/gluster/data

    sudo gluster volume set data  auth.allow 10.1.*

### A global shared scratch -- free for all that any user can access from any project; easy data sharing.

sudo gluster volume create scratch replica 2 transport tcp  10.1.1.1:/home/salvus/vm/images/gluster/scratch 10.1.10.1:/home/salvus/vm/images/gluster/scratch 10.1.2.1:/home/salvus/vm/images/gluster/scratch 10.1.11.1:/home/salvus/vm/images/gluster/scratch 10.1.3.1:/home/salvus/vm/images/gluster/scratch 10.1.12.1:/home/salvus/vm/images/gluster/scratch 10.1.4.1:/home/salvus/vm/images/gluster/scratch 10.1.13.1:/home/salvus/vm/images/gluster/scratch 10.1.5.1:/home/salvus/vm/images/gluster/scratch 10.1.14.1:/home/salvus/vm/images/gluster/scratch 10.1.6.1:/home/salvus/vm/images/gluster/scratch 10.1.15.1:/home/salvus/vm/images/gluster/scratch 10.1.7.1:/home/salvus/vm/images/gluster/scratch 10.1.16.1:/home/salvus/vm/images/gluster/scratch

    sudo gluster volume setm scratch  auth.allow 10.*


Eliminating SPOF on mount: http://www.jamescoyle.net/how-to/439-mount-a-glusterfs-volume


---

PLAN:

 - glusterfs data and scratch volumes, which are shared across the system -- convenient for users.
 - have a large glusterfs volume with replication of (at least 2, for now) where canonical version of projects are stored, and project is periodically rsync'd back when running.

     Testing this: it takes hours (?) just to rsync my 2.5GB devel project over.  Thus to see if this is a usable approach, I will need to try various optimizations.  The big issue is encryption, probably.

     Idea: - completely unencrypted in data center
           - client vm connects to host only to mount (not over vpn)
           - use an ssh tunnel *between* data centers, where encryption is required


     Idea: - completely unencrypted in data center
           - client vm connects to host only to mount (not over vpn)
           - use an ssh tunnel *between* data centers, where encryption is required


 - when user opens or moves a project, if not deployed, it gets rsync from glusterfs volume as first choice.
 -




   - [ ] (1:00?) (0:20?) project move: *all* hubs using that project need to react to project move correctly... argh.  This is hard because there is no inter-hub message system... and I don't want one if I can avoid it!

  - If the project is working, then it can broadcast that it is going to be moved to all connected hubs, and they can take appropriate action.
  - If the project is not working, e.g., because the machine it is deployed on is down, then there are no connected hubs.

... so it seems like all I need is a way for a local hub to tell all connected global hubs to close *all* connections to it.
OR, even more simply, when a global hub looses the connection to the local hub, it checks in the database that the location
is the same before trying to reconnect.

So I just need to modify Project and LocalHub so that instead of caching the VM they are connected to, whenever they need that information they get it fresh from the database.  Then everything else takes care of itself.

---->  - [x] (1:30) new_local_hub should *only* take project_id as input and only cached based on that.

       - [ ] be sure to test this codepath in hub.coffee: "This deals with VERY RARE case where user of project somehow deleted"
       - [x] test this killall change: "pkill -9 -u `whoami`"

  - [ ] when connection gets created for new_local_hub, it will always query database for location and use that.
  - [ ] if new_local_hub queries and gets empty location, it will call function to deploy project, then make connection
  - [ ] Project should get location info it needs from database (or just move that to localhub).


- [ ] (1:30?) proper file rename/move for project files, finally!

- [ ] (1:00?) project move: better ui feedback on move




- [ ] (0:30?) file operations thing on right is too far over for directories (compared to files, which are correct)

- [ ] (2:00?) limit user cpu using cgroups (see redhat guide)
- [ ] (2:00?) limit user memory using cgroups
- [ ] (2:00?) limit user disk io using cgroups
- [ ] (2:00?) limit user disk usage via filesystem quotas
- [ ] (1:00?) change snapshot display to use timeago for the time they happened, when time since utc is available; otherwise, don't.
- [ ] (1:00?) bug in parsing try/except/else -- https://mail.google.com/mail/u/0/?shva=1#starred/1428eb398a87ed4e
        try:
            print('try')
        except:
            print('except')
        else:
            print('else')
- [ ] (1:00?) bug in parser with non-indented comments
- [ ] (1:30?) bug with wrong pill being highlighted.
- [ ] (2:00?) print for other document types (use lstlisting or...?)
- [ ] (1:00?) terminal: preserve history in file...
- [ ] (1:00?) fix password reset to be more robust against browser cache -- just make it some simple html
- [ ] (1:30?) fix terminal hangs

- [ ] can't use # in tex mode in %md cells, i.e., $$ \# $$ doesn't work.
- [ ] typeset_mode(True); EllipticCurve('11a').lseries()   # yuck!

## deploy on Google Compute Engine

Note -- all prices went down on Dec 2!

    - [ ] get quota raised (requested on Nov 29, 2013 at 11:30am; again twice on Dec 2.)
    - [ ] (0:30?) Create disks (snapshots, database images; etc.) at $0.085/GB = $272/month
          All these will use LVM:  sudo apt-get install lvm2
          Will grow later.
               cassandra1g1  100GB
               cassandra2g1  100GB
               cassandra3g1  100GB
               cassandra4g1  100GB
               compute1g1    400GB
               compute2g1    400GB
               snap1g1       1000GB
               snap2g1       1000GB
    - [ ] (1:00?) 4-node cassandra site with replication factor of 3:
               ...  n1-standard-1 instances (1 core, 3.75GB RAM)     $0.115/hour/machine (4 machines) = $331.20/month
               cassandra1g   10.4.1.2
               cassandra2g   10.4.2.2
               cassandra3g   10.4.3.2
               cassandra4g   10.4.4.2
    - [ ] (1:00?) 2-node web serving (hub, haproxy, stunnel, nginx, snap) nodes:
               ....  n1-standard-1 instances (1 core, 3.75GB RAM)     $0.114/hour/machine = $166/month
               each has a public ip address
               web1g         10.4.1.3
               web2g         10.4.2.3
    - [ ] (1:00?) 2 4-core compute vm's with /mnt/home:
               2 .... n1-standard-4-d instances (4 cores, 15GB RAM, diskless)  $0.461/hour/machine = $664/month (2 machines)
               compute1g     10.4.1.4
               compute2g     10.4.2.4

    Total cost: about $1200/month

    - [ ] (0:30?) Add the public web server ip's to godaddy dns.

========================

- [x] (1:30?) (0:58+) bup snapshotting issue when sshfs is stale.
   ',3702601d-9fbc-4e4e-b7ab-c10a79e34d3b,command '/usr/bin/bup' (args=on teaAuZ9M@10.1.2.4 index --one-file-system .) exited with nonzero code 1 -- stderr='/mnt/home/teaAuZ9M/lxc: [Errno 107] Transport endpoint is not connected: 'lxc'

   Should be fixed -- wait 15 minutes to see if cloud.dev project gets snapshotted; if so, good!
   If so, update bup on all compute machines.


- [x] (0:15?) enable crontabs on 10.9.1.2, once the two backups finished.

- [x] (1:00?) (0:40) change the create_unix_user.py (sudo) command to optionally take an account name as input.
- [x] (1:00?) (0:18) fix json worksheet printing issue that Harald pointed out with his theano example; also "tmp/linear regression.sagews" -- worksheet that doesn't print from Gustav Delius

- [x] (0:30?) (0:09) worksheet printing -- command line option to leave files around.

- [x] (1:00?) FAIL to add file attachment to worksheet printing -- http://tex.stackexchange.com/questions/94811/attaching-file-into-a-pdf-with-pdflatex-will-crash-adobe-reader

- [x] (0:10?) (0:09) send updated "file usage" data.

- [x] (0:45?) (0:47) snap -- save last successful snapshot in own table (make sure snap user doesn't have to edit project table).

    CREATE TABLE last_snapshot (
        project_id uuid,
        server_id  uuid,
        repo_id    uuid,
        timestamp  varchar,
        utc_seconds_epoch int,
        PRIMARY KEY(project_id, server_id)
    );


- [x] (1:00?) (2:00) show most recent available snapshot time, with a link to the snapshots listings.
- [x] (0:30?) last snapshot -- doesn't update properly -- why?!

- [x] (1:00?) (0:30) change smc favicon to be sage in color... and/or maybe use harald's?: https://mail.google.com/mail/u/0/?shva=1#inbox/142aead2719273d3

- [x] (1:00?) (1:08) project move: change username to project-id on project move and also new project creation; this is more logical and avoids all possible issues of conflict in future, especially enabling having a hot standby.f
- [x] (2:00?) (0:37) project move: go through each snapshot until success.
- [x] this repo on 10.1.10.3 is broken:
        202630a1-b0a5-422f-8e4f-b48e4998b371
    echo "select timestamp, project_id from snap_commits where repo_id=202630a1-b0a5-422f-8e4f-b48e4998b371; " | cqlsh_connect 10.1.3.2 |sort> a
    I had to go back 14 commits with snap_fix... script!?


