{"desc":"#idea\nincremental tar \n\nBasically do\n\n     tar --listed-incremental=incremntal.data -cvf project_id-timestamp.tar project_id/\n     tar --listed-incremental=incremntal.data -cvf project_id-timestamp.tar project_id/\n\netc. and to extract\n\n     tar --listed-incremental=incremntal.data -xvf project_id-timestamp.tar\n\nwhere incremntal.data just grows with more info over time.\n\nTEST:\n\n\ttime tar --listed-incremental=/tmp/inc -cvf - 5ec67a33-4346-4769-a1f1-b1dd6bb88a34 | lz4 - > /tmp/a.tar.lz4","position":-1,"last_edited":1430814539716,"task_id":"f28cc862-9a58-40e1-96f7-8148af79e9bb","done":1430814539716}
{"desc":"- [x] make a GCS bucket called smc-tar\n- [ ] tar_save: to smc_compute like archive, but with target incremental tar output\n- [ ] tar_pull: opens if necessary and gets the incrementals, optionally making snapshots as you go.","position":-2,"last_edited":1430814540441,"task_id":"a925f69f-cb8f-4d99-8722-f8f12e9fc514","done":1430814540441}
{"desc":"incremental tar -- total  FAIL.\n\nnext idea:\n\n- k nodes each with a 1TB PD ZPOOL with no dedup but yes compression.\n- put each project on 2 of the four nodes\n- in database have column locations that maps timestamps to hostnames\n- run my old bup_server code, but without the bup stuff at all\n- make the rolling ZFS snapshots nicely available to users\n- periodically rsync for replication (but nothing to do with snapshot times, except rsync last snapshot maybe)\n","position":0,"last_edited":1430817907287,"task_id":"b980e216-295d-40d7-b0f3-ea76f92381de","done":1430817906866}
{"desc":"(0:30?) create one more powerful storage node from that backup I was making -- restart those going, but with more --excludes.","position":1,"last_edited":1430817922846,"task_id":"451e23b7-b9b5-42dd-954f-4c4a71ba0ba1","done":1430817922437}
{"desc":"#now (1:00?)  add function to smc_compute.py to save/open/close project using this new method.","position":2,"last_edited":1430820343167,"task_id":"f6fce875-d897-4bc5-a2f3-6f741a41c291","done":1430820342758}
{"desc":"#0\n(1:30?) change compute.coffee to check in db to see if project is using this new method and if so, use it instead on a node that can use it (like experimental).  get this to fully work on test projects.","position":3,"last_edited":1430862561460,"task_id":"b0e84fc9-bc12-43f5-8877-e03b8c03aa91","done":1430862561045}
{"desc":"#0 (1:00?) #now\n- [ ] once the rsync from uw is done, make a snapshot at that point.\n- [x] setup something that rsync's from the 6 machines right now in a loop periodically, and run that.\n- [ ] setup rolling snapshots (find some scripts)","position":3.125,"last_edited":1430907605995,"task_id":"43d2e4bc-0cb4-460c-bfde-a8ce7e4889f8","done":1430907605580}
{"desc":"#idea OK, so the fastest imaginable way to do this is to:\n\n- use the rsync daemon: about 4GB/minute\n- using ssh with the right options: about 3GB/minute\n\nSECURITY:\n\n- i could make a storage daemon and have all moving of files be done *from* there for better security, so the compute vm's don't have to do anything dangerous regarding write... exporting snapshot dirs, which gives full read access to an attacker.","position":3.5,"last_edited":1430855014673,"task_id":"2f812913-5b26-4f91-8f01-b1c0b89b6a95","done":1430855014260}
{"desc":"#1\n(1:00?) implement read-only nfs mounting of snapshots from storage server to compute servers","position":5,"last_edited":1430855028676,"task_id":"29e7bf18-8fd9-4224-8bf4-f403d5bbd0c0"}
{"desc":"#1\n(1:00?) implement way for users to browse their snapshots.  HOW?!??!\n\n1. make it so when git-ls requests listing for `.snapshots/timestamp`, only include  .snapshots/timestamp/project-id in listing....\n2. on client side when user clicks on .snapshots/timestamp/, instead open .snapshots/timestamp/project-id/ instead.\n\n\n","position":6,"last_edited":1430855031336,"task_id":"7e7f0e1c-214b-45f7-9d8a-11738bede73f"}
{"desc":"#0 (2:00?) \nmake new cluster and switch everything over\nPLAN:\n- [x] setup new compute0 that is upgraded and works -- test (make compute0 experimental).\n- [ ] #now make it so smc_compute close on old nodes does an rsync right after a normal close.\n\n(currently getting this error on move open):\n\n        2015-05-07T03:39:58.450Z - debug: mesg (hub <- compute0-us): {\"event\":\"error\",\"error\":{\"errno\":19,\"code\":\"SQLITE_CONSTRAIN\n        T\"},\"id\":\"4ca30a1c-9bc0-4a8c-b08b-fba1d6b74124\"}\n        2015-05-07T03:39:58.450Z - debug: ComputeServerClient.call(hub --> compute0-us): got response -- {\"event\":\"error\",\"error\":\n        \"[object]\",\"id\":\"4ca30a1c-9bc0-4a8c-b08b-fba1d6b74124\"}\n        2015-05-07T03:39:58.451Z - debug: ComputeServerClient.call(hub --> compute0-us): error = [object Object]\n        2015-05-07T03:39:58.452Z - debug: ProjectClient(project_id='666c7f0b-277f-4ed6-868c-9d6df7d8d59c','compute0-us')._action(a\n        ction=open): error calling compute server -- [object Object]\n        DONE { errno: 19, code: 'SQLITE_CONSTRAINT' }\n\n- [x] test save on project with sshfs mounts...\n- [x] ensure rsync save failures are recorded somewhere so I can look at them all\n- [ ] make it so any newly opened projects open on compute0 by making old compute nodes experimental and making new compute0 NOT experimental anymore. test moving some projects.\n- [ ] spin up compute1, compute2, compute3.\n- [ ] amazon dns for compute vm's\n- [ ] move all projects to new cluster\n- [ ] shut down old cluster\n- [ ] switching europe smc nodes to use europe database servers and update all smc servers (so shorter mintime)\n\n- [x] **WORRY:** if error when checking out project, later save will delete everything!!?  Could track state in local DB.  Or only allow save when running except with a force option, and make getting to running require open to succeed.","position":3.25,"last_edited":1430998538099,"task_id":"e163e304-f412-4d0c-be84-e81aa60b6138"}
{"desc":"#1\n(2:00?) make storage server redundant\n- snapshot disk\n- start another similar machine called storage1\n- have the rsync_save process just save to one at random and do something (e.g., timestamp of directory or database entry) so that server knows to push to other\n- have the rsync_open process just try to load from one at random, and if fails, try the other.\n- periodic sync by backend on changed directories","position":3.375,"last_edited":1430855268149,"task_id":"f08f063d-a244-4d29-b808-9d9c88e42bb9"}
{"desc":"3GB/m open scrollbar.","position":3.4375,"last_edited":1430907638362,"task_id":"7c98e4e8-9d00-4f15-a08e-ce6735d07080"}
{"desc":"#1\nsnapshot browser should have default option to restrict to only snapshots where you (or project or some users) were active, based on database....","position":3.1875,"last_edited":1430864667936,"task_id":"7b1c85e6-8755-4a33-90fa-d8aacd759454"}
{"desc":"#2 haproxy -- make the test for functionality also take into account the proxy server failing.\n\nEITHER: don't tie the proxy server and hub, or require both to work or fail whole thing...  ","position":3.46875,"last_edited":1430907685595,"task_id":"29e9fc1b-d22e-4642-98da-d07e3004797d"}
{"desc":"need new cassandra backups to nearline cloud storage and offline...","position":3.484375,"last_edited":1430914984695,"task_id":"a131158f-c259-4544-9f5a-ee5a10f479d1"}
{"desc":"#1\ncritical -- MUST make system immune to user fork bombs!","position":3.498046875,"last_edited":1430947355668,"task_id":"7690b55b-5ecf-4a03-9cb8-6b48d4eecf35"}
{"desc":"#0\nupgrade to Ubuntu 15.04 \n- [x] make base image bigger.\n- [x] may fix btrfs crashes, compile sage problems.\n\n      libghc-gtk-dev libghc-gtk-doc libghc-hint-dev libghc-hint-doc\n      libghc-io-storage-dev libghc-lens-dev libghc-lens-doc\n      libghc-nats-dev libghc-nats-doc libghc-pango-dev libghc-pango-doc\n      libghc-pointedlist-dev libghc-pointedlist-doc libghc-polyparse-dev\n      libghc-prelude-extras-dev libghc-prelude-extras-doc\n      libghc-profunctors-dev libghc-profunctors-doc libghc-reflection-dev\n      libghc-reflection-doc libghc-regex-tdfa-dev libghc-regex-tdfa-doc\n      libghc-scientific-dev libghc-scientific-doc\n      libghc-semigroupoids-dev libghc-semigroupoids-doc\n      libghc-semigroups-dev libghc-semigroups-doc libghc-src-exts-dev\n      libghc-tagged-dev libghc-tagged-doc libghc-terminfo-dev\n      libghc-terminfo-doc libghc-transformers-compat-dev\n      libghc-uniplate-dev libghc-unix-compat-dev libghc-utf8-string-dev\n      libghc-utf8-string-doc libghc-utility-ht-dev libghc-void-dev\n      libghc-void-doc libghc-vty-dev libghc-vty-doc\n      libghc-xdg-basedir-dev libghc-yi-doc libginac2 libgrail6 libgrip0\n      libisl10 libmagick++5 libmagickcore5 libmagickcore5-extra\n      libmagickwand5 libmirclient8driver-mesa libmircommon2\n      libnunit2.6-cil libopenvg1-mesa libpci-dev libpoppler46\n      libprotobuf-lite8 libprotobuf8 libtorque2 libts-0.0-0\n      libwayland-egl1-mesa linux-headers-3.16.0-34\n      linux-headers-3.16.0-34-generic linux-headers-3.16.0-36\n      linux-headers-3.16.0-36-generic linux-headers-3.16.0-37\n      linux-headers-3.16.0-37-generic linux-image-3.16.0-34-generic\n      linux-image-3.16.0-36-generic m17n-contrib python-greenlet\n      ruby-commander ruby-highline ruby-maruku ruby-nokogiri\n      ruby-rdiscount ruby-redcloth tsconf","position":3.15625,"last_edited":1430947367228,"task_id":"f7bae4fc-909e-44b0-859f-55937762870b","done":1430947366818}
{"desc":"#1 (0:30?)\nmove admin machine to us-central1-c","position":3.3125,"last_edited":1430944937647,"task_id":"2cc763bb-bfc3-41b3-8aae-fc68eb74267d"}
{"desc":"#0 (0:30?)\n","position":3.21875,"last_edited":1430961901874,"task_id":"e80d9aca-4843-453c-98c8-9377fb76e195","deleted":true}
{"desc":"#0 (2:00?)\nimplement snapshot system\n\n- [ ] when opening a project, create symlinks for each snapshot in the NFS share\n- [ ] when saving, update this list\n\nThis will basically work and can be improved.  Since old snapshots get deleted sometimes, this will sometimes break.  Good enough for now.","position":3.28125,"last_edited":1430996629883,"task_id":"c96e3ff8-261a-4b5a-ad5f-e6e81d6caf40","done":1430996629462}
{"desc":"#1\nre-enable automatic failover\n","position":3.265625,"last_edited":1430962704813,"task_id":"f7c6ba55-3e56-4838-b634-e7e059e201e9"}
{"desc":"#0\nsnapshots -- on save maybe also make tons of local btrfs snapshots of the project subvolume... ?","position":3.2578125,"last_edited":1430996627259,"task_id":"25111dba-a386-4ab6-825c-d3ead86575f4","done":1430996626830}
{"desc":"#0\nSee console.log message when loading a public page\n\n\topts.content=","position":3.25390625,"last_edited":1431000205628,"task_id":"f4a382e8-168a-4019-9c5c-415c1f8726c4"}
{"desc":"#0\ncan't start nfs snapshots while firewall up.","position":3.255859375,"last_edited":1431002471345,"task_id":"01f80029-bedd-4d7f-971f-4654522854a9"}