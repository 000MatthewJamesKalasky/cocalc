{"desc":"#idea\nincremental tar \n\nBasically do\n\n     tar --listed-incremental=incremntal.data -cvf project_id-timestamp.tar project_id/\n     tar --listed-incremental=incremntal.data -cvf project_id-timestamp.tar project_id/\n\netc. and to extract\n\n     tar --listed-incremental=incremntal.data -xvf project_id-timestamp.tar\n\nwhere incremntal.data just grows with more info over time.\n\nTEST:\n\n\ttime tar --listed-incremental=/tmp/inc -cvf - 5ec67a33-4346-4769-a1f1-b1dd6bb88a34 | lz4 - > /tmp/a.tar.lz4","position":-1,"last_edited":1430814539716,"task_id":"f28cc862-9a58-40e1-96f7-8148af79e9bb","done":1430814539716}
{"desc":"- [x] make a GCS bucket called smc-tar\n- [ ] tar_save: to smc_compute like archive, but with target incremental tar output\n- [ ] tar_pull: opens if necessary and gets the incrementals, optionally making snapshots as you go.","position":-2,"last_edited":1430814540441,"task_id":"a925f69f-cb8f-4d99-8722-f8f12e9fc514","done":1430814540441}
{"desc":"incremental tar -- total  FAIL.\n\nnext idea:\n\n- k nodes each with a 1TB PD ZPOOL with no dedup but yes compression.\n- put each project on 2 of the four nodes\n- in database have column locations that maps timestamps to hostnames\n- run my old bup_server code, but without the bup stuff at all\n- make the rolling ZFS snapshots nicely available to users\n- periodically rsync for replication (but nothing to do with snapshot times, except rsync last snapshot maybe)\n","position":0,"last_edited":1430817907287,"task_id":"b980e216-295d-40d7-b0f3-ea76f92381de","done":1430817906866}
{"desc":"(0:30?) create one more powerful storage node from that backup I was making -- restart those going, but with more --excludes.","position":1,"last_edited":1430817922846,"task_id":"451e23b7-b9b5-42dd-954f-4c4a71ba0ba1","done":1430817922437}
{"desc":"#now (1:00?)  add function to smc_compute.py to save/open/close project using this new method.","position":2,"last_edited":1430820343167,"task_id":"f6fce875-d897-4bc5-a2f3-6f741a41c291","done":1430820342758}
{"desc":"#0\n(1:30?) change compute.coffee to check in db to see if project is using this new method and if so, use it instead on a node that can use it (like experimental).  get this to fully work on test projects.","position":3,"last_edited":1430862561460,"task_id":"b0e84fc9-bc12-43f5-8877-e03b8c03aa91","done":1430862561045}
{"desc":"#0 (1:00?) #now\n- [ ] once the rsync from uw is done, make a snapshot at that point.\n- [x] setup something that rsync's from the 6 machines right now in a loop periodically, and run that.\n- [ ] setup rolling snapshots (find some scripts)","position":3.125,"last_edited":1430907605995,"task_id":"43d2e4bc-0cb4-460c-bfde-a8ce7e4889f8","done":1430907605580}
{"desc":"#idea OK, so the fastest imaginable way to do this is to:\n\n- use the rsync daemon: about 4GB/minute\n- using ssh with the right options: about 3GB/minute\n\nSECURITY:\n\n- i could make a storage daemon and have all moving of files be done *from* there for better security, so the compute vm's don't have to do anything dangerous regarding write... exporting snapshot dirs, which gives full read access to an attacker.","position":3.5,"last_edited":1430855014673,"task_id":"2f812913-5b26-4f91-8f01-b1c0b89b6a95","done":1430855014260}
{"desc":"#1\n(1:00?) implement read-only nfs mounting of snapshots from storage server to compute servers","position":5,"last_edited":1431137105302,"task_id":"29e7bf18-8fd9-4224-8bf4-f403d5bbd0c0","done":1431137104895}
{"desc":"#1\n(1:00?) implement way for users to browse their snapshots.  HOW?!??!\n\n1. make it so when git-ls requests listing for `.snapshots/timestamp`, only include  .snapshots/timestamp/project-id in listing....\n2. on client side when user clicks on .snapshots/timestamp/, instead open .snapshots/timestamp/project-id/ instead.\n\n\n","position":6,"last_edited":1431137110029,"task_id":"7e7f0e1c-214b-45f7-9d8a-11738bede73f","done":1431137109609}
{"desc":"#0 (2:00?) \nmake new cluster and switch everything over\nPLAN:\n- [x] setup new compute0 that is upgraded and works -- test (make compute0 experimental).\n- [ ] #now make it so smc_compute close on old nodes does an rsync right after a normal close.\n\n(currently getting this error on move open):\n\n        2015-05-07T03:39:58.450Z - debug: mesg (hub <- compute0-us): {\"event\":\"error\",\"error\":{\"errno\":19,\"code\":\"SQLITE_CONSTRAIN\n        T\"},\"id\":\"4ca30a1c-9bc0-4a8c-b08b-fba1d6b74124\"}\n        2015-05-07T03:39:58.450Z - debug: ComputeServerClient.call(hub --> compute0-us): got response -- {\"event\":\"error\",\"error\":\n        \"[object]\",\"id\":\"4ca30a1c-9bc0-4a8c-b08b-fba1d6b74124\"}\n        2015-05-07T03:39:58.451Z - debug: ComputeServerClient.call(hub --> compute0-us): error = [object Object]\n        2015-05-07T03:39:58.452Z - debug: ProjectClient(project_id='666c7f0b-277f-4ed6-868c-9d6df7d8d59c','compute0-us')._action(a\n        ction=open): error calling compute server -- [object Object]\n        DONE { errno: 19, code: 'SQLITE_CONSTRAINT' }\n\n- [x] test save on project with sshfs mounts...\n- [x] ensure rsync save failures are recorded somewhere so I can look at them all\n- [x] make it so any newly opened projects open on compute0 by making old compute nodes experimental and making new compute0 NOT experimental anymore. test moving some projects.\n- [x] spin up compute1, compute2, compute3.\n- [x] amazon dns for compute vm's\n- [ ] move all projects to new cluster\n- [ ] shut down old cluster\n- [ ] switching europe smc nodes to use europe database servers and update all smc servers (so shorter mintime)\n\n- [x] **WORRY:** if error when checking out project, later save will delete everything!!?  Could track state in local DB.  Or only allow save when running except with a force option, and make getting to running require open to succeed.","position":3.25,"last_edited":1431137039575,"task_id":"e163e304-f412-4d0c-be84-e81aa60b6138","done":1431137039158}
{"desc":"#1\n(2:00?) make storage server redundant\n- snapshot disk\n- start another similar machine called storage1\n- have the rsync_save process just save to one at random and do something (e.g., timestamp of directory or database entry) so that server knows to push to other\n- have the rsync_open process just try to load from one at random, and if fails, try the other.\n- periodic sync by backend on changed directories","position":3.375,"last_edited":1430855268149,"task_id":"f08f063d-a244-4d29-b808-9d9c88e42bb9"}
{"desc":"3GB/m open scrollbar.","position":3.4375,"last_edited":1431137089349,"task_id":"7c98e4e8-9d00-4f15-a08e-ce6735d07080","done":1431137088940}
{"desc":"#1\nsnapshot browser should have default option to restrict to only snapshots where you (or project or some users) were active, based on database....","position":3.1875,"last_edited":1431295355120,"task_id":"7b1c85e6-8755-4a33-90fa-d8aacd759454","deleted":true}
{"desc":"#2 haproxy -- make the test for functionality also take into account the proxy server failing.\n\nEITHER: don't tie the proxy server and hub, or require both to work or fail whole thing...  ","position":3.46875,"last_edited":1430907685595,"task_id":"29e9fc1b-d22e-4642-98da-d07e3004797d"}
{"desc":"#0 #backup\nneed new cassandra backups to nearline cloud storage and offline...","position":3.1572265625,"last_edited":1431308237064,"task_id":"a131158f-c259-4544-9f5a-ee5a10f479d1","done":1431308236648}
{"desc":"#2\ncritical -- MUST make system immune to user fork bombs!","position":3.498046875,"last_edited":1431295404362,"task_id":"7690b55b-5ecf-4a03-9cb8-6b48d4eecf35","deleted":true}
{"desc":"#0\nupgrade to Ubuntu 15.04 \n- [x] make base image bigger.\n- [x] may fix btrfs crashes, compile sage problems.\n\n      libghc-gtk-dev libghc-gtk-doc libghc-hint-dev libghc-hint-doc\n      libghc-io-storage-dev libghc-lens-dev libghc-lens-doc\n      libghc-nats-dev libghc-nats-doc libghc-pango-dev libghc-pango-doc\n      libghc-pointedlist-dev libghc-pointedlist-doc libghc-polyparse-dev\n      libghc-prelude-extras-dev libghc-prelude-extras-doc\n      libghc-profunctors-dev libghc-profunctors-doc libghc-reflection-dev\n      libghc-reflection-doc libghc-regex-tdfa-dev libghc-regex-tdfa-doc\n      libghc-scientific-dev libghc-scientific-doc\n      libghc-semigroupoids-dev libghc-semigroupoids-doc\n      libghc-semigroups-dev libghc-semigroups-doc libghc-src-exts-dev\n      libghc-tagged-dev libghc-tagged-doc libghc-terminfo-dev\n      libghc-terminfo-doc libghc-transformers-compat-dev\n      libghc-uniplate-dev libghc-unix-compat-dev libghc-utf8-string-dev\n      libghc-utf8-string-doc libghc-utility-ht-dev libghc-void-dev\n      libghc-void-doc libghc-vty-dev libghc-vty-doc\n      libghc-xdg-basedir-dev libghc-yi-doc libginac2 libgrail6 libgrip0\n      libisl10 libmagick++5 libmagickcore5 libmagickcore5-extra\n      libmagickwand5 libmirclient8driver-mesa libmircommon2\n      libnunit2.6-cil libopenvg1-mesa libpci-dev libpoppler46\n      libprotobuf-lite8 libprotobuf8 libtorque2 libts-0.0-0\n      libwayland-egl1-mesa linux-headers-3.16.0-34\n      linux-headers-3.16.0-34-generic linux-headers-3.16.0-36\n      linux-headers-3.16.0-36-generic linux-headers-3.16.0-37\n      linux-headers-3.16.0-37-generic linux-image-3.16.0-34-generic\n      linux-image-3.16.0-36-generic m17n-contrib python-greenlet\n      ruby-commander ruby-highline ruby-maruku ruby-nokogiri\n      ruby-rdiscount ruby-redcloth tsconf","position":3.15625,"last_edited":1430947367228,"task_id":"f7bae4fc-909e-44b0-859f-55937762870b","done":1430947366818}
{"desc":"#1 (0:30?) #now\nmove admin machine to us-central1-c","position":3.3125,"last_edited":1431150341584,"task_id":"2cc763bb-bfc3-41b3-8aae-fc68eb74267d","done":1431150341179}
{"desc":"#0 (0:30?)\n","position":3.21875,"last_edited":1430961901874,"task_id":"e80d9aca-4843-453c-98c8-9377fb76e195","deleted":true}
{"desc":"#0 (2:00?)\nimplement snapshot system\n\n- [ ] when opening a project, create symlinks for each snapshot in the NFS share\n- [ ] when saving, update this list\n\nThis will basically work and can be improved.  Since old snapshots get deleted sometimes, this will sometimes break.  Good enough for now.","position":3.28125,"last_edited":1430996629883,"task_id":"c96e3ff8-261a-4b5a-ad5f-e6e81d6caf40","done":1430996629462}
{"desc":"#1\nre-enable automatic failover\n","position":3.265625,"last_edited":1431137054100,"task_id":"f7c6ba55-3e56-4838-b634-e7e059e201e9","done":1431137053688}
{"desc":"#0\nsnapshots -- on save maybe also make tons of local btrfs snapshots of the project subvolume... ?","position":3.2578125,"last_edited":1430996627259,"task_id":"25111dba-a386-4ab6-825c-d3ead86575f4","done":1430996626830}
{"desc":"#1\nSee console.log message when loading a public page\n\n\topts.content=","position":3.25390625,"last_edited":1431137045010,"task_id":"f4a382e8-168a-4019-9c5c-415c1f8726c4"}
{"desc":"#0\ncan't start nfs snapshots while firewall up.","position":3.255859375,"last_edited":1431137051856,"task_id":"01f80029-bedd-4d7f-971f-4654522854a9","done":1431137051443}
{"desc":"#1 (1:00?) #compute\nrestarting compute needs to open service firewall for all projects that are running and have network access.\n\nhttps://mail.google.com/mail/u/1/#inbox/14d31c56059bfd51j","position":3.1650390625,"last_edited":1431365417049,"task_id":"186f14ed-ae4b-4be0-9ea9-8fc0a8416d98"}
{"desc":"#1 \nrewrite status/state/etc. functionality to not call python script at all -- have compute directly do those things.","position":3.1796875,"last_edited":1431318487548,"task_id":"b5df689e-7602-4288-893f-ddacc07b47ab"}
{"desc":"#0 (1:00?)\nsetup devel cluster\n\n- [x] #now fix so this is automatic\n\n        salvus@smc0-devel-us-central1-c:~/salvus/salvus/data/cassandra-0$ ln -s ../local/cassandra/lib .\n        salvus@smc0-devel-us-central1-c:~/logs$ rm cassandra.log ; ln -s /home/salvus/salvus/salvus/data/cassandra-0/logs/system.log cassandra.log\n\n","position":3.1640625,"last_edited":1431295504006,"task_id":"1e99abdc-a12a-4631-bdd2-4b82f0984a5b","done":1431295503594}
{"desc":"#storage\ndecide how to make storage not a single point of failure\n\nIf I change all the read-only snapshots to be read-write (but still mount read-only via sshfs), then I can replicate even the snapshots using rsync.","position":3.34375,"last_edited":1431188692703,"task_id":"781c73ca-2de5-434b-97c4-73e22fe5f48c"}
{"desc":"#1 (0:30?) #now\ncompletely redo the europe database.\n\nDO NOT FORGOT to turn of auto_bootstrap False this time!!!!","position":3.15966796875,"last_edited":1431315977604,"task_id":"fe4bb9a4-04d2-4aa4-9813-7b3ba7829024","done":1431315977185}
{"desc":"#0\nideas to make storage0 better\n- it could itself be a cache with something else that is ","position":3.169921875,"last_edited":1431188504363,"task_id":"4811648d-9c04-4f9d-aa27-fd2ba3b300c0","deleted":true}
{"desc":"#0 (1:00?) #backup\nupdate nearline backup of all projects that changed in the last 3 days...\n\n- [x] spot checks\n- [x] (0:09) (0:20?) copy all projects data files (but no tarballs) to /backups on storage0 via rsync: `time rsync -axvH --exclude *lz4 backup0:/backups/ /backups/ 1>sync_from_backups.log 2>sync_from_backups.err`\n- [x] #now (0:30?) add function to compute.coffee that runs tarball update on all projects modified since a given point in time.\n- [x] (0:10?) upload new backups to nearline\n- [ ] (0:10?) and offsite...\n- [ ] (0:10?) and delete from local cache\n- [x] (0:30?) setup crontab on storage to update /backups periodically for now...","position":3.158203125,"last_edited":1431313135008,"task_id":"875a1fc7-68ae-4fe8-9a85-c557cf23f2f9","done":1431313134601}
{"desc":"#0 \ngit-ls don't show broken symlinks in .snapshots dir.","position":3.16455078125,"last_edited":1431274838689,"task_id":"57e5e2e6-45e7-4dd9-b1cb-d173c6745e1f","done":1431274838278}
{"desc":"#0 #monitor #now (0:15) (0:15?)\naws health checks and dns","position":3.159912109375,"last_edited":1431345707987,"task_id":"44174bc5-3c03-4441-af60-ddaa3edf8994","done":1431345707575}
{"desc":"#0 #monitor (1:10?)\nadmin monitor script\n- [ ] get it to work (0:30?)\n- [ ] reset disk usage thresh (0:10?)\n- [ ] setup email notification (0:30?)","position":3.165771484375,"last_edited":1431342333601,"task_id":"bcefdbb2-e7cd-4e4f-bf75-ba74b3e3b7c5"}
{"desc":"#0 (0:30?) #storage #devel #now\nsetup devel cluster: storage server","position":3.16015625,"last_edited":1431315984526,"task_id":"68073817-533c-4f55-84c0-576072d999db"}
{"desc":"#now #0 (1:00?) #devel\ngce.py script to stop/start all nodes in devel cluster to save money","position":3.16552734375,"last_edited":1431315236811,"task_id":"6f4d8d39-ee12-42a5-9716-b91a9d824f5e","done":1431315236403}
{"desc":"#1 (0:30?) #compute\nthat RAM usage thing in quotas looks dumb -- hide or fix...","position":3.1669921875,"last_edited":1431365411864,"task_id":"665b4e46-53df-4c44-9461-3968ed4ec953"}
{"desc":"#0 #now\nrestart each cassandra to get rid of incremental backups.","position":3.1591796875,"last_edited":1431308217537,"task_id":"2316ad65-955f-47ea-9798-a7a328d4897b","done":1431308217120}
{"desc":"","position":3.15869140625,"last_edited":1431313008583,"task_id":"3ead3628-d687-4df2-a628-0719bf4fc8ce","deleted":true}
{"desc":"#0 (1:00?) #critical\nrewrite smc_compute.py to use setuid instead of su -- it will be massively faster. GAME CHANGER.\n\n- http://stackoverflow.com/questions/25928190/python-run-command-as-normal-user-in-a-root-script\n- http://stackoverflow.com/questions/1770209/run-child-processes-as-different-user-from-a-long-running-process\n\n\n\t\t>>> os.setuid(uid)\n        >>> os.system(\"cd /projects/97145daf-9bea-4b42-b075-807e07950981/.sagemathcloud && ./status\")\n        {\"sage_server.pid\": false, \"secret_token\": false, \"raw.port\": false, \"sage_server.port\": false, \"installed\": true, \"version\": 1430868307, \"local_hub.pid\": false, \"local_\n        hub.port\": false, \"console_server.pid\": false, \"console_server.port\": false}\n        0\n        >>> os.system(\"cd /projects/97145daf-9bea-4b42-b075-807e07950981/.sagemathcloud && whoami\")\n        97145daf9bea4b42b075807e07950981\n        \n        \nor\n\n        >>> import os\n        >>> os.setuid(12090511)\n        >>> os.chdir(\"/projects/c5e7c26c-93bb-4d95-8622-fca228b6cb1c/.sagemathcloud\")\n        >>> os.popen(\"./status\").read()\n        '{\"sage_server.pid\": 2844, \"secret_token\": \"LUgJopyLhEofMY08qt24ylFBbhU2DEw1Lrs74liDurLy5makwLQka93R9rBqv+U//DxbGLJnk/MM6+3idqRzCq9Ei3wjosGypmTPv7ws81l/lbchV3gSzwpBJ1wLO\n        XVF9HfkVO/IkNlYWpQeFjn98z1zcnV8MnvqzX7iEK3yg74=\", \"raw.port\": 54297, \"sage_server.port\": 36738, \"installed\": true, \"version\": 1430868307, \"local_hub.pid\": 2423, \"local_h\n        ub.port\": 56005, \"console_server.pid\": false, \"console_server.port\": false}\\n'","position":3.162109375,"last_edited":1431351921097,"task_id":"add378d2-6d9f-46d4-8a09-b87b6167b6c4"}
{"desc":"#now\ndelete snapshots\n\njust finish it...","position":3.1611328125,"last_edited":1431359327627,"task_id":"e8d98fbe-46d0-4d81-9459-965fee4313fd","done":1431359327200}
{"desc":"(1:00?) #0\nchange smc_compute.py to not use btrfs operations at all?\n\n- [ ] just rsync the .sagemathcloud template\n- [ ] just mkdir the project home\n- [ ] use traditional quotas (later)\n\nIt's not clear if this helped or not.   It seemed to remove a bottlekneck.","position":3.1630859375,"last_edited":1431371532671,"task_id":"55fabe80-8261-411f-a389-13d3e0ae182e"}
{"desc":"(1:00?) #0 #now\nspin up commercial machine and move projects there as requested","position":3.16357421875,"last_edited":1431367004244,"task_id":"79b829c1-df62-4319-a8e6-90de4f9e7342","done":1431367003823}