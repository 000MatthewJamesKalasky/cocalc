# Cluster-wide change plan

- [ ] New VM: modify database schema to use new system

    alter table projects add bup_location uuid;

    CREATE TABLE storage_servers (
        dummy     boolean,
        server_id uuid,
        host      inet,
        port      int,
        dc        int,
        vnodes    int,
        health    float,
        secret    varchar,
        PRIMARY KEY(dummy, server_id)
    );

- [ ] loop for each project that has been touched since last time -- running on cloud11

   Do the following in parallel until it basically stabilizes, then switch over my code.

      - if is an abuse project (bitcoin) skip and mark as such in database; skip in future; also disable corresponding account
      - if necessary, add in recent missing snapshots by pulling them remotely (but grabbing at most 100 total snapshots)
      - get the absolutely latest live version of file as a new snapshot from any recently modified projects
      - compact bup repo -- only if we bring in many snapshots
      - rsync out bup repo in parallel to one replica in data center 0 *only*.

   At the same time, finish stabilizing all the code on my laptop vm's.

    alter table projects add last_migrate_bup;
    alter table projects add last_migrate_bup_error;

- [ ] new VM's -- at least for dc0
      - [ ] updated code
      - [ ] delete storage user
      - [ ] bup zpool configured as it should be
      - [ ] visudo with hashrings and bupstorage.py code.
      - [ ] root ssh equivalence for rsync (for now -- will use an rsync server at some point)
      - [ ] standard automatic zfs snapshotting system for the bup pool


# Fixing/polishing remaining issue with new system:

- [x] bup storage: bup index doesn't work when something is fuse mounted

- [ ] don't 'bup save' if no interesting files have changed.
- [ ] rsync over vpn doesn't have to be encrypted



- [ ] "project is opening" appears on all opening

- [ ] idempotence if multiple clients request certain operations one after the other with same params -- combine them on the server side into one if in queue or current.  E.g., repeated "restarts".

- [ ] save: make it so save ops are spaced out by a certain interval

- [ ] client code: new way to get/view status of replicas and move

- [ ] decide what to do about these
          close_stale_projects:
          replicate_projects_needing_replication:
          replicate_all_with_errors:
          touch_project -- I deleted the code to automate making snapshots.  Should probably write code in storage_server.coffee to snapshot if...

- [ ] use zfs part of status info to update this quota stuff for UI: 'Files and snapshots: 424MB (or 11%) this project's quota of 3.9GB (3.5GB are left)



============================


 - [ ] setup quotas

    /dev/hda1 UUID=4ea80ca3-37b7-447e-831a-700a38627029 / ext3 defaults,errors=remount-ro,usrquota,grpquota 0 1
    sudo su
    mount -o remount /
    quotacheck -vugm /dev/mapper/ubuntu--vg-root -F vfsv1
    quotaon -av

OR with zfs:



  - [ ] update bup and salvus, of course.





New plan:


 - [x] (EASY!) change bup fuse code to fully support metadata: https://github.com/williamstein/bup-1; also set the uid and gid for bup fuse.

>> - [ ] migrate all projects with all snapshots -- in progress...

- [ ] make storage2.coffee that is like storage.coffee, but each zfs replication and snapshot command is replaced by corresponding bup version using bup save and rsync.  Use that bup_storage.py script to make easier.



Architecture of `storage_server.coffee`.  This will be much like a cassandra node.

- We will keep local state of all projects in a sqlite database, using node-sqlite3, rather than using another cassandra database.  Why?  Because local information only needs to be known locally.

- Save -- just makes a snapshot

- Sync:

    - determines replicas
    - records entry in database that project needs to be sync'd to those replicas
    - keeps retrying (with exponential backoff?) until every sync succeeds -- note this is 100% initiated locally






===================================





 - [ ] switch





- [ ] make it so tinc on host restarts automatically if dies...?

- [ ] eliminate the ssh + portforwarding stuff for local_hub, etc. -- this uses up a lot of resources, slow stuff down, has no value given the vpn.

- [ ] remove storage account completely, and also the create_project_user.py script (and similar ones) in /usr/local/bin/.  Move the zpool status crontab stuff to salvus user.

- [ ] storage_server: for *next round* of migrate_delete, need to delete storage/streams path first, due to the above... or at least change the sync code to check filesize.



- [ ] storage: re-image project, which means replace all the sparse image files by one new image file, possibly of a different size; can/will save space

- [ ] storage: re-compact: replace all streams by a single big stream; throws away some history so can save space/time, but means caches have to be recreated.

- [ ] transition everything from my salvus keyspace on old cassandra cluster to new database, cleaning up the tables/schemas in the process
       this took 1 minute:
               copy salvus.projects (project_id,account_id,title) to 'projects.csv';

- [ ] storage_server: make it so can limit the number of simultaneous ZFS operations (?)

- [ ] use cgroups to give salvus user much more priority than all other users compute vm.

- [ ] make it so storage servers in google dc use the google cassandra nodes...?

- [ ] progress for certain actions, e.g., sync_streams, etc.

- [ ] latex: "go to page"

- [ ] need an operations queue for projects... of some sort, e.g., many clients all doing open at once wouldn't be good.

- [ ] latex: display overfull errors with a different color/icon than underflow; also, maybe a checkbox for errors (?)

- [ ] long-term standardization of vm hostnames: [cassandra|compute|web]dc[0,1,2,3]..with it starting count from 1 in each dc...


- [ ] install http://neuro.debian.net/  ?

- [ ] fix this for GOOD!  (line 113 bs)
        salvus@web10:~/salvus/salvus$ vi /home/salvus/salvus/salvus/node_modules/http-proxy/lib/http-proxy/passes/ws-incoming.js
- [ ] fix pexpect issue: https://mail.google.com/mail/u/0/?shva=1#inbox/144a87737d982c93
- [ ] add something to monitor somehow that informs me if hubs have tracebacks!
- [ ] get rid of updatedb from
- [ ] monitor: free space on / and vm/images/ filesystems on host!
- [ ] when opening a sagews file, if sage won't run, still open the file, but with a warning that Sage doesn't work.
- [ ] solid simple GUI editor for %html: https://github.com/guardian/scribe



# DONE
- [x] change hosts to use UTC
- [x] finish first round of migrating projects from -- 3,5,6 (then re-enable for users in database)

- [x] storage: better schedule for applying streams, which does the right thing when forks happen, multiple start times, etc.  I.e., no matter what even when we don't delete streams.   Basically, we start with the stream with the newest end time, then work back until hitting a stream with 0 interval.   Make *that* the only thing returned by the "def streams" function.

- [x] storage: trim function that goes through and deletes all (non-fork) streams that would not be applied (except forks)


   - [x] implement a different offsite backup system of *cassandra* via incrementals of one data center.
         We could recover from anything using these by rebuilding the nodes.  Once I merge the
         two cassandra db's, this would provide a complete backup of everything.
         Just follow this: http://www.datastax.com/documentation/cassandra/2.0/cassandra/operations/ops_backup_restore_c.html

    1. I just did "nodetool snapshot storage -t 2014-03-12" on all padelford nodes at once. -- took about 10 seconds.
    2. I'm doing this on bsd.math, which is writing data at a rate of about 2GB/minute.

        time rsync -axvH cloud3:/home/salvus/vm/images/storage/storage/storage_chunks/snapshots/2014-03-12/ 10.1.3.1/home/salvus/vm/images/storage/storage/storage_chunks/snapshots/2014-03-12/

    3. There are 2.8TB to backup right now, and the target USB disk is 3.7TB.  At this rate, that will take 1400 minutes, or "one day".  WOW.    After making the initial backups, additional ones will be incrementals, which are easy.
       If I keep two copies of this data, I could easily do a complete restore from scratch in 1 day.  That satisfies my requirements.


        time rsync -axvH cloud2:/home/salvus/vm/images/storage/storage/storage_chunks/snapshots/2014-03-12/ 10.1.3.1/home/salvus/vm/images/storage/storage/storage_chunks/snapshots/2014-03-12/

    4. (started March 12 at 4pm) Now using this script with target an *encrypted sparseimage*:

    bsd:database salvus$ more backup.py

            #!/usr/bin/env python

            # make encrypted disk with
            #  sudo hdiutil create -size 3.5t -encryption -type sparse -fs hfs= smc-offsite-2-encrypted

            import os, time


            def cmd(s):
                print s
                os.system(s)

            hosts=[('cloud%s'%i,'10.1.%s.1'%i) for i in [1,2,3,4,5,6,7]]

            snapshot = '2014-03-12'

            for hostname, name in hosts:
                for keyspace in ['storage']:
                    for table in ['storage','storage_log','storage_writing', 'storage_chunks']:
                        path = "/home/salvus/vm/images/storage/%s/%s/snapshots/%s/"%(keyspace,table,snapshot)
                        cmd("mkdir -p %s/%s"%(name,path))
                        cmd("time rsync -axvH %s:%s %s%s &"%(hostname,path,name,path))

    Once the above finishes and works, I'll delete this snapshot and make a brand new one to a different bigger USB drive, and offsite the first.
    Let's get this process going.

    And once I move all data into the same database, we'll hence have excellent backups, with a solid restore process.

    1.5 hours = 172GB, so 2800GB will take about 25 hours.  nice.

- [x] restore the internal firewalling on compute VM's, namely disallow connections on user ports from anywhere except a
      specific whitelist of external hub VM's.  Right now, it's mainly security by obscurity.
          https://mail.google.com/mail/u/0/?shva=1#inbox/1448b75264eeda11


- [x] storage.coffee: replicate (as storage user, using storage project)

- [x] official ipython upgrade: http://trac.sagemath.org/ticket/14713    ; running tests in scratch on compute13a


- [x] storage: should I use a database (+local streams cache) in order to separate compute from storage, which will make compute much, much more dynamically scalable?  ANS -- YES

   - [x] (2:00?) (12:00 --yep 12 hours) write code to split (into small pieces) and save (in parallel) a big object to the database and test.
         - I wrote this demo code.
         - When saving in parallel (with many threads) I guess across the C* cluster, it's about the same as rsync/scp, but seems more robust:
               it's about 10MB/s across my encrypted network/vpn.
         - can't load a big file directly into memory -- will have to do something with streaming and chunking (more complicated code)
         - I think this is viable, but as a separate keyspace (for security and to use a different replication factor)
         - This will be the long-term storage of projects.   We can still cache on local disk.  But view that only as a cache.
         -

    CREATE KEYSPACE storage WITH replication = {'class': 'NetworkTopologyStrategy', 'DC0': '2', 'DC1': '2'};
    CREATE USER storage0 WITH PASSWORD='random...';
    GRANT ALL ON KEYSPACE storage TO storage0;



- [x] cassandra -- redo to use ZFS -- e.g., 5,6,7 will save 100's of GB's of space (and we could use it there).

DONE: 1,2,5,6,7,4,3,10,11,12,13,14,15,
TODO: 16,17,18,19,20,21


n=7
import admin; reload(admin); cloud = admin.Services('conf/deploy_cloud/'); h = lambda x: cloud._hosts('cassandra%s'%n, x, sudo=True, timeout=3600)
cloud.restart('vm',hostname='cassandra%s'%n)  # may take doing it multiple times...
cloud.wait_until_up('10.1.%s.2'%n)

h('zpool create -f cassandra /dev/vdc')
h('zfs set compression=lz4 cassandra')
h('zfs set atime=off cassandra')
h('zfs set logbias=throughput cassandra')
h('rsync -axH /mnt/cassandra/ /cassandra/')
h('umount /mnt/cassandra')
h('zfs set mountpoint=/mnt/cassandra cassandra')
h('zfs snapshot cassandra@init')
h('zpool export cassandra')

--> Which is the same as log in and...

    # fdisk /dev/vdc
    zpool create -f cassandra /dev/vdc
    zfs set compression=lz4 cassandra
    zfs set atime=off cassandra
    zfs set logbias=throughput cassandra
    time sudo rsync -axH /mnt/cassandra/ /cassandra/
    umount /mnt/cassandra
    zfs set mountpoint=/mnt/cassandra cassandra
    zfs snapshot cassandra@init
    zpool export cassandra


--> remove the old image from services file, then

import admin; reload(admin); cloud = admin.Services('conf/deploy_cloud/'); h = lambda x: cloud._hosts('cassandra%s'%n, x, sudo=True, timeout=3600)

cloud.restart('vm',hostname='cassandra%s'%n)
cloud.wait_until_up('10.1.%s.2'%n)
h('zpool import -f cassandra')
cloud.start('cassandra',host='cassandra%s'%n,wait=True)

print cloud._hosts('cassandra%s'%n, 'cd ~/salvus/salvus; . salvus-env; nodetool status', wait=True)


---> Once it is working, move the cassandra-cassandra.img to TRASH.

---
    Net result for cassandra5 -- it takes up 2.2G on disk instead of 150G :-)

        150G    cassandra5-cassandra.img
        2.2G    cassandra5-cassandra-zfs.img

    And migrating to an encrypted SSD device later will be easy (zfs send/recv).  Plus I can snapshot all nodes for a backup-in-place.
    And I can also make offsites of all nodes via zfs send.

---

- [x] build sage-6.2.beta with ipython update:  git fetch git://git.sagemath.org/sage.git u/ohanar/ipython-upgrade
            use system-wide atlas so it's fast to build and portable.
      - [x] ensure clawpack fully works in new sage version
      - [x] more useful crontab monitoring pools:
            */5 * * * * sudo zpool list projects > /home/storage/.zpool.list.projects && cp /home/storage/.zpool.list.projects /home/storage/zpool.list.projects
            */5 * * * * sudo zpool list storage > /home/storage/.zpool.list.storage&& cp /home/storage/.zpool.list.storage /home/storage/zpool.list.storage
            */5 * * * * sudo zpool list cassandra > /home/storage/.zpool.list.cassandra && cp /home/storage/.zpool.list.cassandra /home/storage/zpool.list.cassandra
      - [x] make storage user have more sudo powers:
            # to manage the zfs pool for other projects
            storage ALL=(ALL) NOPASSWD: /sbin/zfs *
            storage ALL=(ALL) NOPASSWD: /sbin/zpool *
            storage ALL=(ALL) NOPASSWD: /usr/bin/pkill *
            # to create users for projects
            storage ALL=(ALL) NOPASSWD: /usr/local/bin/create_project_user.py *
      - [x] automate put latest smc_storage.py in /home/storage/



   - [x] create a new ZFS pool on each compute machine called "storage" with filesystems -- no dedup and no compression.
        storage/images
        storage/streams

        zpool create storage -f /dev/vdd
        zfs set compression=lz4 storage
        zfs set sync=disabled storage
        zfs create storage/images
        zfs create storage/streams
        chown salvus. /storage/streams
        chown salvus. /storage/images
        chmod og-rwx /storage/streams
        chmod og-rwx /storage/images
        zfs create storage/conf
        chmod og-rwx /storage/conf
        chown salvus. /storage/conf


        zpool export storage

    (x) done at UW

- [x] create 425GB/each storage devices on all of 3.1.x.4 for x=1,2,..,8, attach and configure (with care!)
      (only 8 remains)

- [x] export and delete storage on 3.1.x.4 for x=1,2,3.
- [x] create zpool image ubuntu-base-salvus-qcow, and copy over data.

- [x] pexpect bug -- fix in base image and update
       /usr/local/sage/sage-6.2/local/lib/python2.7/site-packages/pexpect.py

- [x] transition the 8 compute machines at Google from Debian to Ubuntu:
       - [x] setup compute-base-disk and run doctests
       - [x] worry about startup script and switch 10.3.4.4 first, since no projects are running there.
          - [x] 10.3.4.4
          - [x] 10.3.3.4
          - [x] 10.3.1.4
          - [x] 10.3.2.4
          - [x] 10.3.5.4
          - [x] 10.3.6.4
          - [x] 10.3.7.4
          - [x] 10.3.8.4

ssh-keygen -f "/home/salvus/.ssh/known_hosts" -R 10.3.1.4; ssh-keygen -f "/home/salvus/.ssh/known_hosts" -R 10.3.2.4; ssh-keygen -f "/home/salvus/.ssh/known_hosts" -R 10.3.3.4; ssh-keygen -f "/home/salvus/.ssh/known_hosts" -R 10.3.4.4; ssh-keygen -f "/home/salvus/.ssh/known_hosts" -R 10.3.5.4; ssh-keygen -f "/home/salvus/.ssh/known_hosts" -R 10.3.6.4; ssh-keygen -f "/home/salvus/.ssh/known_hosts" -R 10.3.7.4; ssh-keygen -f "/home/salvus/.ssh/known_hosts" -R 10.3.8.4


 - [x] Make sure to do this: "zfs set sync=disabled storage" on storage1-8 on gce nodes.

- [x] (1:03) fix bug with raw http server failing.

The top of the log for a failed project is:

    start debug: info: starting tcp serverdebug: read '/proinfo: listening on port 40414
    info: starting raw server...
    info: raw server (port=47050), host='10.1.16.4', base='/3702601d-9fbc-4e4e-b7ab-c10a79e34d3b/raw/'
    sername":"3702601d9fbc4e4eb7abc10a79e34d3b","path":".","host":"10.1.16.4","port":22},"base_url":""}
    error: Uncaught exception: Error: listen EADDRNOTAVAIL
    Trace
        at process.daemon.pidFile (/projects/3702601d-9fbc-4e4e-b7ab-c10a79e34d3b/.sagemathcloud/node_modules/local_hub.js:2160:24)
        at process.EventEmitter.emit (events.js:126:20)
    debug: PARENT: received connection
    debug: PARENT: received connection
    debug: received control mesg {"event":"codemirror_get_session","path":"./.sagemathcloud.log","project_id":"3702601d-9fbc-4e4e-b7ab-c10a79e34d3b","id
    ":"ce075976-4157-4c87-847e-05d57d8397c3"}

(1) what is this /proinfo thing?



- [x] try setting up a cassandra 2.x storage cluster from scratch on *bare metal* (/dedicated public ip GCE's there later) with everything SSL encrypted.

   - [x] read security section of docs
   - [x] <http://www.datastax.com/documentation/cassandra/2.0/cassandra/install/installDeb_t.html>

           add-apt-repository ppa:webupd8team/java; apt-get update; apt-get install oracle-java7-installer libjna-java
               # if the above goes wrong, do this:
               #  rm /var/lib/dpkg/info/oracle-java7-installer*; apt-get purge oracle-java7-installer*; rm /etc/apt/sources.list.d/*java*

           echo "deb http://debian.datastax.com/community stable main" | sudo tee -a /etc/apt/sources.list.d/cassandra.sources.list
           curl -L http://debian.datastax.com/debian/repo_key | sudo apt-key add -
           apt-get update; apt-get install dsc20; service cassandra stop; rm -rf /var/lib/cassandra/data/system/*
           rm /etc/rc2.d/S50cassandra     # so doesn't auto-start -- do that manually depending on node hostname

   - [x] setup insecure cassandra 2.x cluster for testing.

       created and put in /etc/cassandra/
            salvus/conf/deploy_cloud/storage/*.yaml

            NOTE!! -- I forgot to put: "auto_bootstrap: false" at the bottom of cassandra.yaml, which costs me about 2 hours. DANG.

       data location:
            mkdir /home/salvus/vm/images/storage
            chown -R cassandra. /home/salvus/vm/images/storage

       start the seed nodes one at a time, and then start the rest of the nodes:
            service cassandra start
            tail -f /var/log/cassandra/system.log

   - [x] test/benchmark writing using my storage system:
          my 1.4GB blob:
            limit   chunk_size      write        read
              5         10          35-55           51
              5         64          56
          Basically it takes about 55 seconds to send 1400MB.
          To send with scp is about the same.
          With iperf it would be about 18 seconds (?), so there
          is a lot of encryption overhead, evidently.

          Reading and sending are at about the same speed, but one has to happen before the other can... hmm.

          I just tried storing a 380MB file via the DB in cassandra versus my bare metal setup... and
          it took 94s on VM's and 14s on bare metal...

          So this is a GO.

          Testing pushing projects 0-999 into the DB to see how that goes tomorrow morning...





- [x] upgrade python:

   sage -sh
   umask 022; easy_install -U distribute
        # pip install --upgrade clawpack   ## NOT YET

- [x] temporary (line 113 bs)
        salvus@web10:~/salvus/salvus$ vi /home/salvus/salvus/salvus/node_modules/http-proxy/lib/http-proxy/passes/ws-incoming.js



- [x] benchmark a cassandra setup on raw hardware but over VPN: it's 90 seconds to write my 1.4GB test file instead of 60 seconds.
by the way:
salvus@cloud3:~/salvus/salvus$ time openssl aes-128-cbc -in blob -out blob.ssl -pass file:blah
real    0m16.047s

- [x] setup clean cassandra server

- [x] write table that records only active writes

x={};require('cassandra').storage_db((e,d)->x.d=d;x.c=x.d.chunked_storage(id:'3702601d-9fbc-4e4e-b7ab-c10a79e34d3b') )



- [x] write scrub command that deletes anything that is in the active table, but is sufficiently old (based on timestamp)

- [x] during the save this sometimes happens (when pushing load hard):  Make it retry with exponential backoff... like the Netflix code does.
    coffee> error: Query cql('UPDATE storage_chunks SET chunk=?, size=? WHERE chunk_id=?',params=[[16,138,14,232,246,161,0,253,95,133,...) caused a CQL error:
    ResponseError: Operation timed out - received only 0 responses.
    s.verbose=1


- [x] rewrite database code to not use find -- ugly; doesn't clean up properly...
        debug: execute_code: "find /home/salvus/streams//06049aff-157e-4cf1-9f72-adf1aafcd29e -type f -printf %s %P
        "
        debug: Spawn the command find with given args /home/salvus/streams//06049aff-157e-4cf1-9f72-adf1aafcd29e,-type,f,-printf,%s %P
        events.js:72
                throw er; // Unhandled 'error' event
                      ^
        Error: spawn EMFILE
            at errnoException (child_process.js:980:11)
            at ChildProcess.spawn (child_process.js:927:11)
            at Object.exports.spawn (child_process.js:715:9)
            at async.series.exit_code (/home/salvus/salvus/salvus/node_modules/misc_node.js:334:27)
            at /home/salvus/salvus/salvus/node_modules/async/lib/async.js:548:21

    These tests suggest it does work fine:

        coffee> m=require('misc_node');async=require('async');0;
        0
        coffee> f=(i,c)->m.execute_code(command:'find', args:['/tmp/'], cb:(e,out)->c(e))
        [Function]
        coffee> async.mapSeries([0..1000],f,(e)->console.log(e))
        undefined
        coffee> null



- [x] I was able to trigger the find error by pushing the entire directory tree of `node_modules` twice... but the problem was due to not closing fd when calling put.  Very important to fix this.

- [x] error storing a too-large file:

        debug: storage_migrate: done syncing 00bffa4f-314b-4d33-b46c-5b8a1bf78161 in 7.086999893188477 seconds
        DONE { '006a0178-f833-40f0-8fbf-fc6326d482d2':
           { name: 'ResponseError',
             message: 'unable to make int from \'2529408766\'',
             info: 'Represents a error message from the server',
             code: 8704,
             isServerUnhealthy: false,

    change size type to bigint fixes.


- [x] add user/password auth to db:

        ALTER KEYSPACE system_auth WITH REPLICATION = {'class' : 'NetworkTopologyStrategy', 'DC0' : 3, 'DC1' : 3};


- [x] move compute5 data from cloud5 to cloud2 for now, since it is so huge and that disk will run out soon.
  - [x] copy all projects into database

  - [x] write `storage_server` TCP server:

     - [x] create storage_server user and account and password with limited permissions.
        CREATE USER storage_server WITH PASSWORD '...';
        GRANT MODIFY ON table storage TO storage_server;
        GRANT MODIFY ON table storage_writing TO storage_server;
        GRANT MODIFY ON table storage_chunks TO storage_server;
        GRANT MODIFY ON table storage_servers TO storage_server;
        GRANT SELECT ON table storage TO storage_server;
        GRANT SELECT ON table storage_writing TO storage_server;
        GRANT SELECT ON table storage_chunks TO storage_server;
        GRANT SELECT ON table storage_servers TO storage_server;
        GRANT SELECT ON table storage_log TO storage_server;
        GRANT MODIFY ON table storage_log TO storage_server;
        GRANT SELECT ON table project_storage  TO storage_server;
        GRANT MODIFY ON table project_storage  TO storage_server;
        GRANT SELECT ON table project_storage_host TO storage_server;
        GRANT MODIFY ON table project_storage_host TO storage_server;

  - [x] enough sudo to migrate as salvus user

   - [x] mkdir -p /usr/local/sage/sage-6.2/local/share/sage/ext/octave/user; chmod a+rwx /usr/local/sage/sage-6.2/local/share/sage/ext/octave/user
   - [x] update salvus repo
   - [x] update visudo as smc_storage.py
  - [x] start storage_servers running on all compute vm's.



new vm image:

  - [x] logging for storage
  - [x] storage_server: robustness of client when the remote server dies/moves to another port
  - [x] run migrate_delete on some projects as a test:
            - doing it on projects with location = 1
            - doing it on projects with location = 13

   2 errors on 13, none on 1:  ["7bbb2a01-61e0-4d33-932f-07d38013a9f6","5e6b5009-04e7-4bd4-bca4-96071bca5605"]
      - the first seems clearly a ZFS BUG :-(, which only a reboot will fix...
      - second had messed up sparse image -- adding delete to api fixed problems.



  - [x] code to update migration of a given project using a given host:

  Steps for migration update, which should be command in `storage_server`, which gets removed.  Can be unsafe for now.

     - sync from db to local
     - if any files there, mount and call migrate_snapshots
     - if no files there, call migrate
     - call sync_put_delete

  - [ ] design change: consider having the streams cache in a normal directory and the images zpool separate from this.
        - On boot-up, we could simply create the images pool from scratch each time.
        - Alternatively, maybe just ensure that there are at most n images/project-id filesystems, and destroy old ones.


  - [x] update all storage servers with newer code and restart


  - [x] add 4-node n1-highmem-2 + 1TB disk google DC to storage cassandra cluster:
       <http://www.datastax.com/documentation/cassandra/2.0/cassandra/operations/ops_add_dc_to_cluster_t.html>
          - make my snapshot image have newest cassandra-2.0.5 system-wide

                ALTER KEYSPACE "storage" WITH REPLICATION = {'class': 'NetworkTopologyStrategy', 'DC0':'2', 'DC1':'2', 'DC2':'2'};

          - Did "nodetool repair -pr" simultaneously on new nodes, so they're getting data.  It looks like this will take
            about one week.  I'm glad I started now.

  - [x] make change so `storage_server`'s connect exactly to the cassandra servers in *their* data center only.
   - [X] FAIL -- try bup-based backup of everything:
      - the exact files from the DB, for relatively fast restore,
      - provides point-in-time, and also easy incrementals to offsite.
      - *maybe* the deduplication would divide out the replication factor (!); we'll see.
      - the pack files could easily be encrypted... after the fact
      - to test them, we could restore to a testing cluster, which would also be useful for testing purposes.
      ... BUT --  it is a total fail: "Saving: 0.04% (172299/444825403k, 167/595 files) 1302h4m 92k/s"

   - [x] implement a new offsite backup system, which could just involve --
            (1) add a new table storage_write_log that is like the storage_log, but with a ttl, which only records writes
            (2) make a backup machine that can read (but not write!) from the database, which scans that
                table regularly, and sync_get's each modified project to a local directory (the offsite backup).
         - a problem with this is that it's not a complete backup of the whole database (e.g., project, accounts, etc.)
         - it would take a very long time to recover, since the data must be re-read into the database, which is complicated
           and time consuming.




  - [x] get needs to write to a temp file, then move it over -- destroy on fail.
  - [x] this is hanging forever when server isn't running:
        debug: Storage client (undefined:undefined): connect to locked socket
        debug: misc_node: connecting to a locked socket on port 37401...
  - [x] coffee> debug: storage Client(undefined:44646).call({"mesg":{"event":"storage","project_id":"e53d0f98-f674-4569-aa8d-66fac50c61a5","action":"sync","id":"75d1
71ed-5672-4490-b452-a3e4ac4b83eb"},"timeout":3600}): got response -- {"event":"error","id":"75d171ed-5672-4490-b452-a3e4ac4b83eb","error":"No row in table 's
torage' matched condition '[object Object]'","result":[null,null,null,null,[null]],"time_s":28.618000030517578}
No row in table 'storage' matched condition '[object Object]' undefined

- [ ] admin monitor FAIL:

        Connected to 10.1.6.2
        ---------------------------------------------------------------------------
        OperationalError                          Traceback (most recent call last)
        <ipython-input-4-dce543985046> in <module>()
        ----> 1 cloud.monitor.go(15,7)

        /home/salvus/salvus/salvus/admin.pyc in go(self, interval, residue)
           1565                 if now % interval == residue:
           1566                     last_time = now
        -> 1567                     self._go()
           1568             time.sleep(20)
           1569

        /home/salvus/salvus/salvus/admin.pyc in _go(self)
           1533     def _go(self):
           1534         all = self.all()
        -> 1535         self.update_db(all=all)
           1536         self.print_status(all=all)
           1537         down = self.down(all=all)

        /home/salvus/salvus/salvus/admin.pyc in update_db(self, all)
           1509         # Fill in disabled field for each compute node; this is useful to record in
           1510         # the monitor, and is used by the move project UI code.
        -> 1511         v = dict(list(cassandra.cursor_execute("SELECT host,disabled FROM storage_topology", user='monitor', password=password)))
           1512         if 'compute' in all:
           1513             for x in all['compute']:

        /home/salvus/salvus/salvus/cassandra.pyc in cursor_execute(query, param_dict, keyspace, timeout, user, password)
            114             print msg
            115             cur = cursor(keyspace=keyspace, use_cache=False, user=user, password=password)
        --> 116             cur.execute(query, param_dict)
            117     finally:
            118         signal.signal(signal.SIGALRM, signal.SIG_IGN)

        /home/salvus/salvus/salvus/data/local/lib/python2.7/site-packages/cql/cursor.pyc in execute(self, cql_query, params, decoder, consistency_level)
             78         prepared_q = self.prepare_inline(cql_query, params)
             79         cl = consistency_level or self.consistency_level
        ---> 80         response = self.get_response(prepared_q, cl)
             81         return self.process_execution_results(response, decoder=decoder)
             82

        /home/salvus/salvus/salvus/data/local/lib/python2.7/site-packages/cql/thrifteries.pyc in get_response(self, cql_query, consistency_level)
             75         if self.use_cql3_methods:
             76             doquery = self._connection.client.execute_cql3_query
        ---> 77             return self.handle_cql_execution_errors(doquery, compressed_q, compress, cl)
             78         else:
             79             doquery = self._connection.client.execute_cql_query

        /home/salvus/salvus/salvus/data/local/lib/python2.7/site-packages/cql/thrifteries.pyc in handle_cql_execution_errors(self, executor, *args, **kwargs)
            103                                        "more nodes were unavailable.")
            104         except TimedOutException:
        --> 105             raise cql.OperationalError("Request did not complete within rpc_timeout.")
            106         except TApplicationException, tapp:
            107             raise cql.InternalError("Internal application error")

        OperationalError: Request did not complete within rpc_timeout.

  - [x] storage_server: make it so every database operation gets retried until success

 - [x] code to ensure storage server is started (as salvus!) when machine boots up (in `first_boot`*.py): written, now testing

  - [x] for each project:
          - find last modification time and most recent successful migrate_delete time (if any); if migrade_delete > mod time, done.
          - get current location or newest replica location from database
          - run migrate_delete there
        Locations -- right now:
            debug: got 38062 projects
            debug: of these, 22673 are on 'undefined'


 - [x] start a new node with dev hub content, which the other haproxies don't send traffic to:
         10.1.15.5

 - [x] FIRE: can't boot up vm's on 10.1.2.1!

This was missing:

        echo never > /sys/kernel/mm/transparent_hugepage/enabled; echo never > /sys/kernel/mm/transparent_hugepage/defrag
        cat /sys/kernel/mm/transparent_hugepage/enabled /sys/kernel/mm/transparent_hugepage/defrag

     - info about location/storage of a project in the the storage database:
         - cache/use *state* on each node:
            - streams:
                 - last sync time or nothing there
            - image fs:
                 - last recv time or not received
            - sparse image pool:
                 - whether or not mounted
                 - last activity  (modification time of file)
                 - last scrub time
  - [x] storage -- refactored code to have some clean well-named operations, and also store their last usage in the database
  - [x]  use a uuid to refer to storage hosts, instead of an ip address
        - this would make it possible to change the ip address of a compute machine.
        - this is what cassandra does.
        - where would uuid be stored?
        -
 - [x] disk space on cloud7 is getting problematic... 70G in "storage" so can reset that tomorrow morning.

- [x] rewrite send_streams in smc_storage:  need to locate the newest snapshot that we have
      such that there is also a stream that ends there, which isn't too small.
      Then make snapshot that starts from that point.


 - [x] gcutil hints:

    - connect to base image via ssh
            gcutil --service_version="v1" --project="sagemathcloud" ssh  --zone="us-central1-a" "compute-base-vm"

    - snapshot base image
            gcutil --service_version="v1" --project="sagemathcloud" addsnapshot --source_disk "compute-base-disk" --wait_until_complete compute-base-disk-`date +"%Y-%m-%d-%H%M"`


         compute4dc2-projects	compute4dc2-projects2	compute4-storage




    This is what is used now:

         - open_project_somewhere
         - close project
         - save project
         - create_project
         - last_snapshot
         - snapshot project
         - snapshot_listing
         - close_stale_projects
         - storage status

    Plan:

     - [x] add flag to projects table for "storage2 project" -- when set, fully use new storage system everywhere.



  - [x] storage: way to get timestamp of last snapshot of storage pool

  - [x] storage: easy way to delete anything in database that is redundant with that stream

git pull root@10.1.13.4:/projects/3702601d-9fbc-4e4e-b7ab-c10a79e34d3b/salvus/

  - [x] storage/images/project-id fs keeps "locking" after one usage.  very disturbing.  WEIRD.  *must* address this.  even happens on non-loaded system.
  - [x] action queue for project object in server, so that the *server* will do at most one operation at a time...
- [x] reduce current stupid zpool call to once an hour...

- [x] (2:00?) (2:17) fix every bug revealed by migrate

- [x] (1:00?) (0:09+) stress test doing crazy blasts of operations.

    I fixed a bunch of issues, but I've now HIT a *major* problem again, which I thought was easy.  Namely,
    I keep having the `/storage/images/project_id` filesystem *lock* up completely when trying to import
    from it.  This really sucks.
    This happens even with sync=standard.
    And when doing a sync right before the import.
    It's really annoying.
    It's way worse... it's like this... running full blast:

         13739 root      20   0 28560 1352 1116 R   98  0.0  45:34.72 zpool

         13739 ?        R     46:09 /sbin/zpool import -fN project-bec33943-51b7-4ebb-b51b-15998a83775b -d /storage/images/bec33943-51b7-4ebb-b51b-15998a83775b

    It's totally unkillable.  Burns up resources.  Makes zfs stop working.

 Oh my god, after all this testing two things are crystal clear:

    - I HAVE to use Zvol's.  They aren't as fast as sparse images, but they are fast, and much more robust and flexible.
      This means re-doing everything from scratch.

    - I think the chunk size for files in the storage in db should be smaller, so queries are much more likely to succeed in the allocated time, e.g., probably 4mb.

 * Programming-wise, switching to zvols is probably just a few lines of code difference.
 * Data wise, I will honestly have to start completely from scratch, throwing away all database computed over nearly a month (?).
   That said, the rsync's were really slow, and if cassandra can delete data properly, then maybe I can just "read from db"/transform/write back and save quite a lot of work.


 - [x] re-write `smc_storage.py` to use zvols: create zvol_storage.py; with filenames `start--end.zvol.lz4`

    Basic question:

        - (a) should I make a huge (thin) zvol pool, but give it a little quota?
      or
        - (b) or... should I make a small zvol and pool, and figure out how to enlarge it by adding new zvols when needed (much more painful).

    The answer depends on whether the streams "blow up" in size if I do (a).
    The answer seems to be that indeed there is HUGE blow up.
    This means that we need to create several zvol's

    Nice fact!  It is possible to resize the zvol and resize the pool *online*, as explained here: https://github.com/zfsonlinux/zfs/issues/120

           sudo zfs set volsize=10g storage/zvol/test3
           sudo zpool online -e test3 /dev/zvol/storage/zvol/test3

    This really works and is amazing.  So even starting small, we won't ever need more than one zvol.  YES.


    Problem: doing this on a live filesystem that a user is using... could be ugly.

    Unfucking believable.  I just finished going through the whole `smc_storage.py` file rewriting it to use Zvols... and
    it hung on an import. What the hell!

    Also, zvol's are definitely significantly slower than sparse image files.  The only problem with sparse image files really
    is that they also hang on import.

    Trying again zvol's seem to work - maybe the hang was the result of the tons of other crazy testing I had done, e.g., live
    shrinking a device on a pool.

     - [x] try switching to daily build of ZFS for ubuntu: https://launchpad.net/~zfs-native/+archive/daily
     To remove dpkg -r kmod-spl-3.2.0-59-generic kmod-spl-devel kmod-spl-devel-3.2.0-59-generic spl kmod-zfs-3.2.0-59-generic kmod-zfs-devel kmod-zfs-devel-3.2.0-59-generic zfs zfs-devel zfs-dracut zfs-test kmod-spl-devel-3.2.0-60-generic kmod-zfs-3.2.0-60-generic kmod-zfs-devel-3.2.0-60-generic

 - [x] (0:45?) (0:44) implement zvol re-compaction
         1. *ensure* that compression is enabled on the pool that contains the zvol but not on the
            pool that is live on the zvol!
         2. Inside the mounted zpool do this:  "dd if=/dev/zero of=MYFILE bs=1M; rm MYFILE"
         3. We will only do this shrinking rarely, when doing a save *and* the size of the user's
            zpool is (significantly) smaller than the zvol.  It only takes about 5s/gb.
            We'll disable compression, do the shrink, then re-enable compression.
            This isn't the worst trick ever.
    Discussion here: http://comments.gmane.org/gmane.os.solaris.opensolaris.zfs/35630



> - [x] (0:20?) move compute7a - I'm tired of having to worry about disk space there.  In progress in a tmux session.  ETA: 10am-ish

       # this is impressively fast -- 250GB/hour
       rsync -axvH --numeric-ids --delete --progress -e "ssh -T -c arcfour -o Compression=no -x" persistent/ cloud1:vm/images/persistent/incoming/

 - [x] update base vm: new kernel, zfs...

 - [x] (1:00?) write a img --> zvol convertor script.
      YUCK.  Just messing with the img files is giving me the heebie-jeebies...

 - [x] (0:30?) write a new migrate that is much faster and loosing history.  If this is good just 100% start over from scratch.
         rsync <hostname>


- [x] reduce chunk size to 4mb

 - [x] migrate some things, then stress test, as before.

   Observation 1: Damn -- zfs is *incredibly* easy to deadlock.  Scary easy.
                  But... it works fine with only one thread.

   control from 10.1.15.5
   servers on 10.1.3.4 and 10.1.15.5


 - [x] The zvol_storage queue is not working:

 debug: Project(b67f2994-ec40-4104-9697-dcd78853176d).exec(["recv_streams"]): executing zvol_storage.py script
debug: process_zvol_storage_queue: _zvol_storage_queue_running=1; _zvol_storage_queue.length=10
^C
salvus@compute2dc1:~/salvus/salvus$ ps ax |grep zvol_sto
 6819 pts/2    S+     0:00 grep --color=auto zvol_sto
salvus@compute2dc1:~/salvus/salvus$ ps ax |grep zvol_
 6821 pts/2    S+     0:00 grep --color=auto zvol_

debug: _zvol_storage_no_queue: finished running ["--pool","storage","--mnt","/projects/0c284c92-d1ef-4cfa-89dc-1769b71e8a74","--stream_path","/storage/streams/0c284c92-d1ef
-4cfa-89dc-1769b71e8a74","migrate_from","--host","10.1.3.4","0c284c92-d1ef-4cfa-89dc-1769b71e8a74"] -- null
debug: process_zvol_storage_queue: _zvol_storage_queue_running=0; _zvol_storage_queue.length=2
debug: _zvol_storage_no_queue: running ["--pool","storage","--mnt","/projects/0caf51a4-b10c-46fb-8198-07a71804ed17","--stream_path","/storage/streams/0caf51a4-b10c-46fb-819
8-07a71804ed17","migrate_from","--host","10.3.2.4","0caf51a4-b10c-46fb-8198-07a71804ed17"]
debug: execute_code: "zvol_storage.py --pool storage --mnt /projects/0caf51a4-b10c-46fb-8198-07a71804ed17 --stream_path /storage/streams/0caf51a4-b10c-46fb-8198-07a71804ed1
7 migrate_from --host 10.3.2.4 0caf51a4-b10c-46fb-8198-07a71804ed17"
debug: Spawn the command zvol_storage.py with given args --pool,storage,--mnt,/projects/0caf51a4-b10c-46fb-8198-07a71804ed17,--stream_path,/storage/streams/0caf51a4-b10c-46
fb-8198-07a71804ed17,migrate_from,--host,10.3.2.4,0caf51a4-b10c-46fb-8198-07a71804ed17
error: Uncaught exception: Error: spawn ENOMEM



  - [x] (1:00?) (0:50) re-create database from scratch in all three dc's
        - stop all
        - move old files out of the way for today, in case I change my mind.
        - start all; if this doesn't work, be more careful.
        - recreate schemas, keyspaces, tables, and users
        - setup backup system to bsd
               nodetool snapshot storage -t 2014-03-17

- [x] (1:00?) (0:45) spin up vm's for use as new compute vm's -- all migration runs there.

        zpool create storage -f /dev/vdb
        zfs set compression=lz4 storage
        zfs create storage/zvols
        zfs create storage/streams
        chown salvus. /storage/streams
        chmod og-rwx /storage/streams
        zfs create storage/conf
        chmod og-rwx /storage/conf
        chown salvus. /storage/conf

        - start storage server running on them


  - [x] (0:30?) (1:13) test migrating a few projects by hand and moving around.:

l=console.log;s=require('storage_server');c=s.client_project(project_id:'bec33943-51b7-4ebb-b51b-15998a83775b',cb:l)
c.open(host:'10.1.1.5',cb:l)
c.migrate_from(host:'10.3.6.4',cb:l)
c.save()

c=s.client_project(project_id:'3702601d-9fbc-4e4e-b7ab-c10a79e34d3b',cb:l)
c.open(host:'10.1.1.5',cb:l)
c.migrate_from(host:'10.1.13.4',cb:l)
c.save()
c.queue(cb:l)

d=s.client_project(project_id:'6cd832d3-c523-41e3-9e54-c8f2d2e8fa2a',cb:l)
d.open(host:'10.1.1.5',cb:l)
d.migrate_from(host:'10.1.19.4',cb:l)
d.save()
d.queue(cb:l)

c.open(host:'10.1.1.5',cb:l)
d.open(host:'10.1.1.5',cb:l)


  - [x] (2:00?) write migrate3 and start new migration from scratch of everything, with a 0 tolerance for errors policy and no concurrency of ZFS

s=require('storage');s.init()
status=[];s.migrate3_all(start:0,stop:10,status:status,cb:(e)->console.log("DONE",e))


status=[];s.migrate3_all(limit:30,retry_errors:true,start:0,stop:100,status:status,cb:(e)->console.log("DONE",e))

(x for x in status when x.status=='failed')
(x for x in status when x.status=='migrating...')
(x for x in status when x.status=='failed' and JSON.stringify(x.error).indexOf("MOUNT") == -1)

Go bigger:

status=[];s.migrate3_all(limit:10,start:100,status:status,cb:(e)->console.log("DONE",e))

status=[];s.migrate3_all(limit:50,start:20000,status:status,cb:(e)->console.log("DONE",e))

status=[];s.migrate3_all(limit:10,status:status,cb:(e)->console.log("DONE",e))

  - [x] add quota stuff for migrate
  - [x] add timeout for migrate, since some nodes can just take forever to mount, etc.


- [x] backup:

    salvus@cloud3:~$ more bin/tmuxlogin-cassandra-dc1
    tmuxlogin -t cassandra-all 10.1.1.1 10.1.2.1 10.1.3.1 10.1.4.1 10.1.5.1 10.1.6.1 10.1.7.1


    nodetool snapshot -t 2014-03-18

  - [x] (0:55) run migrate on all projects within a certain last usage time.

status=[];require('storage').migrate3_all(limit:10,max_age_h:1,status:status,cb:(e)->console.log("DONE",e))

id='4b35e542-7f48-4f64-b1e0-a142efe2aca5'; d=require('storage_server').client_project(project_id:id); d.state(cb:console.log,host:true)


>>>  - [x] (0:45+) figure out how to *not* have ~/.sagemathcloud, etc., in $HOME.

        # on client
        l=console.log;s=require('storage_server');c=s.client_project(project_id:'bec33943-51b7-4ebb-b51b-15998a83775b',cb:l)
        c.open(cb:l)

        # on server (10.1.1.5)
        sudo zfs create storage/dot

        sudo zfs create storage/sagemathcloud/template
        sudo chown salvus. /storage/sagemathcloud/template
        time rsync -axvH /home/salvus/salvus/salvus/scripts/skel/.sagemathcloud/   /storage/sagemathcloud/template  # would update this on boot...
        # that rsync took over 1 minute!
        sudo zfs snapshot storage/dot/template@init
        time sudo zfs clone storage/dot/template@init storage/dot/bec33943-51b7-4ebb-b51b-15998a83775b    # 2 seconds
        time sudo chown -R bec3394351b74ebbb51b15998a83775b. /storage/dot/bec33943-51b7-4ebb-b51b-15998a83775b    # real    0m0.108s

        sudo cp -rv /usr/local/bin/skel/.ssh /projects/bec33943-51b7-4ebb-b51b-15998a83775b/
        sudo chown -R bec3394351b74ebbb51b15998a83775b. /projects/bec33943-51b7-4ebb-b51b-15998a83775b/.ssh

        Add warning in .sagemathcloud template that NOT BACKED UP.


- [x] (2:00?) create user cgroup, and make cgroup stats part of project definition

NOTE: can artificially clamp down a user this way -- https://access.redhat.com/site/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Resource_Management_Guide/sec-cpu.html

    root@compute7a:/sys/fs/cgroup/cpu/e73ad65285574182bcacc57fe92aa1ab# echo "200000" > cpu.cfs_quota_us

- [x] debug and install ssh/sagemathcloud/cgroup code:

  701  sudo cp scripts/ensure_ssh_access.py /usr/local/bin/
  702  sudo cp scripts/zvol_storage.py /usr/local/bin/
  703  ls -lht /usr/local/bin/*.py
  704  sudo chmod a+xr /usr/local/bin/*.py




>  - [x] update storage servers when above finishes (?) and re-do, but for last day, instead of hour.

status=[];require('storage').migrate3_all(limit:20,max_age_h:24,status:status,cb:(e)->console.log("DONE",e))

status=[];require('storage').migrate3_all(limit:30,max_age_h:3*24,status:status,cb:(e)->console.log("DONE",e))


 - [x] (0:12) .forever: change logfile to be in ~/.sagemathcloud, as explained here: https://github.com/nodejitsu/forever-monitor

 - [x] (0:30?) (0:20) custom bashrc/bash_profile template

 - [x] WAIT -- the `local_hub` daemon should not be served using forever.  If it restarts itself automatically, then the ports get messed up and things don't self heal.  We need the hub to restart it.


 - [x] improve kill

 - [x] write my own cqlsh that (1) gets password from file instead of command line (=insecure).

Simple as making a file:

salvus@cloud3:~$ cat .cassandra/cqlshrc
[authentication]
password=xxxx

Also, be sure to set the permissions restrictive on the ~/.cassandra directory.

chmod og-rwx -R .cassandra/

- [x] "sysctl vm.swappiness=1" -- tried this on hosts since suggested by https://www.youtube.com/watch?v=_IL1u1IIRhc&index=10&list=PLqcm6qE9lgKJzVvwHprow9h7KMpb5hcUU
    if works, make permanent, add to build.py!

- [ ] starting cassandra tricks: fix the limits; also "echo 0 > /proc/sys/vm/drop_caches " freed a lot of memory.



hosts = ['compute%sdc0'%i for i in range(10,22)] + ['compute%sdc1'%i for i in range(1,8)]
[cloud.stop('vm',hostname=h,wait=False) for h in hosts]

---
- [ ] test this to solve ZFS lockup problemS?!
      This looks like what I'm seeing a lot of:  https://github.com/zfsonlinux/zfs/issues/791
      which is basically that doing zpool stuff and zfs send hangs things with DEDUP.
      And that just disabling DEDUP is *not* sufficient.  Noooo!  AFter 20000 projects migrated
      to use dedup. FUCK.  And their solution is terrifying:

           "Sounds like we need to chalk this one up to documentation,"

      What am I supposed to do?   Restart from scratch, but without dedup?







- [x] new vm: install dstat

ps kstart_time -ef|grep zfs|more


- [x] try updating ZFS on all my compute*dc storage-server nodes  to the daily, increase the concurrency and see what happens under stress.

RESULT: no deadlock'd systems.  Mounting is reaosnable

        Last login: Fri Mar 21 15:14:36 2014 from 10.1.3.1
        salvus@compute5dc1:~$ uptime
         15:22:11 up 9 min,  2 users,  load average: 2.38, 96.06, 77.92
        salvus@compute5dc1:~$ time sudo zfs list -r /storage/zvols|wc -l
        1599

        real    0m6.366s
        user    0m0.144s
        sys     0m2.428s

Well there was *one* with a problem (and only one) - a send is taking forever:

    salvus@compute14dc0:~$ ls -lht /storage/streams/78f83ab8-e139-4744-8242-a2751a4b7f45/
    total 512
    -rw-rw---- 1 salvus salvus 0 Mar 21 14:39 2014-03-21T14:39:35--2014-03-21T14:39:35.zvol.lz4.partial
    salvus@compute14dc0:~$ ps ax |grep send
    14401 ?        Dl     0:01 /sbin/zfs send -Dv storage/zvols/78f83ab8-e139-4744-8242-a2751a4b7f45 2014-03-21T14:39:35
    15505 pts/2    S+     0:00 grep --color=auto send
    salvus@compute14dc0:~$ date
    Fri Mar 21 15:27:47 UTC 2014
    salvus@compute14dc0:~$ time sudo zfs list -r storage/zvols|wc -l
    1309
    real    0m6.172s
    user    0m0.240s
    sys     0m2.208s

Can't resend and get a segfaults in dmesg:

   [36409.273057] zfs[15522]: segfault at 2196650 ip 00007f26ac99e8c3 sp 00007f26ab5ae700 error 4 in libzfs.so.2.0.0[7f26ac97a000+3d000]

This and one other node have segfaults in dmesg.  Other zfs stuff still works here though.

I tried exporting the mounted zpool and do the send above... and it worked fine.  Ah ha!

- [ ] test btrfs on a zvol (?)

- *** change blob storage of images code to database -- right now network load makes it time out and plots don't work in worksheets! ***

- [ ] `storage_server.coffee` -- critical to write code to update db state for projects based on reality, otherwise I can't reset a node easily.
- block connections to "ltc.ghash.io"

(in progress) - [ ] migrate all projects with latest version only as snapshot timestamp

- [x] delete existing database
      - drop keyspace storage;
      - delete storage path
 - [x] migrate the db nodes at google from ZFS to ext4 and reboot them to be sure; switch to 512GB/machine -- add more machines when I need more space, or just shutdown, rsync, etc.

 - [x] create new schema: wow, this is helpful -- http://www.ecyrd.com/cassandracalculator/

(in progress) - [ ] sync the bup repos into new db, but excluding/deleting the objects/tmp* files
    - i'm rsync'ing them all off the vm's to cloud2 (then cloud3); deleting tmp* files (since I'm too lazy to figure out --exclude), and
      using a new function in storage to upload everything to the database.

- [ ] database backup to bsd.math *using bup* -- which will dedup it:
         nodetool snapshot storage -t 2014-03-23
         # then....

--> - [ ] rewrite `storage_server` to use bup:

     mkdir /mnt/projects/working
     mkdir /mnt/projects/bup
     rm -rf /mnt/projects/lost+found
     mount -o bind /mnt/projects/working /projects
     chmod a+rx /projects/

salvus@compute5dc1:~/salvus/salvus$ mkdir mnt
salvus@compute5dc1:~/salvus/salvus$ sshfs 3702601d9fbc4e4eb7abc10a79e34d3b@10.1.13.4:salvus/salvus/  mnt/

for i in [3..7]:
     print "echo 'USAGE'; du -sch /home/salvus/vm/images/bup-cassandra"
     print "time /usr/bin/bup on 10.1.%s.1 index /home/salvus/vm/images/storage/storage/storage_chunks/snapshots/2014-03-23"%i
     print "time /usr/bin/bup on 10.1.%s.1 save /home/salvus/vm/images/storage/storage/storage_chunks/snapshots/2014-03-23 -n 10.1.%s.1-storage-storage_chunks-2014-03-23"%(i,i)
     print "echo 'USAGE'; du -sch /home/salvus/vm/images/bup-cassandra"



----

   - have a single bup repo associated to every project.
   - use my file sync tools to sync/store *that repo* via database
   - mounting a project is just restoring last version from bup:
       - really, I would want to just restore the *modified files*, since
         there is likely a live version cached.   Can tell which files
         changed with find, then question is: how quickly copy out just those!?
   - my old forked bup... doesn't work with zfs, but the current bup does.
   - bup_storage.py
   - migration would be:
       - init or get bup repo from db
       - rsync to fixed uid, exactly as now.
       - put in bup repo
   - use would be:
       - use "bup index -m" to get the list of changed files -- always save that along with backup
       - when updating a project from repo, use restore with the union of the lists of files
       - bup fuse with "-o" mounted in a directory that is owned by the user... seems to work very well.


bench: my 3GB salvus-dev project --

   - 13m to bup save the first time on my "slow-ish" zfs on zvol
   - 6m to do a full restore: to ext4 /tmp
   - index takes 1m44s on ZFS zvol pool and 42s on ext4
   - bup resave from/to ext4

- [ ] PLAN:
    - delete my stupid attempt at copies -- too much bandwidth
    - follow https://fedoraproject.org/wiki/Features/Virt_Live_Snapshots#Live_backup
    - make copies of project qcow's on all machines in a DC, so I can start them on a second machine: do the copy with rsync with bandwith limitations!
    - migrate all at some point in time
    - update any that have mods after that point


QUESTIONS:

 - [x] bup file structure and "conflicts" -- what happens?  -- pack files are md5 of content, so they don't overwrite each other

 - [x] many files, etc. -- can run "git gc --auto", and they get shrunk down into a few which WORK -- see http://git-scm.com/book/ch9-7.html
       and many files isn't a scaling problem anymore!  WOW.


----

=====


- [x] create snapshot qcow's of all projects img's and spin up machines, then start them doing a "mount -a".

This fails due to apparmor:  https://bugs.launchpad.net/ubuntu/+source/libvirt/+bug/1004606
But it looks like restarting the VM is required to disable apparmor to work around the bugs.  Great.

    virsh snapshot-create-as  compute10a snap1 snap1-desc --disk-only --diskspec vdc,snapshot=external,file=/home/salvus/vm/images/persistent/compute10a-projects-sn1.img

    qemu-img create -b compute10a-projects.img -f qcow2 bup10-projects.img


Screw it:

tmuxlogin 10.1.10.1 10.1.11.1 10.1.12.1 10.1.13.1 10.1.14.1 10.1.15.1 10.1.16.1 10.1.17.1 10.1.18.1 10.1.19.1 10.1.20.1 10.1.21.1

Disabling apparmor:

    salvus@cloud10:~$ sudo apparmor_status
    apparmor module is loaded.
    3 profiles are loaded.
    3 profiles are in enforce mode.
       libvirt-59418884-82c9-5678-fef0-5d402ef45369
       libvirt-ba36a745-2571-ddda-db80-f03a9ab28c64
       libvirt-f8876046-795a-fcb4-7293-74a5e0587d0b
    0 profiles are in complain mode.
    3 processes have profiles defined.
    3 processes are in enforce mode.
       libvirt-59418884-82c9-5678-fef0-5d402ef45369 (55266)
       libvirt-ba36a745-2571-ddda-db80-f03a9ab28c64 (34826)
       libvirt-f8876046-795a-fcb4-7293-74a5e0587d0b (55260)
    0 processes are in complain mode.
    0 processes are unconfined but have a profile defined.
    salvus@cloud10:~$
    salvus@cloud10:~$ sudo apparmor_parser -R /etc/apparmor.d/libvirt/libvirt-59418884-82c9-5678-fef0-5d402ef45369
    salvus@cloud10:~$ sudo apparmor_parser -R /etc/apparmor.d/libvirt/ libvirt-ba36a745-2571-ddda-db80-f03a9ab28c64
    input in flex scanner failed
    salvus@cloud10:~$ sudo apparmor_parser -R /etc/apparmor.d/libvirt/libvirt-ba36a745-2571-ddda-db80-f03a9ab28c64
    salvus@cloud10:~$ sudo apparmor_parser -R /etc/apparmor.d/libvirt/libvirt-f8876046-795a-fcb4-7293-74a5e0587d0b


Doing migration on a machine:

    time sudo zpool import -fN projects

    In [2]: import hashlib
    In [3]: n = int(hashlib.sha512(project_id).hexdigest()[:8], 16)
    1705215468

    time rsync -axvH --exclude .zfs --exclude .npm --exclude .sagemathcloud --exclude .node-gyp -
    -exclude .cache --exclude .forever --exclude .ssh /projects/0170431a-a4e7-4731-a3e9-3766b5e741c5/ /tmp/0170431a-a4e7-4731-a3e9-3766b5e741c5/

    chown -R 1705215468:1705215468 0170431a-a4e7-4731-a3e9-3766b5e741c5

    # install NEW BUP
    git clone https://github.com/bup/bup bup-new
    ./configure; make -j4
    sudo make install

    cd /tmp; mkdir bup; export BUP_DIR=/tmp/bup/; bup init
    bup index 0170431a-a4e7-4731-a3e9-3766b5e741c5

    ls /projects/0170431a-a4e7-4731-a3e9-3766b5e741c5/.zfs/snapshot|tail  # <-- get time; convert to seconds since epoch with javascript:
    coffee> (new Date('2014-03-15T03:47:29') - 0)/1000
    1394855249

    bup save 0170431a-a4e7-4731-a3e9-3766b5e741c5 --strip -n master -d 1394855249

    # Maybe I want to keep history?

    time rsync -axvH --delete --exclude .zfs --exclude .npm --exclude .sagemathcloud --exclude .node-gyp --exclude .cache --exclude .forever --exclude .ssh  /projects/0170431a-a4e7-4731-a3e9-3766b5e741c5/.zfs/snapshot/2014-01-13T16:50:16/  /tmp/0170431a-a4e7-4731-a3e9-3766b5e741c5/

    time rsync -axvH --delete --exclude .zfs --exclude .npm --exclude .sagemathcloud --exclude .node-gyp --exclude .cache --exclude .forever --exclude .ssh  /projects/0170431a-a4e7-4731-a3e9-3766b5e741c5/.zfs/snapshot/2014-03-12T04\:46\:54/  /tmp/0170431a-a4e7-4731-a3e9-3766b5e741c5/
    chown -R 1705215468:1705215468 0170431a-a4e7-4731-a3e9-3766b5e741c5
    bup index 0170431a-a4e7-4731-a3e9-3766b5e741c5
    bup save 0170431a-a4e7-4731-a3e9-3766b5e741c5 --strip -n master -d 1394599614



- [x] write a Python script `zfs_to_bup.py <project_id>` that fully converts a /projects/project-id that is mounted on the local machine to a directory project-id (in the path where run) that is a bup repo with all snapshots migrated, uid's fixed, etc.




===

--> - [ ] write `bup_storage.py`.

- [ ] modify "chunked_storage" to also support empty directories -- just make empty file that ends in / (?)

- [ ] completely delete cassandra database (again!) -- this time do replication factor of 3 everywhere -- the talks convinced me.

- [ ] read bup repo's into the database.


=====



- [ ] while stress testing pushing projects to db:
debug: ChunkedStorage(10b1c3bd-2743-4d17-bab1-87f91c104d47).put("bupindex"): saving the chunks
debug: ChunkedStorage(10b1c3bd-2743-4d17-bab1-87f91c104d47).put("bupindex"): reading and store chunk 0/0: dcf411a9-185d-48e2-9b7b-9b6e1abacd58

events.js:72
        throw er; // Unhandled 'error' event
              ^
Error: This socket is closed.
    at Socket._write (net.js:635:19)
    at doWrite (_stream_writable.js:221:10)
    at writeOrBuffer (_stream_writable.js:211:5)
    at Socket.Writable.write (_stream_writable.js:180:11)
    at Socket.write (net.js:613:40)
    at async.whilst.self.isRunning (/home/salvus/salvus/salvus/node_modules/node-cassandra-cql/lib/writers.js:242:22)
    at Object.async.whilst (/home/salvus/salvus/salvus/node_modules/async/lib/async.js:616:13)
    at WriteQueue.process (/home/salvus/salvus/salvus/node_modules/node-cassandra-cql/lib/writers.js:225:9)
    at WriteQueue.run (/home/salvus/salvus/salvus/node_modules/node-cassandra-cql/lib/writers.js:219:10)
    at WriteQueue.push (/home/salvus/salvus/salvus/node_modules/node-cassandra-cql/lib/writers.js:214:8)
    at Connection.<anonymous> (/home/salvus/salvus/salvus/node_modules/node-cassandra-cql/lib/connection.js:233:21)
    at Object.callback (/home/salvus/salvus/salvus/node_modules/node-cassandra-cql/lib/connection.js:279:14)
    at next (/home/salvus/salvus/salvus/node_modules/async/lib/async.js:720:43)
    at /home/salvus/salvus/salvus/node_modules/async/lib/async.js:24:16
    at /home/salvus/salvus/salvus/node_modules/node-cassandra-cql/lib/types.js:270:9
    at Object.async.whilst (/home/salvus/salvus/salvus/node_modules/async/lib/async.js:624:13)
    at /home/salvus/salvus/salvus/node_modules/node-cassandra-cql/lib/types.js:258:11
    at Object.q.process [as _onImmediate] (/home/salvus/salvus/salvus/node_modules/async/lib/async.js:728:21)
    at processImmediate [as _immediateCallback] (timers.js:330:15)


- [ ] make open try the next best storage server if the first one is currently down.

- [ ] test/debug server starting, and store the ports that are serving, along with the key, in the database; update them.  This would eliminate the ssh + portforwarding stuff for local_hub, etc. -- this uses up a lot of resources, slow stuff down, has no value given the vpn.

- [ ] (1:30?) auto-close: make timeout part of project definition.
         make this a setting on the server that is set via a message.   Basically, a ttl on a project, when it is opened.
         Must be stored in database.  Server would query database for ttl's...  Not sure.

- [ ] (2:30?) rewrite hub so it uses new project system when project has a certain setting in db, testing on this new server...

- [ ] Function in storage server to start project (template ssh) with ttl. Add a running col to state table and have storage server monitor that for all its projects.  If running false then kill.  Make message to update that col.   Have settings for cgroup stuff, qupta , etc...

- [ ] NOTE: you can't reboot a storage node -- must restart it.  Otherwise, end up with a bunch of hosed zpools.

- [ ] dstat -lrvn 10;    iostat -x 1

- it would be nice if the `storage_writing` table had the client doing the write and also progress updates (?).  maybe a waste though.

- [ ] migrate:

        status=[];require('storage').migrate3_all(retry_errors:true,limit:20,max_age_h:3*24,status:status,cb:(e)->console.log("DONE",e))
        status=[];require('storage').migrate3_all(retry_errors:true,limit:20,status:status,cb:(e)->console.log("DONE",e))

        # by night
        status=[];require('storage').migrate3_all(retry_errors:true,limit:30,status:status,cb:(e)->console.log("DONE",e))

        # by day
        status=[];require('storage').migrate3_all(retry_errors:true,limit:5,status:status,cb:(e)->console.log("DONE",e))


        status=[];require('storage').migrate3_all(oldest_first:true, retry_errors:false,limit:40,status:status,cb:(e)->console.log("DONE",e))

        status=[];require('storage').migrate3_all(retry_errors:false,limit:40,status:status,cb:(e)->console.log("DONE",e))

        cqlsh:storage> select * from compute_hosts where dummy=true; select id,name,timestamp,size from storage_writing where dummy=true and timestamp>='2014-03-20 04:54:20+0000';

        status=[];require('storage').migrate3_all(retry_errors:false,limit:20,status:status,cb:(e)->console.log("DONE",e))


>console.log("DONE",e))

- [ ] use the cloud!  add new compute nodes at google in an automated way - they will *automatically* get used
    by the migration to speed things up!   Doing this with a script, I could spin up a 100, get this done quickly, etc.
    And have done something useful.

    - make 8 vm's compute9dc2...compute16dc2
        - one core each etc. n1-standard
        - 500GB storage zpool/each - detach from existing vm's
    - then add them to my pool

- [ ] possibly change to levelcompactionstrategy as strongly recommended in http://www.datastax.com/documentation/cassandra/2.0/cassandra/operations/ops_configure_compaction_t.html

- [ ] rewrite "delete_lost_chunks" in cassandra.coffee to be more careful.  VERY important before running it.

- [ ] (1:00?) what if reboot machine so not open, but database thinks open..  Need a refresh or something -- when `storage_server` starts up it should just update the database based on scanning what is there.




- [x] use two ZFS filesystems:
    - storage/bups - no compression, no dedup, store all bup repos here; mount as /bups
    - storage/projects -- mount as /projects; yes dedup, yes compression

            zpool create -f storage /dev/sdc
            zfs create storage/projects
            zfs set mountpoint=/projects storage/projects
            zfs set dedup=on storage/projects
            zfs set compression=lz4 storage/projects
            zfs mount storage/projects
            zfs create storage/bups
            zfs set mountpoint=/bups storage/bups
            chmod og-rwx /bups

- [x] make /bups and /projects the canonical things

- [x] change `bup_storage.py` to work with zfs USE quotas (but keep traditional quota code):
          zfs set userquota@7063e18c4477488fbcc6a07a6c9ef5ae=1M projects/scratch
      When syncing, need to set local disk quota on remote before doing sync.

- [x] make `bup_storage.py` script much safer to run with sudo, e.g., remove options

- [x] the `bup_server.coffee` script will run as salvus (!not root!!), but on *compute vm's*, where the ssh private key has been deleted, which reduces attack surface.

- [x] create api interface for bup-based storage to hub.



- [x] use two ZFS filesystems:
    - storage/bups - no compression, no dedup, store all bup repos here; mount as /bups
    - storage/projects -- mount as /projects; yes dedup, yes compression

            zpool create -f storage /dev/sdc
            zfs create storage/projects
            zfs set mountpoint=/projects storage/projects
            zfs set dedup=on storage/projects
            zfs set compression=lz4 storage/projects
            zfs mount storage/projects
            zfs create storage/bups
            zfs set mountpoint=/bups storage/bups
            chmod og-rwx /bups


- [x] make /bups and /projects the canonical things

- [x] change `bup_storage.py` to work with zfs USE quotas (but keep traditional quota code):
          zfs set userquota@7063e18c4477488fbcc6a07a6c9ef5ae=1M projects/scratch
      When syncing, need to set local disk quota on remote before doing sync.

- [x] make `bup_storage.py` script much safer to run with sudo, e.g., remove options

- [x] the `bup_server.coffee` script will run as salvus (!not root!!), but on *compute vm's*, where the ssh private key has been deleted, which reduces attack surface.

- [x] create api interface for bup-based storage to hub.

- [x] enable compression of some rsyncs

- [x] client code: move snapshot browsing/listing to be client side in implementation

