# TOP PRIORITY

- [x] storage.coffee: replicate (as storage user, using storage project)

- [ ] (just wrote to Keith) restore the internal firewalling on compute VM's, namely disallow connections on user ports from anywhere except a
      specific whitelist of external hub VM's.  Right now, it's mainly security by obscurity.

- [ ] storage: should I use a database (+local streams cache) in order to separate compute from storage, which will make compute much, much more dynamically scalable?

   - [ ] (2:00?) write code to split (into small pieces) and save (in parallel) a big object to the database and test.
         

- [ ] storage.coffee: make replicate exclude .partial files.

- [ ] storage.coffee: code that replicates out *all projects* on a given host, k at a time (?)

- [ ] finish first round of migrating projects from -- 3,5,6 (then re-enable for users in database)

- [ ] storage: better schedule for applying streams, which does the right thing when forks happen, multiple start times, etc.  I.e., no matter what even when we don't delete streams.   Basically, we start with the stream with the newest end time, then work back until hitting a stream with 0 interval.   Make *that* the only thing returned by the "def streams" function.

- [ ] storage: trim function that goes through and deletes all (non-fork) streams that would not be applied (except forks)

- [ ] implement storage.coffee Storage class that does:
       - snapshot
       - replicate
       - mount
       - close
      using new system if /storage/streams/project_id exists.

- [ ] official ipython upgrade: http://trac.sagemath.org/ticket/14713    ; running tests in scratch on compute13a


# LATER

# UI

- [ ] solid nice simple GUI editor for %html: https://github.com/guardian/scribe



# DONE


- [x] cassandra -- redo to use ZFS -- e.g., 5,6,7 will save 100's of GB's of space (and we could use it there).

DONE: 1,2,5,6,7,4,3,10,11,12,13,14,15,
TODO: 16,17,18,19,20,21


n=7
import admin; reload(admin); cloud = admin.Services('conf/deploy_cloud/'); h = lambda x: cloud._hosts('cassandra%s'%n, x, sudo=True, timeout=3600)
cloud.restart('vm',hostname='cassandra%s'%n)  # may take doing it multiple times...
cloud.wait_until_up('10.1.%s.2'%n)

h('zpool create -f cassandra /dev/vdc')
h('zfs set compression=lz4 cassandra')
h('zfs set atime=off cassandra')
h('zfs set logbias=throughput cassandra')
h('rsync -axH /mnt/cassandra/ /cassandra/')
h('umount /mnt/cassandra')
h('zfs set mountpoint=/mnt/cassandra cassandra')
h('zfs snapshot cassandra@init')
h('zpool export cassandra')

--> Which is the same as log in and...

    # fdisk /dev/vdc
    zpool create -f cassandra /dev/vdc
    zfs set compression=lz4 cassandra
    zfs set atime=off cassandra
    zfs set logbias=throughput cassandra
    time sudo rsync -axH /mnt/cassandra/ /cassandra/
    umount /mnt/cassandra
    zfs set mountpoint=/mnt/cassandra cassandra
    zfs snapshot cassandra@init
    zpool export cassandra


--> remove the old image from services file, then

import admin; reload(admin); cloud = admin.Services('conf/deploy_cloud/'); h = lambda x: cloud._hosts('cassandra%s'%n, x, sudo=True, timeout=3600)

cloud.restart('vm',hostname='cassandra%s'%n)
cloud.wait_until_up('10.1.%s.2'%n)
h('zpool import -f cassandra')
cloud.start('cassandra',host='cassandra%s'%n,wait=True)

print cloud._hosts('cassandra%s'%n, 'cd ~/salvus/salvus; . salvus-env; nodetool status', wait=True)


---> Once it is working, move the cassandra-cassandra.img to TRASH.

---
    Net result for cassandra5 -- it takes up 2.2G on disk instead of 150G :-)

        150G    cassandra5-cassandra.img
        2.2G    cassandra5-cassandra-zfs.img

    And migrating to an encrypted SSD device later will be easy (zfs send/recv).  Plus I can snapshot all nodes for a backup-in-place.
    And I can also make offsites of all nodes via zfs send.

---

- [x] build sage-6.2.beta with ipython update:  git fetch git://git.sagemath.org/sage.git u/ohanar/ipython-upgrade
            use system-wide atlas so it's fast to build and portable.
      - [x] ensure clawpack fully works in new sage version
      - [x] more useful crontab monitoring pools:
            */5 * * * * sudo zpool list projects > /home/storage/.zpool.list.projects && cp /home/storage/.zpool.list.projects /home/storage/zpool.list.projects
            */5 * * * * sudo zpool list storage > /home/storage/.zpool.list.storage&& cp /home/storage/.zpool.list.storage /home/storage/zpool.list.storage
            */5 * * * * sudo zpool list cassandra > /home/storage/.zpool.list.cassandra && cp /home/storage/.zpool.list.cassandra /home/storage/zpool.list.cassandra
      - [x] make storage user have more sudo powers:
            # to manage the zfs pool for other projects
            storage ALL=(ALL) NOPASSWD: /sbin/zfs *
            storage ALL=(ALL) NOPASSWD: /sbin/zpool *
            storage ALL=(ALL) NOPASSWD: /usr/bin/pkill *
            # to create users for projects
            storage ALL=(ALL) NOPASSWD: /usr/local/bin/create_project_user.py *
      - [x] automate put latest smc_storage.py in /home/storage/



   - [x] create a new ZFS pool on each compute machine called "storage" with filesystems -- no dedup and no compression.
        storage/images
        storage/streams

        zpool create storage -f /dev/sdc
        zfs set sync=disabled storage
        zfs create storage/images
        zfs create storage/streams
        chown storage. /storage/streams
        chown storage. /storage/images
        chmod og-rwx /storage/streams
        chmod og-rwx /storage/images
        zpool export storage

    (x) done at UW

- [x] create 425GB/each storage devices on all of 3.1.x.4 for x=1,2,..,8, attach and configure (with care!)
      (only 8 remains)

- [x] export and delete storage on 3.1.x.4 for x=1,2,3.
- [x] create zpool image ubuntu-base-salvus-qcow, and copy over data.

- [x] pexpect bug -- fix in base image and update
       /usr/local/sage/sage-6.2/local/lib/python2.7/site-packages/pexpect.py

- [x] transition the 8 compute machines at Google from Debian to Ubuntu:
       - [x] setup compute-base-disk and run doctests
       - [x] worry about startup script and switch 10.3.4.4 first, since no projects are running there.
          - [x] 10.3.4.4
          - [x] 10.3.3.4
          - [x] 10.3.1.4
          - [x] 10.3.2.4
          - [x] 10.3.5.4
          - [x] 10.3.6.4
          - [x] 10.3.7.4
          - [x] 10.3.8.4

ssh-keygen -f "/home/salvus/.ssh/known_hosts" -R 10.3.1.4; ssh-keygen -f "/home/salvus/.ssh/known_hosts" -R 10.3.2.4; ssh-keygen -f "/home/salvus/.ssh/known_hosts" -R 10.3.3.4; ssh-keygen -f "/home/salvus/.ssh/known_hosts" -R 10.3.4.4; ssh-keygen -f "/home/salvus/.ssh/known_hosts" -R 10.3.5.4; ssh-keygen -f "/home/salvus/.ssh/known_hosts" -R 10.3.6.4; ssh-keygen -f "/home/salvus/.ssh/known_hosts" -R 10.3.7.4; ssh-keygen -f "/home/salvus/.ssh/known_hosts" -R 10.3.8.4


 - [x] Make sure to do this: "zfs set sync=disabled storage" on storage1-8 on gce nodes.

- [x] (1:03) fix bug with raw http server failing.

The top of the log for a failed project is:

    start debug: info: starting tcp serverdebug: read '/proinfo: listening on port 40414
    info: starting raw server...
    info: raw server (port=47050), host='10.1.16.4', base='/3702601d-9fbc-4e4e-b7ab-c10a79e34d3b/raw/'
    sername":"3702601d9fbc4e4eb7abc10a79e34d3b","path":".","host":"10.1.16.4","port":22},"base_url":""}
    error: Uncaught exception: Error: listen EADDRNOTAVAIL
    Trace
        at process.daemon.pidFile (/projects/3702601d-9fbc-4e4e-b7ab-c10a79e34d3b/.sagemathcloud/node_modules/local_hub.js:2160:24)
        at process.EventEmitter.emit (events.js:126:20)
    debug: PARENT: received connection
    debug: PARENT: received connection
    debug: received control mesg {"event":"codemirror_get_session","path":"./.sagemathcloud.log","project_id":"3702601d-9fbc-4e4e-b7ab-c10a79e34d3b","id
    ":"ce075976-4157-4c87-847e-05d57d8397c3"}

(1) what is this /proinfo thing?




