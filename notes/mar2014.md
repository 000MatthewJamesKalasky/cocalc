# TOP PRIORITY

- [ ] - [ ] upgrade to sage-6.x:  git fetch git://git.sagemath.org/sage.git u/ohanar/ipython-upgrade
      - [ ] ensure clawpack fully works in new sage version
      - [x] make storage user have more sudo powers
      - [ ] put latest smc_storage.py in /home/storage/
      - [ ] push out a new vm base.
      - [ ] update vm base in services file.

- [ ] add 512G storage qcow2 image to every compute machine in services file

- [ ] restart vm's due to controlling vm.py failing: 5,6,7, 10, 17-20

- [ ] create a new ZFS pool on each compute machine called "storage" with filesystems:
        storage/images
        storage/streams

- [ ] implement storage.coffee Storage class that does:
       - snapshot
       - replicate
       - mount
       - close
      using new system if /storage/streams/project_id exists.

- [ ] storage3: better schedule for applying streams, which does the right thing when forks happen, multiple start times, etc.  I.e., no matter what even when we don't delete streams.   Basically, we start with the stream with the newest end time, then work back until hitting a stream with 0 interval.   Make *that* the only thing returned by the "def streams" function.

- [ ] storage: function that goes through and deletes all (non-fork) streams that would not be applied (except forks).

- [ ] cassandra -- redo to use ZFS -- e.g., 5,6,7 will save 100's of GB's of space (and we could use it there).

        fdisk /dev/vdc
        zpool create cassandra /dev/vdc
        zfs set compression=lz4 cassandra
        zfs set atime=off cassandra             # see http://www.slideshare.net/planetcassandra/c-summit-2013-practice-makes-perfect-extreme-cassandra-optimization-by-albert-tobey
        zfs set logbias=throughput cassandra
        rsync -axvH /mnt/cassandra /cassandra/
        # rm stuff

      On startup, right now I have to manually do this:

         sudo zpool import -f cassandra

      I could just stick this in the /etc/rc.local.

    Net result for cassandra5 -- it takes up 2.2G on disk instead of 150G :-)

        150G    cassandra5-cassandra.img
        2.2G    cassandra5-cassandra-zfs.img

    And migrating to an encrypted SSD device later will be easy (zfs send/recv).  Plus I can snapshot all nodes for a backup-in-place.
    And I can also make offsites of all nodes via zfs send.


- [ ] add functionality to vm.py to *add* a device dynamically, so I can create new pools (if linux supports this).


# OTHER

# DONE