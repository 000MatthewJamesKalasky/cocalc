{"desc":"#0 #v1 #syncstring #now (2:30?)\nautomatic version snapshotting when editing files\n\nSomehow enforce rules on backend:\n  - one can never insert a patch in the syncstring table that is more than x minutes old\n\nstep 1:\n\n- [x] (2:25) when call snapshot need to: (1) set the snapshot in the patches table, (2) update the syncstring table to change the last_snapshot timestamp; when loading syncstring, use last_snapshot to restrict how much needs to be loaded by default\n\n- [x] (1:38) loading old history\n\n- [x] (0:45?) (0:47) integrate loading all history with the history slider page\n\nstep 2 -- syncstring \"manager\"\n\n- [x] (1:00?) (1:08) automatic snapshots: somehow ensure there is a snapshot that is at least t steps back from the most recent patch, but also at least t steps from the previous snapshot.  Maybe t = 500?\n\nworry:\n\n- [x] (0:10?) (0:01) when getting a given version, ensure that the best snapshot is used, rather than starting from the beginning (this is just a 10-minute double check)\n\n\n\nWhat we want is that every time n patches are saved since the last snapshot plus k seconds elapses, a new snapshot is made at k seconds ago.  Problem: who makes this new snapshot?  As long as the rules are well defined, and making a snapshot is idempotent, it doesn't matter too much.\n\nThis requires changing the database schema slightly.  This change is **critical** since right now, each time a new user gets added, the full base snapshot gets changefeed'd out to everybody.  Also, we are only saving one snapshot right now, not many of them. \n\n","position":-1.3134439250479772,"last_edited":1453482601579,"task_id":"9d38081d-8304-46ae-9ef5-04678f97be12","done":1453482601169}
{"desc":"#now (1:42) disconnect and reconnect needs to send patches made when offline.","position":-2,"last_edited":1448166337932,"task_id":"df5f6b4d-4a8a-43d1-bc08-aaaee7396c70","done":1448166205961}
{"desc":"#v2 #syncstring (3:30?) #8\naddress possibility that the same account_id records two edits to the same file at the same ms in time, but say from two browser sessions.  \n\nNightmare scenario: classroom of students all logged into same account editing same document\n\nSolution: somehow make it so that when client does a write, if there is already a record with that key (and a different value!!!!!!), then the write fails with appropriate error. The client can then increase the timestamp by 1ms and try again.\n\nPlan:\n\n- [ ] (1:00?) add generic schema option to make tables immutable, in that you can write new things, but you can't change anything already there.  If you do, then it just gives an error.  This could be generally useful in other ways.\n- [ ] (0:15?) make the patches table immutable\n- [ ] (0:45?) make synctable aware of immutable tables: add an event that is emitted whenever an issue involving immutability is encountered.\n- [ ] (1:30?) if hit immutability issue for a given patch with timestamp, add 1ms to time and try again until; make sure to update any relevant data structures in syncstring.\n\n\nOther worse ideas:\n\n- if same user is editing the syncstring in more than one browser, create multiple user numbers for them?\n- assign a slight time skew for each one, i.e., add a few ms of time to each one to make conflict impossible.","position":17,"last_edited":1451605251418,"task_id":"7d7cde76-1cdb-4900-a46c-54fe951f3c6a"}
{"desc":"synctable ack: make it so that each changefeed update waits k seconds for an ack; if it fails, it cancels that changefeed.\n\n#wontfix","position":2,"last_edited":1448215322102,"task_id":"ab953de2-e995-4a13-a379-aee3f94e01e6","done":1448215321680}
{"desc":"#now -- only save patches at most once every k ms, not after every single change.","position":-1.5,"last_edited":1448215211632,"task_id":"607373b4-218e-452c-b555-ee19fe0f7cb2","done":1448215211215}
{"desc":"#d4 #v1 #syncstring #offline #rebase (3:00?)\nimplement way to rebase/apply offline changes, if they are sufficiently old.\n\nWhen user comes back online (in reconnect function) instead of just sending all of our offline changes to the server, delete them from the table, and emit a message saying \"here are some unsaved changes\".  The syncstring could then apply those changes--i.e., rebase-- then make a new patch at the top that has the same effect as those changes.  (The user could be asked if they want to apply or not...)\n\n- [ ] (1:00?) add option to synctable so that instead of just applying offline changes on reconnect, they are removed and a message is emitted.\n- [ ] (1:30?) in syncstring, handle that emitted message; have to replay all changes merging our with whatever new changes have happened.  Avoid weird issues of having stuff disappear from synctable messing up our data structure.  Obtain resulting new string... and save as a new commit.","position":-1.3134439250425203,"last_edited":1458142700988,"task_id":"ae17e5dc-d0de-4294-b372-3b479afbb188"}
{"desc":"#v1 #syncstring #history #0 (4:30?) (1:31)\nhistory browser for files using new sync system\n\nTHE Design:\n\n- Same window instead of a separate tab (have another row of buttons to control, but make generic enough that it will work for everything, including tasks, jupyter, normal editing documents, etc.)\n- Load more history functionality for syncstring -- so can only show whatever is already in memory for editing, then have a button to \"load more\".\n\n\n- [ ] (1:30?) create ui that shows a history control row, basically like the current one, which pushes the editor start down by one row.\n- [ ] (0:45?) add a \"load more\" button when the history isn't fully in memory\n- [ ] (0:45?) make document be read only when history control shown.\n- [ ] (1:00?) display document when slider selects a particular point in history\n- [ ] (1:00?) button to revert document to display state\n","position":-1.312744140625,"last_edited":1451635201798,"task_id":"4c6d285c-e59f-4cc8-8af6-f1a5d71ea074","done":1451635201390}
{"desc":"#0 #local_hub (1:30?) (1:06) #now\nmake it so local_hub watches disk file, and if file changes on disk load from disk, and set string to that.","position":-1.314453125,"last_edited":1450506895847,"task_id":"887d5392-5e2a-486d-bbae-7b221f8fdd87","done":1450506895436}
{"desc":"#0 #local_hub (0:30?)\nimplement: save to disk\n\n- [ ] (0:20?) open syncstring in local_hub, if not already opened\n- [ ] (0:05?) compute current version\n- [ ] (0:05?) write it to file","position":-1.31298828125,"last_edited":1451435598837,"task_id":"a52d149d-58d4-4f14-821d-2223af53958b","done":1451435598420}
{"desc":"#local_hub\nfile change on disk resets live version to what is on disk","position":-1.03125,"last_edited":1448216806619,"task_id":"cebc557f-2029-4e6c-8d17-f36fdbbb9d8e","deleted":true}
{"desc":"#0 #local_hub (3:30?)\nability for local_hub to connect to synctables (no worry about security at all)\n- [x] (1:00?) (2:12) message framework from local_hub to hub\n    - keep map in memory of all connections from hubs to this local_hub, and when they last sent us a message\n    - function that calls local hub, i.e., sends a message.  If hub connected, it will try from most recent to least hub until one works (or fail if none work).\n    - also plugs into thing that receives messages to do callback.\n- [x] (1:30?) (1:42+) new message type from local_hub to hub for doing a query, plus handling\n- [x] (1:00?) project get query\n- [x] (1:00?) (1:30+) hook in synctable to this query functionality\n","position":-1.34375,"last_edited":1450380171740,"task_id":"f614e5cc-6dda-4620-8dd2-7b86819b638d","done":1450380171327}
{"desc":"#v1 #local_hub #2 (2:30?)\nsecurity model -- so project can only access very specific tables\n\n- [ ] message to handle query from local_hub: come up with way to allow queries; or for now only allow querying the syncstring and patches tables (?)\n\nThere are going to be many problems with the tables that might be difficult!  Address them all when things are working.","position":-1.015625,"last_edited":1451766621309,"task_id":"a4a5fc03-bcd1-4a28-a88c-0944f64e082c","done":1451766620900}
{"desc":"#syncstring #0 (2:00?)  (2:19) #now\nreplace existing syncstring with new db based one - so syncdb, chat, tasks, jupyter, etc., all automatically switch to use new syncstring...\n\n- [x] basic switch basic editors to work with new approach\n- [x] basic switch of syncdb-based stuff to work\n- [ ] really do new editors\n- [ ] (1:00?) read the exact api of old diffsync based syncstring\n- [x] (1:00?) write something the same based on new syncstring, then plug in\n- [ ] (3:00?) sage worksheets ! tricky!","position":-1.313446044921875,"last_edited":1451452398893,"task_id":"63639ea3-0267-421e-9626-e4199da18a59","done":1451452398482}
{"desc":"#syncstring (1:30?)\nreplace existing diffsync doc with new db based one - so file editing uses this.\n\n- [ ] (1:00?) rewrite some code in syncdoc.coffee, after replicating the syncstring api","position":-1.31341552734375,"last_edited":1451604953969,"task_id":"3b426f45-72da-4cbd-b125-3252ce3049a4","done":1451604953560}
{"desc":"#0 #v1 #local_hub (1:00?) #now\nopening a file on disk should properly create sync session using the file contents, but take into account timestamp.\n\n- [ ] (0:15?) open syncstring editing session.\n- [ ] (0:45?) if timestamp (??) of file is newer than that of most recent edits, set the syncstring to contents of file\n\ncheck to see if the above makes sense, given how git, etc., works.   Alternative could be that if timestamp of file changes, even to something old, then we always revert to that.  Not sure.  Could be an option.","position":-1.3134439270943403,"last_edited":1451763224008,"task_id":"20195c89-6f54-4d85-be3f-ccbea5c95d06","done":1451763223597}
{"desc":"#v1 #local_hub (1:30?)\ndisble/comment out all existing diffsync code in the local hub, etc., (since replaced by new syncstring code.)","position":16,"last_edited":1456462112377,"task_id":"7bd9b51d-b57e-4129-9d04-f30899e26af4","done":1456462111968}
{"desc":"#v2 #9 #hub #limit #unclear\nlimit the document **history size** on the backend to avoid insanity.\n\nIdea: Make it so after n patches (say n=10000), something on the backend automatically truncates to a snapshot.  This could be done via some sort of generic capped collection notion in the schema.\nHowever, to keep it efficient, would have to be careful about only doing the truncation if there are a lot of writes happening in a relatively short amount of time (if there aren't... who cares).","position":10,"last_edited":1451605193691,"task_id":"9b2bc540-4880-40b6-9440-c7d8a1607bba"}
{"desc":"#d4 #v1 #0 #hub #limit #unclear (4:00?)\nlimit the editable document **size** on the backend to avoid insanity.\n\n- [x] (1:28) syncstring file -- don't allow opening documents at all over a certain size.  Gracefully fail.\n- [ ] (1:00?) if backend file suddenly changes to be large, do something appropriate.  Obviously we need some sort of error state, where UI editing stops, and user is told \"file is very big -- you must edit using the command line or otherwise\".  Right now you silently lose data on save.\n- [ ] (1:00?) don't allow writing a patch beyond a certain size (already messages beyond a certain size are dropped)\n- [ ] (2:00?) make it so frontend and/or local hub reasonably gracefully fail when get an error about trying to write something that isn't allowed.","position":-1.3134439561981708,"last_edited":1458310566963,"task_id":"f9cab25d-9be6-49c4-a7c3-b2059d115c28"}
{"desc":"#d3 #v1 #0 #syncstring #security  #now (1:30?) (3:00+)\nenforce schema for mapping thing that defines a syncstring to a uuid\n\n- [x] creation of new files doesn't x work now.\n\nFor files in a project:\n\n   sha1('file', project_id, 'path/to/file')\n\nThen we can use something else besides \"file\" for other things, e.g., 'chat' for ephemeral chats, etc.\n\nWe then have to enforce exactly this in the schema and backend.\n\n#unclear -- we might not do this at all!  The reason is because of compound primary keys, etc.\n\n---\n\nThis is getting really painful/complicated.\n\nThe other option: \n\n- for a given syncstring, have a map of project_id's so that all collabs of that project are allowed to use it.\n- have (have) later account_id's\n\n--- \n\nWAIT: idea!  require the project_id, path for syncstring, but then *use* the syncstring table to *determine* read/write permissions for the patches/cursors/etc. tables.\nThis lets us use the sha1 hash, not require any sort of global agreement, and avoid all issues of race conditions :-).  It's eloquent.\n\n- [ ] 'string_write' -- in analogy with 'project_write', if user does a set query, check that this is valid.  should cache \"yes\" result for a while...?\n\n","position":-1.313443956292074,"last_edited":1458191511044,"task_id":"cd0becca-f9d6-49b1-b324-76a894728c01","done":1458191510489}
{"desc":"(2:00?) #client #0\nimplement cursors\n\n- [x] (0:30?) create another table, like syncstring, that is for cursors for a document\n- [x] (0:30?) (0:30) when editing set the cursor **positions** for a given client, and account_id, and when set.\n- [x] (1:00?) (1:53) everybody sees and renders accordingly\n\n- [x] (0:30?) (0:37) only broadcast cursor movement when user-initiated\n\n- Worry: Join synctable?\n- Cursors: who should report if same account but many client sessions?  probably last to move?\n\n```\nt = smc.client.sync_table({cursors:[{id:'string_id', locs:null, time:null}]})\nt.set({id:['string_id', 'user_id2'], locs:[{x:15,y:100}, {x:25,y:30}], time:new Date()}, 'none', function(e){console.log(e)})\n```\n","position":-1.31396484375,"last_edited":1451196433843,"task_id":"8929411f-daa6-46ff-9385-4ebb3d7e030c","done":1451196433425}
{"desc":"#0 (2:00?) (3:01) #client #syncstring #local_hub #now\nsave to file\n\nmake it so the save-to-file state is part of the syncstring state (in the database) and synchronized/persisted across all users.\n","position":-1.3203125,"last_edited":1450498987909,"task_id":"98614c7c-910c-4760-a655-f77bf51cfe0f","done":1450498987501}
{"desc":"#d4 #1 #v2 #file (3:00?)\nfile move/rename\n\nActually properly handle file rename/move.  \n\nNot sure how, but natural would be to abstract away the id of the sync editing session (so has nothing to do with project_id/path).  Then file rename is a quick single change in the syncstrings table (namely, changing which file is being edited).  I like this.","position":9.5,"last_edited":1458309369048,"task_id":"20d61a3d-bc49-4474-a17a-f5c05b5529f9"}
{"desc":"#now (0:07) (0:15?) \n(unrelated to sync) fix the download link","position":-1.2421875,"last_edited":1448374552390,"task_id":"12238a67-7317-4ae6-be80-aa43738d88c3","done":1448374551986}
{"desc":"","position":-1.24609375,"last_edited":1448374602225,"task_id":"4e021924-d9c6-4141-81bf-94d71297b37f","deleted":true}
{"desc":"#local_hub (5:00?)\nlocal_hub refactor\n \n- [x] (2:30?) (0:10+) refactor local_hub.coffee into several files so code is easier to work on.\n    - [x] (0:20) diffsync_file\n    - [x] (0:05) desctructured import functions\n    - [x] (0:20?) (3:00) raw server  -- (way harder than expected since I wanted to also test; but valuable to get another one of those \"2 hour\" projects out of the way!)\n    - [x] (0:20?) (0:19) printing files\n    - [x] (0:20?) (0:55) console sessions\n    - [x] (0:20?) (0:39) optimize console session start (?) -- instead did maybe a slight cleanup.\n    - [x] (1:10?) (1:11) \"CodeMirror\" editing sessions and sage sessions\n    - [x] (0:30?) (0:32) factor out reading/writing file to project\n    - [x] (0:45?) (1:20) #now -- refactor jupyter server (?) -- and get it to work\n    \n- [x] more refactor:\n    - [x] (0:20?) (0:03+) blob handling -- do this later after have better way to send messages to hub\n\n- [x] (0:54+) delete lots of unused code, e.g., sage session.\n\n- [x] fix that sending blobs is flakie/broken in dev mode\n    ","position":-1.248046875,"last_edited":1448482559779,"task_id":"fce35c3a-8ebe-4fbd-a3ce-a9ac3d7a0b13","done":1448482559361}
{"desc":"#0 (1:00?) (0:26) #now merge in master","position":-1.2490234375,"last_edited":1450237972215,"task_id":"2caef818-2d72-40ef-9e8f-44ad3563854e","done":1450237971808}
{"desc":"#0 (1:30?) (1:05) #now\ninvestigate compressing the file snapshots.\n\nOptions..?\n\n- lz-string: http://pieroxy.net/blog/pages/lz-string/guide.html So in my testing in browser, about 1MB of coffeescript (not repeating – several different files) compressed to about 70k (so 7% of original size); and this takes about 250ms.; Decompressing takes about 30ms..    A 4MB file would lock the browser for 1s.  Not acceptable.\n\nWe could: (1) use web workers, or (2) use the local_hub, or (3) break up the strings to make lz-string more async.\n\n#rejected\n\nFrom the point of view of the browser in memory, there is no advantage to using compression at all – only a disadvantage, since it takes cpu cycles, and more memory to hold both the compressed and uncompressed version of things for some amount of time.\n\n4 minutes ago\nW\nWebsockets should/will/already do (?) support much better compression natively.\n\n4 minutes ago\nW\nIn any case, whether or not they do it now, they will at some point, and that is the place to deal with size. So doing anything related to compression in the browser isn’t really beneficial to the browser.\n\n1 minute ago\nW\nSo the only real motivation is reducing the database size.\n\n1 minute ago\nW\nHowever, I can just used a compressed filesystem, so that what rethinkdb stores is compressed.\n\nless than a minute ago\nW\nI definitely want to switch to running rethinkdb on zfs + compression, so I get snapshots anyways. ok, do don’t compress.","position":-1.375,"last_edited":1457846462264,"task_id":"add4ce43-42b0-4a1a-9969-3a9974b9d2af","done":1450306507790}
{"desc":"#0 #local_hub (1:00?) (1:47) #now\nget syncstring to work from local_hub","position":-1.328125,"last_edited":1450486278128,"task_id":"7cd3ec92-325e-45e4-a99e-41e6161fa79b","done":1450486277721}
{"desc":"#v2 #9 (0:45?)\nmake set queries on synctable verify that the keys being set are allowed by the original query.\n\nthis is mainly to avoid user/author confusion (me)","position":-1.313443956329138,"last_edited":1451605179497,"task_id":"465f4480-598f-43bf-82ca-cd9a7baae726"}
{"desc":"#v1 #0 #now (0:45?) (1:20)\nmake the save ui button properly reflect state of save and hash\n\n(so is perfectly well defined across clients, etc.)\n\n- [x] (1:14) codemirror document editor\n","position":-1.3134441375732422,"last_edited":1451626549906,"task_id":"f74d1a70-2f0b-4d86-85b3-4f1cc1b8ea2a","done":1451626549495}
{"desc":"#0 #local_hub (2:00?) (1:47)\nopen a file for editing\n\ngiven a filename and project_id, function that opens that file for sync editing\n\n- [x] create a new table in the schema that defines the state of the local_hub, e.g., which files are open, by whom, etc.\n- [x] opening a file would also update that state thing\n- [x] this would cause the local hub to open the file for editing in memory\n- [x] set project_id and path when opening a file first time\n\nIt's very unclear what the paramters/semantics/behavior of this should all be:\n- what if file changes on disk and not open in local_hub?","position":-1.3134765625,"last_edited":1451443001013,"task_id":"73b8dda0-e1c0-425e-928f-cc9ed91b8175","done":1451443000588}
{"desc":"#9 #v2 (5:00?)\ntiered storage\n\n> Even text editing/history could be tiered though eventually, where all the data about editing a file gets moved from rethinkdb to GCS after a certain amount of time, then moved back when the user re-requests info. There would be a 3s pause (say) while this happens, but I think it could be made otherwise pretty transparent. So we go from lots of non-compressed patches/snapshots of files in the database to dumping them all to a single compressed JSON archive, uploading to GCS, etc.","position":14,"last_edited":1456356460298,"task_id":"6bbb023d-077f-4359-b6df-e398e5fba747"}
{"desc":"#v2 #9 #cursors (1:15?)\nmake a way for schema to ensure something about structure of cursors table?\n\nimagine invalid data goes in which makes the cursor parsing code get messed up...","position":15,"last_edited":1451605145528,"task_id":"e2568b9c-1481-47a1-b425-c58d274aeb05"}
{"desc":"#v2 #9 #compression (3:00?)\nre-investigate compression ideas:\n\nI realized something about compression when I was \"mulling things over\" during the break.   If patches/snapshots can work either compressed or not compressed, then we could have a backend process -- running on some dedicated machine -- that just goes along and compresses anything not compressed.  Since compression takes 10x longer than decompression, this might still be well worth doing.  Decompression in browser is still very fast.  \n\nAlso: are the patches stored as JSON strings or as a complicated javascript object?  This is relevant to compression questions...","position":14.5,"last_edited":1456357019172,"task_id":"4d8dedd9-d3af-4efb-a2ee-463197e10d3d","done":1456357018761}
{"desc":"#v2 #3 #cursor (1:00?)\ncursor fade out still doesn't work well... :-(","position":-1.313232421875,"last_edited":1457733499654,"task_id":"f46c7ca2-d096-4fc2-9ed4-e1af76ba354d","done":1457733499246}
{"desc":"#v1 #0 #error (1:30?) (1:03+) #now\nsyncdoc.coffee doesn't handle errors opening a file sufficiently well.\n\nIf you try to open a file from a (1) project you can't use or (2) path you can't access...something needs to indicated this quickly in an error message.\n\nI think somehow directories and things we shouldn't watch are getting watched.  \nDirectories add files.  It's bad.\n\nActually, i think with the changes I just made, it is good enough.","position":-1.3134439250698051,"last_edited":1455494579408,"task_id":"1eecd2b8-3794-4230-8b69-7e638aef14c3","done":1455494578999}
{"desc":"#v2 #2 (0:45?) \nget rid of SynchronizedDocument2 and SynchronziedDocument","position":18,"last_edited":1451605428225,"task_id":"07c5df4c-319d-4dbf-93e5-9951bab183fc"}
{"desc":"#0 #v1  #sagews  (3:00?)\nsage worksheets ! tricky!\n\n- [x] (0:15?) (0:06) initial rendering\n- [x] (2:00?) (4:25) code evaluation:  without using messages? -- is it possible? \nYES!  It takes about 100ms without even trying to write/read/write/read from database back and forth from my house to google.  So a reactive approach to evaluation of Sage worksheet code is viable.  How shall we structure it? [[WOW, that was hard.]]\n- [x] (1:00?) (1:15) actual code evaluation in worksheets\n- [x] (0:30?) (0:02) interacts\n- [x] (0:30?) (2:42) introspection\n- [x] (0:30?) (0:16) making blobs permanent\n- [x] (0:45?) (1:11) evaluation continuation when frontend stops working\n- [x] (1:00?) (0:26) ability to interrupt and restart\n- [ ] (0:15?) whenever session starts, have the backend stop anything listed as running.\n- [ ] BONUS: provide precise information to ALL about exactly what is running now and why evaluation of something new isn't happening.","position":-1.313443925086176,"last_edited":1451865794999,"task_id":"fbe46aac-7445-4051-b9a8-f45affed3b62","done":1451865794581}
{"desc":"#v1 #1 #now (1:30?) (0:11)\nrendering/sync of side chat is a mess; seems to get all corrupted on re-connect, etc.\n\n- maybe just redo using react\n\n- doesn't seem any worse than before...; may have been really messed up due to patch sort order issue with dates, which I fixed.","position":-1.3134440183639526,"last_edited":1451696992481,"task_id":"91802ede-525b-4306-8e39-d01a010237ec","done":1451696992073}
{"desc":"#v1 #0 (1:00?) (2:20) #now\ninitial version of document \n\n- [x] (0:30?) (1:05) use version on disk: if file exists on disk and last mod time is > last database patch, set value to what is on disk.\n\n- [x] (0:30?) (1:14) use template -- if when local hub opens the syncstring it finds that the file doesn't exist on disk *and* the syncstring is empty.","position":-1.3131103515625,"last_edited":1451621367205,"task_id":"1464eaa9-19c8-4859-8ba8-c86fec15866c","done":1451621366796}
{"desc":"#d4 #v1 #2 (3:00?)\ndo the release and make this new thing live.\n\n- [ ] update_schema -- will change table durability, add new tables, etc.","position":16.96875,"last_edited":1458162345212,"task_id":"7b02f4f8-e5d9-4833-815a-3fa6af53de12"}
{"desc":"#1 (0:45?) (1:18) #bug #now\nensure latest file changes on disk are reflected in memory\n\nbecause of debouncing or whatever, the file on disk can be slightly newer than what is in database via sync.  E.g., open local_hub.log...","position":-1.31304931640625,"last_edited":1451695830762,"task_id":"68aeba43-363a-4d49-8b54-28f2e763e082","done":1451695830352}
{"desc":"#0 #v1 #now (1:00?) (1:45)\nopening a file that already exists on disk for the first time\n\n- [x] should not show file as blank for a second -- instead show \"loading...\"\n\n- [x] ensure that the undo history doesn't start with \"empty document\", but with what got loaded\n\n- [x] opening that file for the first time is also SLOW.","position":-1.3134439587593079,"last_edited":1451713098749,"task_id":"9a52c6cf-4d53-4d26-bfe5-6911a38d96d9","done":1451713098341}
{"desc":"#0 #v1 #now\nfirst tim","position":-1.3130950927734375,"last_edited":1451616454575,"task_id":"386de53e-c466-4294-bdc8-a67432e7a259","deleted":true}
{"desc":"#v1 #0 (0:45?) (0:29) #now\nmaintain cursor position when opening/closing editor\n\n- could just rewrite them in react... :-(","position":-1.3131027221679688,"last_edited":1451628825683,"task_id":"d31962d9-e205-4b7e-ac33-0a9bbd8a6e01","done":1451628825270}
{"desc":"#0 #v1 #latex (1:30?) (0:17) \nlatex editor isn't doing the initial build for some reason\n\n- could also changes to execute commands via db state, rather than messages, exactly like will have to do for sage worksheets.  (save this for the react rewrite)","position":-1.3134439250725336,"last_edited":1451922569300,"task_id":"2e3c1e19-9957-4781-844a-fc26a45ab3cc","done":1451922568880}
{"desc":"#2 (1:30?)\npotential jupyter issues\n\n- [ ] make sure save/load works properly; don't have weird issues with files/timestamps, saving.","position":-1.313443660736084,"last_edited":1451626337567,"task_id":"887c92d3-7dd2-47ea-bb35-74d42b5e781b"}
{"desc":"#v1 #0 (1:30?) #now\nSynchronizedString improvements\n\n- [ ] (0:45?) (0:20+) #now make the save ui button properly reflect state of save and hash for tasks (and anything based on SynchronizedString) -- I'm goint to wait for the react rewrite to do this...\n\ndocs already done completely and nicely.\n\nthe test for this is edit a task, save, make a change, undo it, and see the save grey out.\n\n- [x] (0:45?) (0:12) like for SynchronizedDocument, ensure for a while that SynchronizedString save definitely saves before returning -- retry until success\n","position":-1.3134439253481105,"last_edited":1451766449672,"task_id":"c818fa66-1681-48af-a24e-5c6642424b2e","done":1451766449262}
{"desc":"#0 #v1 (0:30?) (0:12) #now\non first time to ever open file, the backend should set the save hash\n\nTest -- open new file and see the save button stay disabled.","position":-1.3134439382702112,"last_edited":1451755542608,"task_id":"e6419294-cb49-4db6-a629-e7ebafbc9ce0","done":1451755542199}
{"desc":"#0 #v1 (1:00?) #now (1:35)\nproperly handle read only files","position":-1.3134439326822758,"last_edited":1451761126692,"task_id":"1f19faf7-ac51-429c-82f5-9309d2b90dd3","done":1451761126282}
{"desc":"#0 #v1 (0:45?) (0:11)\nsagews history\n\nwow, it just works perfectly already, way *better* than live version!","position":-1.3134439256973565,"last_edited":1451763845092,"task_id":"5acba352-b1ae-4a9b-8ced-0788ec55f8da","done":1451763844676}
{"desc":"#1 #v1 #jupyter  (4:00?)\nrewrite jupyter sync to work with new sync, and be generally better.\n\n- [x] (0:30) get it to sort of work\n- [x] (2:00?) (1:45) rewrite jupyter to work using new syncstrings.\n- [x] (1:30?) (1:00+) history\n- [x] (1:00?) (1:10) cursors\n- [x] (0:45?) address this todo: \"# TODO: If this cell was focused and our cursors were in this cell, we put them back:\"\n\n- [x] loading jupyter in the iframe sometimes hangs browser now or gets in crazy feedback loop.  BAD.","position":-1.3134439250711694,"last_edited":1457662051202,"task_id":"d04c4061-20e6-4e05-852e-ab8c5872d181","done":1457662050794}
{"desc":"#2 #v2 (1:30?)\ntasks and courses history slider","position":-1.3126907348632812,"last_edited":1456462598491,"task_id":"c7e7e008-999c-4fcf-bae3-6a8f106f3645","deleted":true}
{"desc":"#2 #v2 (0:30?) #react\nuse timeago also for the main time description, and actual time in parens","position":-1.3127059936523438,"last_edited":1451672059923,"task_id":"9a6b3b07-2815-4ec5-a49b-88fc132783a3"}
{"desc":"#0 #v1 (0:45?) (0:45)\noptimize history\n\n- [x] (0:45) debounce slider\n- [x] make sure it is fast enough to go from one point to another","position":-1.3127021789550781,"last_edited":1451680760685,"task_id":"97975f59-cd9b-4b45-951e-8976273c96a9","done":1451680760275}
{"desc":"#2 #v2 (2:00?)\nway to link history to file somehow... (maybe import/export...?)","position":-1.3127002716064453,"last_edited":1451679319945,"task_id":"75904b9a-2330-48ef-875b-96be4ba6a99d"}
{"desc":"#2 #v2 (0:45?)\nadd blue button linking back from history to original file","position":-1.3127012252807617,"last_edited":1456462582463,"task_id":"b312e4a1-9f84-4425-ae66-82fe669b3471","done":1456462582053}
{"desc":"#1 (1:00?) #bug\nwhen testing/developing locally on my laptop and the server time gets all messed up,\nedits get discarded.\n\n- maybe they appear to be slightly in the future?","position":-1.313018798828125,"last_edited":1451682203615,"task_id":"985c9e35-99af-4a94-9fb6-21d31ad80159"}
{"desc":"#2 #v1 (1:00?)\ncan we do anything sensible when the underlying file being edited is moved...?","position":9.875,"last_edited":1451713483464,"task_id":"1476e868-7e9b-43a3-866b-6d8adbc06c7d","deleted":true}
{"desc":"#1 #asap (0:30?)\nrebuild rethinkdb from source for compute nodes -- the 2.2.2 release is SHIT.\n","position":-1.3134439438581467,"last_edited":1453656610043,"task_id":"e22caa17-38e7-41f4-9ae9-1eb854903956","done":1453656609633}
{"desc":"#d3 #1 (1:30?) #v1\nany time load the file from disk, also set the \"hash of saved version\" to that.\n\nMaybe change \"saved version\" to \"disk version\" in function naming.\n\nEffect: newly opened file won't have save ready to go.  Also, when git checking out, etc., will do right thing.\n\nI'm unsure why/if this is needed.","position":-1.3134439562920723,"last_edited":1458240918620,"task_id":"9b5d9fa2-41de-4756-9f12-58db04de3c9c","done":1458240918205}
{"desc":"","position":-1.3134437911212444,"last_edited":1451763281235,"task_id":"424f4d76-4230-4f20-9871-7b69952f30d4","deleted":true}
{"desc":"#v2 #3 (1:00?) (0:20+)  #react\nmake the save ui button properly reflect state of save and hash \n\n- for tasks (and anything based on SynchronizedString) -- I'm goint to wait for the react rewrite to do this...","position":-1.3134439251734875,"last_edited":1451766441743,"task_id":"14d46970-33df-471d-915f-01c8e3a80b33"}
{"desc":"#2 (0:30?) #v2 #history\nwhen thing changing the file is project -- rather than a user, put that in history properly.","position":-1.313443925075262,"last_edited":1451866014564,"task_id":"ae7e018c-c50f-4a2f-9551-406ccd0864f3"}
{"desc":"(0:05) #0 #bug #history #v1 (0:30?)\nclosing the history does *NOT* actually work -- still get slider update messages, etc.\n\nfalse alarm; I can't replicate this.","position":-1.313443925080719,"last_edited":1453414407382,"task_id":"a6b39352-b97f-41b0-9acc-2f3b20d25bea","done":1453414406967}
{"desc":"#2 #question (0:20?)\nshould \"strip trailing whitespace\" be a menu option, rather than something that happens on save?  It's possibly very evil...","position":-1.3134439250834475,"last_edited":1451787557384,"task_id":"fdb7891a-3c11-49f4-a891-002d9cbf3b24"}
{"desc":"#1 #v1 #bug #history (0:20?)\nmaybe the history is missing the very last step (?)","position":-1.3134439250848118,"last_edited":1451865954583,"task_id":"7ae24149-6cb0-4a65-b6b0-7bf36b33158a","done":1451865954174}
{"desc":"#1 #v1 #bug\nsynchronized editing of sage worksheets leads to all kinds of corruption.\n\nmaybe soemthing  wrong with setvaluenojump...?\n\nalso had corruption with history, so temporarily disabled setvaluenojump","position":-1.313443925085494,"last_edited":1451806476270,"task_id":"84ecfe4e-55cf-4dec-ac0f-0e217dc454cf","deleted":true}
{"desc":"#v1 (1:00?)\nmake db set/get queries explicit and make synctable use them\n\nit's just too confusing otherwise.","position":-1.313443925085835,"last_edited":1451865926801,"task_id":"b79944b3-2611-4c5a-a2c1-354a075f0495","done":1451865926393}
{"desc":"#0 #v1 (0:15?) (0:04) #history\ndon't show name if not known","position":-1.3134439250704872,"last_edited":1451922828611,"task_id":"0123f2e1-77fb-4c3d-907f-bcf05834c8f3","done":1451922828202}
{"desc":"#0 #v1 (0:30?) (0:02) #now\nremove \"revision tracking\" option from account settings; it is no longer optional.","position":-1.3134439250718515,"last_edited":1452024748166,"task_id":"f29c5697-3c07-4095-8dd1-511d5e3514d7","done":1452024747756}
{"desc":"#d1 #0 #v1 (1:30?) (0:35) #now\nmake certain tables use soft durability\n\n- cursors\n- eval stuff\n\nDaniel explains exactly what to do here: https://mail.google.com/mail/u/1/?zx=v6crnjw8ww15#inbox/152fc888a7429712\n\n> Like Postgres, RethinkDB defaults will not acknowledge writes until they’re fsynced to disk. Users may obtain better performance at the cost of crash safety by relaxing the table’s or request’s durability setting from hard to soft. As with all databases, the filesystem, operating system, device drivers, and hardware must cooperate for fsync to provide crash safety. In this analysis, we use hard durability and do not explore crash safety.\n\nhttps://aphyr.com/posts/329-jepsen-rethinkdb-2-1-5\n\n(Also, maybe I should make the reads even more rigorous for project storage related stuff, e.g., project state, host, etc.)","position":16.953125,"last_edited":1458162758213,"task_id":"40652cb5-b54f-40ec-817b-c993f444b1e6","done":1458162757803}
{"desc":"#d4 #0 #v1 (1:30?)\noutput computation limits; need to limit how many output messages can be written to the database per unit time!","position":-1.313443925071084,"last_edited":1458310535543,"task_id":"702135c8-f220-4d89-bfdd-7c4aabf27c21"}
{"desc":"#d4 #1 #v2 #sagews (1:30?) (1:25+)\nre-implement raw_input  (and another related thing).\n\nhttps://github.com/sagemathinc/smc/issues/358\n\nAlso: https://github.com/sagemathinc/smc/issues/78 -- input not implemented\n\n- I realize now that the way to do this is to add sage_session.coffee's call the relevant code, then use the evaluator table, but write a different event. \n\nTODO: still -- maybe due to delete_last_output not working right now.\n\n- [ ] additional output in the *same* cell (did this ever work)?\n- [ ] normal input\n","position":-1.3134439550340176,"last_edited":1458310544827,"task_id":"7a2628f4-0bce-406a-9849-7359c9d4f7c9"}
{"desc":"#now #0 #v1 (0:20?) (0:07) #sagews\ndoesn't cause a sync when toggling input/output of a cell.","position":-1.3134439250452488,"last_edited":1453415500043,"task_id":"fbfef191-3c76-4d9f-816e-2048e4002527","done":1453415499634}
{"desc":"#0 #now (1:00?) (0:35) #v1\nrefactor syncstring.coffee code some","position":-1.313443925046613,"last_edited":1453420481870,"task_id":"59c523de-1924-43c2-947b-cde85aa3439c","done":1453420481463}
{"desc":"#d3 #0 (2:00?) (6:16) #v1 #now\npatches are not determined only by time, but by time and user.  \n\nHowever, it seems that in the add: method in patch_list, it's assumed they are determined by time? check on this.\n\n- [x] **best idea**: fail and ask forgiveness any time saving a patch, later check that it succeeded, and if not perturb the time and try again.  If this fails repeatedly, need to do some offline thing.  This seems like a good idea anyways.    EVEN BETTER: when getting new updates, if get one that will overwrite an existing one... call a hook.  \n\n- [x] simulate this so it will be easy to test\n- [x] make it so locally won't save a patch over one with the same timestamp\n- [x] conflict\n- [x] maybe(?) change patches to have key just [string_id, time].\n\nStep for now: get rid of any code that assumes time uniqueness?  How hard?\n\n---\n\n- Maybe terrible idea: make it so the timestamp of any patch that a user commits is got by rounding down to the nearest 1/10th of a second, then adding the user index.   Then if there are up to 100 simultaneous users who edit the doc, we are always guaranteed that the timestamp determines the user.  Changing the timestamp by 1/10th of a second shouldn't impact the behavior in a noticeable way.\n\n- [ ] in versions and all_versions and PatchList versions, we would get duplicates timestamps if there are two patches with the same timestamp and different users.\n\n","position":-1.3134439250472951,"last_edited":1458340794320,"task_id":"24f0fae2-dfca-4842-b606-9c7abdf003ec"}
{"desc":"#1 (0:45?) #v2\noptimization -- when making a snapshot, we could throw away all in-memory edit history before the snapshot.  \n\nNot sure.","position":-1.313443925046954,"last_edited":1453482707765,"task_id":"8c54b8ec-d4ab-47b6-9dc4-f9da05af8346"}
{"desc":"#v2\nability to load some but not all history\n\nBasically a function like load_full_history, but which goes back just one snapshot in time.\nMaybe expand_history().\n\nAlso, function to tell whether there is any more history available.","position":-1.3134439250476362,"last_edited":1453470299577,"task_id":"defe03b0-f5b7-493a-b62d-a330931b7264"}
{"desc":"(1:00?) #v2\nrobustness idea\n\n- [ ] robustness: if load using last_snapshot, and for some reason no snapshots in the list of patches we got, then load everything starting from the beginning.\n\nI don't know if this is ever needed, but it couldn't hurt to implement...","position":-1.3134439250478067,"last_edited":1453482331241,"task_id":"517db47b-fa4c-49d7-950a-524199f436b2"}
{"desc":"#v2 (2:00?) \n**consider** compressing snapshots (and/or patches) via a manager.\n\nI did this, implemented it, and it adds subtle bugs, issues, problems, for possibly little gain.  It's better to compress the filesystem / network. \n\nREJECTED.","position":-1.313443925047892,"last_edited":1456356947997,"task_id":"26b69eb7-eb8c-4c45-9c1d-790572098e7c","done":1456356947582}
{"desc":"#v1 #0 #now (0:30?)\nfix some bugs/fallout from new snapshot code.","position":-1.3134438580818824,"last_edited":1453488364190,"task_id":"48f7cb23-834e-458a-832d-869fca4b5960","done":1453488363781}
{"desc":"#1 #v1 #jupyter (2:00?)\nsync jupyter is still not working right\n\n- [ ] output now keeps getting removed, possible due to some lineheight css hack race condition (not sure).   Anyway, very broken.\n\n- [x] still get weird infinite loop sometimes when opening jupyter history.","position":-1.3134439250711054,"last_edited":1457662083136,"task_id":"32417b5a-2647-4d5e-986e-da077eb40366","done":1457662082726}
{"desc":"#v2\nif possible, history should support \n\n- [x] task list\n- [ ] course editor\n- [ ] chatroom -- which would make allowing users to edit all past chats of theirs more reasonable...","position":-1.313443925071148,"last_edited":1456462531364,"task_id":"515b45f3-56bd-473a-a327-3e8b525547d3"}
{"desc":"#v1 #1 #jupyter #now (2:00?) (0:45+) \njupyter blobs!\n\n\n- [ ] rewrite get() to iterate through cells\n- [ ] cash result for a given cell...\n\nRules:\n\n- If it is the first time to open the document (as defined by syncstring being new), save all blobs to backend.\n\n- When converting from dom to string, save any blobs not seen before.\n\n- When converting from string to dom: \n  - swap in any known blobs\n  - attempt to async load any not known blobs, and if succeeds try to update that cell... if it doesn't change.\n\n","position":-1.3134439562927582,"last_edited":1457733442525,"task_id":"04488a42-1b67-4baa-a152-601ce3428a9b","done":1457733442111}
{"desc":"#1 #v1 #jupyter (0:10?)\nhistory -- open file needs to have a special case for jupyter notebooks.","position":-1.3134439250711534,"last_edited":1457662077633,"task_id":"4b7173b3-c86f-44e1-ab95-ba691a073444","done":1457662077226}
{"desc":"#1 #v1 #jupyter (0:30?)\ncommand+s (or control+s) doesn't work to do our save...\n\ncareful of firefox!","position":-1.313443925071156,"last_edited":1457733491179,"task_id":"c1364ce8-33aa-4da9-9a0a-0c0f623fc0ac","deleted":true}
{"desc":"#v1 #jupyter  (1:30+)\nwhen opening file `Checking whether ipynb file has changed...` don't look at the file on disk, but instead at the last change in memory (from db)\n\nActually, need to do something more interesting.\n\nIdea: Have the backend, when opening a .ipynb file, watch the .ipynb file, but immediately convert it to our sync format.   If the ipynb file changes, it re-reads it and converts to a syncstring.  Save will use this, which avoids any weird race conditions, etc.\n\nBackend conversion to syncstring will take care of removing and saving all the blobs to hub.  (So need a generic client.save_blob() on both front and backend.)\n\nThis is a basic step in going from having the on-disk format and the sync format be different when editing a document.  This might also be useful for latex...\n\n- [ ] factor out the jupyter <--> syncstring code so that it can run in both client and project (and be easier to test).\n- [ ] make project \"aware\" of ipynb files","position":-1.3134439250711574,"last_edited":1457662072015,"task_id":"a4813b45-e6aa-487b-8403-7aace5a8eb0c","done":1457662071596}
{"desc":"#0 (1:00?) (1:35) #now\nhistory for task lists\n\n- [x] (0:10?) #now add a history button\n- [x] (0:10?) try opening one and see what happens.\n- [x] (0:25?) add case to create editor\n- [x] (0:25?) add code to set value.","position":-1.3134439250711374,"last_edited":1453662665611,"task_id":"1723fe76-e5d6-4d93-8dce-43c5098146a1","done":1453662665196}
{"desc":"#1 #v2 (0:45?)\nmake the hashtags in task list history work","position":-1.313443925071132,"last_edited":1453662663238,"task_id":"2dc813b7-0d1b-49f4-8e4a-1b01c3bcc371"}
{"desc":"#1 #v1 #sagews (0:40?)\ngreen \"running\" bar flickers like crazy whenever running anything.  \n\nNo idea why.\n\nhsy: that's fixed, the new loading banner caused a css animation name collission","position":-1.3134439250717875,"last_edited":1453731735687,"task_id":"295ec2f3-46ed-4e19-9514-d05c9b8753f6","done":1453731735273}
{"desc":"#v2 (0:30+) I tried opening a file in a path that involves a symlink using the “open” terminal command.\n\n(Namely the file sagews.coffee in the dev project).\n\nThings went to hell – the file wouldn’t open. Opening the same file from the UI works.\n\n> this is actually a general problem having nothing to do with new sync. It’s a subtle problem of using SMC inside SMC and how path’s work. Delay until later. Orthogonal to sync.","position":-1.3134439250717982,"last_edited":1454588983632,"task_id":"01a00452-41d8-4869-8059-54a972fcf131"}
{"desc":"#v2 (0:45?)\neliminte use of smc-project/file_session_manager.coffee \nand also all the codemirror messages from message.coffee","position":-1.3134439250718035,"last_edited":1456462297185,"task_id":"23d86235-0888-49c3-9977-795386f5a19a","done":1456462296774}
{"desc":"#2\ninsane processes going crazy in project dev, at least with *new* refactor -- they are console servers\n\n     7580 5811740+  20   0 1490000 641884  15884 R  78.0  2.4   0:57.39 /usr/bin/nodejs /usr/lib/node_modules/forever/bin/monitor /projects/45f4aab5-7698-4ac8-9f63-9fd307401ad7/smc/src/smc-project/console_server.coffee               \n    26349 5811740+  20   0 5832692 4.754g  15908 R  68.7 18.6  11:51.67 /usr/bin/nodejs /usr/lib/node_modules/forever/bin/monitor /projects/45f4aab5-7698-4ac8-9f63-9fd307401ad7/smc/src/smc-project/console_server.coffee               \n","position":-1.3134439250718062,"last_edited":1453702764357,"task_id":"6527b75f-71c0-420a-9b1b-ca0dc248fe3a"}
{"desc":"#0 #v1 #sagews (0:45?) (2:12)\nre-implement delete_last_output\n\nThis is maybe needed for raw_input.\n\nIt's actually missing/not implemented even in live SMC.\n\nEnded up taking a long time and cleaning up code, and going down the rabit hole and learning more... ","position":-1.3134439531713724,"last_edited":1453739389968,"task_id":"1e97b64a-ddf1-4cfa-8227-3173ed56c631","done":1453739389549}
{"desc":"#2 #v1 #sagews (0:45?)\nDeal with this in `syncstring_evaluator.coffee`\n\n\t# TODO: need to perturb time if same as last eval","position":-1.3134439522400498,"last_edited":1456640761974,"task_id":"5da6f2aa-488a-4975-95a8-f3be78b54604","done":1456640761564}
{"desc":"#0 #v1 (0:30?)\nsyncstring save is annoying!  it often doesn't save until requesting saving again.\n\na very long time to delete four lines...","position":-1.3134439568966627,"last_edited":1454648882453,"task_id":"5082da21-f539-4c52-ae30-47dc71487b51","done":1454648882041}
{"desc":"#v2\ntasks save should be green only if there are definitely unsaved changes.\n\n(this could be part of react tasks rewrite)","position":-1.3134439559653401,"last_edited":1454638138214,"task_id":"4c0196bc-0da6-4fa4-99d7-05831516215a"}
{"desc":"#v1 #0 (1:00?) (2:30) #now\nmake it so editing a file triggers file_use notifications\n\nOne way: do on the backend\n\n- [ ] remove the scan_local_hub_message_for_activity function (and anything only for it) from hub\n- [ ] change schema so writing to the patches table looks up (and caches) which file this is for, and -- if not too recent -- updates file use table.\n\nOther way: do entirely on the client\n\n- [ ] add an action in the file_use Actions to record activity; make it \"debounce\" non-chat activity by 1 min; less debounce for chat\n- [ ] have syncstring call it when saving patches.\n- [ ] delete all backend code related to this\n\nPros and cons: \n - backend is harder to update and develop\n - backend will work no matter what bad happens with frontend.\n - user cheating -- this isn't a video game.\n - doing it on client means clients can easily implement a \"stealth\" mode if they want.  That's good.\n \nSubtle issue:\n\n - the idle timeout functionality needs to be aware of file editing.  Right now there are *NO* messages to local hub, so project just doesn't get touched.","position":-1.3134439564310014,"last_edited":1454913008198,"task_id":"f43d9080-4285-486a-b388-ef43e1741837","done":1454913007759}
{"desc":"#0 #v1 #file_use #now (0:30) (1:15?)\n\n- [x] there is something wrong with the wstein@sagemath.com project in that it doesn't show all recent notifications initially on load.  Why?  Something wrong with sort or...?\n\nE.g., the file xxxxx.","position":-1.3134439564091736,"last_edited":1455229641634,"task_id":"457ef005-9d81-4a49-8374-347fe609c522","done":1455229641220}
{"desc":"#1 #v1 #file_use #now (0:24) (0:45?)\n\n- [x] fix so that just marking the file_use notifications read, which happens on clicking the bell, no longer results in the file presence showing the face as if the recipient actually looked at the chat/file.","position":-1.3134439562563784,"last_edited":1455472916613,"task_id":"a99993fc-b716-4986-b779-7f12ca6aaf0e","done":1455472916205}
{"desc":"#1 #jupyter #file_use #v1\nnot implemented at all (that weird transform)\n\nif we just move the processing to the backend, it won't matter.","position":-1.313443956314586,"last_edited":1457045345343,"task_id":"d1ef3607-24f9-4561-85ef-c6a6d3d63d8d","deleted":true}
{"desc":"#2 #file_use #v2\ndelete the backend hub code related to differential sync.","position":-1.3134439563727938,"last_edited":1456462121611,"task_id":"8aefbb99-5cd4-47af-8f4d-e96739c4eeb6","done":1456462121204}
{"desc":"#0 #v1 #now (1:00?) (1:30)\nidle timeout of project when user editing\n\nWe need to somehow touch project.  Current functionality in hub doesn't work since local hub no longer does the file editing org directory.\n\nSo... the idle timeout functionality needs to be aware of file editing.  Right now there are *NO* editing messages to local hub, so project just doesn't get touched.\n\nSimilarly, need to automatically start up project whenever user does something with a file that... requires project to be running.  ","position":-1.3134439564018976,"last_edited":1455240521730,"task_id":"4e0cc188-c13f-490d-a7e9-92d26c277836","done":1455240521319}
{"desc":"#0 (1:00?) (0:38) #v1 #now\nmake it so **opening/foregrounding** a file explicitly (not by a reconnect, etc.) triggers file_use notification properly","position":-1.3134439564164495,"last_edited":1455225966372,"task_id":"ba1caf33-b8b1-413a-8fb7-ba77407d4927","done":1455225965962}
{"desc":"#0 #now(2:00?) (1:25)\nchange to storing patches in db as strings; include offline compression \n\n- [ ] (0:45?) #now change all code so patches are strings rather than lists\n- [ ] (0:15?) in schema have way to indicate if compressed or not (with index)\n- [ ] (0:30?) write function that goes through and compresses (and decompresses) using lz-string (http://pieroxy.net/blog/pages/lz-string/guide.html)\n\n","position":-1.3134439250711576,"last_edited":1455169083199,"task_id":"1030175a-56a5-46d8-9a91-1185d105935f","done":1455169082785}
{"desc":"#v1 #0 #sync #speed #now (0:07)\n- [ ] (0:30?) the code that adds patches to list and JSON'ifies ends up going back and forth every time, which seems wasteful.","position":-1.3134439250711583,"last_edited":1455499673348,"task_id":"7e84b008-6102-4af1-89b7-20594eeb2441","done":1455499672939}
{"desc":"#0 #v1 (0:45?) (0:45)\nwhen compressing patches, compress both the patch *and* the snapshot.\n\nAlso, when decompressing, decompress both.","position":-1.3134439250711585,"last_edited":1455220502858,"task_id":"aa0f7413-02ad-4ad6-801f-326dd1ac4654","done":1455220502449}
{"desc":"#0 (0:30?) (0:21) \nmake it only *try* to query db after it has signed in (rather than before!); this resolves a bunch of issues.","position":-1.313443925071158,"last_edited":1455222350977,"task_id":"cb4fe164-19bc-4fed-93b6-f5e7bcb934b3","done":1455222350568}
{"desc":"#3 #v2\n- synctable query errors: retry a few times with exponential backoff","position":-1.3134439250711578,"last_edited":1455222338414,"task_id":"f91c0780-2067-405f-9ed3-1b19c51070d8"}
{"desc":"#0 (0:20?)\ncursors -- don't use underscore.debounce -- instead use underscore.throttle.","position":-1.3134439563873457,"last_edited":1455232158929,"task_id":"8356f20d-e99a-4ba3-a441-20f5dc6b4415","deleted":true}
{"desc":"#0 #v1 #sage\nadd more hooks to start project\n\n- [x] (0:42) making touching a file in a project start that project running (so can check if file on disk changed, or open it in case it hasn't been opened yet, etc.)\n\n- [ ] start sage server running on sage worksheet evaluation.\n\nThis is similar to the hook on synctable save change, which causes the project to start, and get a connection.","position":-1.3134439563946216,"last_edited":1455245942996,"task_id":"171ae0fe-bc79-4e02-a5aa-fe8fcae13f59","done":1455245942580}
{"desc":"#0 #v1 (1:13)\nIt's time -- come up with a way to **guarantee** that the synctables all stay in sync with the database.\n\n- [ ] do not ignore any errors.\n\nactually didn't do this -- but found issues with sagews and fixed those instead.","position":-1.3134439563909837,"last_edited":1455258251966,"task_id":"9d7b8556-42d6-4c32-815c-955c62d34a26","done":1455258251555}
{"desc":"#1 #v1 (0:45?) (0:55) #now\nwhen opening a document, if there is an error, then it sets codemirror to that error\nand makes it read-write. Pretty idiotic.","position":-1.3134439561399631,"last_edited":1455430068998,"task_id":"8cccbcde-440f-43a1-abc1-29f6a5203c72","done":1455430068589}
{"desc":"(0:10?) (0:10) #1 #v1 #now\ncomment out code related to compression -- fact is, it is more trouble than it is worth, probably buggy, doesn't seem worth it, and uses too much client cpu.  I have a bad feeling.","position":-1.313443956169067,"last_edited":1455306231193,"task_id":"e98853db-b731-415e-9e1b-5ce4f6183e8f","done":1455306230783}
{"desc":"#0 (0:05)\nmerge with master","position":-1.3134439561836189,"last_edited":1455425606418,"task_id":"4ba15e3e-41e9-404f-9fbb-ecd57a1002b2","done":1455425606013}
{"desc":"#v1 #0 #now (2:45)\nsyncstring saving breaks sometimes\n\nFiguring out why and fixing this is obviously critical!\n\nSo far I figured out that the changefeed for the syncstrings table is discarded for some reason.  Thus the browser stops getting updates, including that the save worked.\n\nI had an example of this, and checked with `smc.client.query_get_changefeed_ids({cb:function(e,cf) {window.cf =cf}})` then reconnected with `s._syncstring_table._connected = false; s._syncstring_table._reconnect()` and it fixed the problem.    There is an explicit cancelling of the changefeed in the log.\n\nI think what might happen is the connection closes and re-opens very quickly; the changefeed id is still listed so it doesn't get reset.  However, it then gets closed moments later.  Yes, that's *EXACTLY* it -- in fact, clicking reconnect breaks everything completely.  Idea for a fix -- more quicly remove all the changefeed id's.\n\nANSWER: fixed via getting rid of destroy timer and reconnecting properly.","position":-1.3134439562272746,"last_edited":1455469254946,"task_id":"699cda28-8543-4a38-848d-e303418fedcd","done":1455469254536}
{"desc":"#0 #v1 (0:30?) (0:45)\nfix this: `FileUseActions.mark_file error:  don't know server yet`\nin the log a lot","position":-1.3134439250588912,"last_edited":1455497694225,"task_id":"bfc49359-f31d-48d7-83e4-443d661372db","done":1455497693813}
{"desc":"#0 #v1 (1:30?) #crazy #wontfix\nensure that the client and hub (both project and browser) know with certainty exactly what syncstrings are being managed by the hub.  \n\nNo matter how hard I try to make this event based, it's just not working well. \n\nHow about this:\n\nAny time the list of changefeed id's being managed changes send a message to the client saying \"here's how they changed\", and be satisfied with a response that is a hash of the result proving that they applied the update.  If not, send a different message saying \"Here's the whole thing\", reset to this.\n\nProblems -- the above could go very wrong in case of lots of changefeeds connecting, disconnecting, etc.\n\nOther ideas.\n","position":-1.3134439250534342,"last_edited":1455497707756,"task_id":"dd286797-150d-424d-92d2-ef42ca6fd4a1","done":1455497707349}
{"desc":"#0 (0:45?) #v1 #now (2:30)\nWhy does jupyter spin out of control???\n\nDue to not waiting for the kernel to be fully connected.  Doing that fixes things nicely.","position":-1.3134439250471246,"last_edited":1456356342276,"task_id":"d9b7439f-7733-4386-a6d4-e82347c81a99","deleted":false,"done":1456356341819}
{"desc":"#jupyter #v1\nif one user changes the kernel, have to make everybody else update their kernel setting\n\neither:\n\n- [ ] refresh iframe (easy but ugly)\n- [ ] somehow run some javascript on the page (harder)","position":-1.3134439562854823,"last_edited":1457733704815,"task_id":"bcfa25ec-f436-4a37-a38c-175025babc38","done":1457733704401}
{"desc":"#1 #jupyter #v1 (2:00?)\nsimulate cell that won't JSON and make sure this problem gets treated sensibly.\n\nRight now it definitely can go into a horrible infinite loop, breaking all clients, eating the worksheet, etc.!   This one experienced that https://cloud.sagemath.com/45f4aab5-7698-4ac8-9f63-9fd307401ad7/port/54249/projects/6976fd09-bf0e-4c21-81c7-066214b123fd/files/2016-02-14-185842.ipynb\n\nSo this is critical to get right.","position":-1.3134439250470393,"last_edited":1457662092939,"task_id":"84b303e1-69d2-489f-bfba-5cccdae5a001","done":1457662092534}
{"desc":"#1 #v1 #jupyter  (1:00?)\ncursors suddenly start using huge amounts of cpu","position":-1.313443925071164,"last_edited":1457733487900,"task_id":"516cb219-1900-44ae-abc0-4f1ebe798d1e","deleted":true}
{"desc":"#0 #v1 (1:15?) (0:25)\nsync2 -- rewrite tasks to not use DiffSyncDoc.","position":-1.3134439250711614,"last_edited":1456362546788,"task_id":"99b1220e-fa7c-442f-b79b-8f1d161eb50e","done":1456362546377}
{"desc":"#0 (1:30?)\ndelete old sync code in syncdoc.coffee\n\n- [x] (0:30?) (0:13) delete all DiffSyncDoc code from syncdoc.coffee\n- [x] (0:30?) (0:19) delete most code from AbstractSynchronizedDoc\n- [x] (0:30?) (0:47) something similar.. to \"merge SynchronizedDocument2 extends\" SynchronizedDocument\n- [x] (0:20?) (0:33) move markers `{MARKERS, FLAGS, ACTION_FLAGS} = require('diffsync')` elsewhere.\n- [x] (0:45?) (0:27) #now move diffsync.SynchronizedDB, which is used in syncdb.coffee, to not be in diffsync and delete diffsync.coffee\n","position":-1.31344395634369,"last_edited":1456379894483,"task_id":"2301f8f6-ea37-4670-8f47-b27276b9011d","done":1456379894073}
{"desc":"#0 (0:20?) delete codemirror messages from message.coffee","position":-1.3134439563582418,"last_edited":1456461403829,"task_id":"c5046e7f-ef98-497a-ac0b-2784a71bfc09","done":1456461403417}
{"desc":"#v1 #jupter\nsync and backup files\n\nSee -- https://mail.google.com/mail/u/1/?zx=v6crnjw8ww15#inbox/1531a70af158f297\n\nMake sure this gets resolved in the rewrite:\n\n>   *)  Annoying bug report:  the Jupyter notebooks have an autosave and\n> checkpoint restore feature that seems buggy.  For example, when using git\n> students have noticed that they'll sync a notebook from GitHub only to have\n> the SMC autorestore mechanisms immediately revert the notebook after they\n> open it.  It is very difficult to prevent this behavior, but we found that\n> deleting the hidden checkpoint files sometimes works.  Any way to make git\n> play more nicely with the auto-backup would be nice.","position":16.75,"last_edited":1458142449148,"task_id":"9a69b884-7ef8-410e-9b09-accf437405d7","deleted":true}
{"desc":"#now #d1 #0 #v1 (0:45?) (0:40)\nEnsure that this gets done when files get opened: `database.log_file_access`\n\nI deleted the one place in old code that did that.","position":16.5,"last_edited":1458160212107,"task_id":"f2a1e720-c63d-414d-8f2c-15c77f01c632","done":1458160211695}
{"desc":"#0 #v1 (1:00?) (1:00)\noutput messages ordering\n\nsyncstring_evaluator.coffee -- ensure output messages are reordered when rendering them on the client.\n\nRight now they go via the database, so can, in theory, appear in any order in the worksheet output.\n\nThis exhibits the problem pretty readily:\n\n    for i in range(20):\n        print i\n        sys.stdout.flush()\n        \nSolutions:\n\n- put in number so we know order on client.\n- only display when we get.\n- display out of order then reorder as more come in (?) -- no","position":-1.3134439517743886,"last_edited":1456630159413,"task_id":"9d452a34-80e9-450e-a2de-d0a3d7d80f3e","done":1456630159002}
{"desc":"#0 #v1 (1:00?) (0:50)  #now\nmake it so `misc.seconds_ago(...)` etc. on the client are relative to the server clock, not the client clock.\n\nE.g., in syncstring_evaluator.coffee not doing this would cause serious trouble if clock wrong.\n\nDo via a hook, e.g., misc.set_server_time()...","position":-1.313443951541558,"last_edited":1456641011525,"task_id":"a1f574eb-fb4a-4476-a1a3-85823676b17c","done":1456641011115}
{"desc":"#1 #v1 (0:45?)\nmake it so evaluating a cell that takes a bit, then evaluating same cell again, cancels that cell and doesn't let multiple outputs pile up.\n\nExample cell input:\n\n    2+3\n    sleep(1)\n    3+3","position":-1.3134439514251426,"last_edited":1456638512738,"task_id":"b66dd01f-d0c9-4d2c-bcce-9cb80049f45d","done":1456638512330}
{"desc":"#0 (0:45?) (0:17) #sagews #v1 #now\nsage worksheet interrupt doesn't work at all","position":-1.3134439250711567,"last_edited":1456636718044,"task_id":"09e49171-651e-41cd-8a67-75fd811666fe","done":1456636717635}
{"desc":"#0 (0:20?) (0:09)\nchange sync to use debounce again, rather than throttle\n\ninspired by that facebook talk about how undo works in bursts.","position":-1.3134439563000342,"last_edited":1456630697277,"task_id":"67662eaa-8c0a-45b6-b81e-fe9c47b97b28","done":1456630696865}
{"desc":"#0 #sagews #now (0:30?) (0:55)\nevaluating a cell before it is done evaluating leads to getting old output\n\nDo this twice:\n\n    2+3\n    sleep(1)\n    3+3","position":-1.313443954102695,"last_edited":1456634251124,"task_id":"37c50914-ab70-4e45-bc34-e20011c2df71","done":1456634250716}
{"desc":"#0 (0:30?) (0:14) #now\nfile editing autosave -- seems to not work at all anymore.","position":-1.3134439250711565,"last_edited":1456637588882,"task_id":"cb1f16e2-9c4f-470b-946e-41fa7021f8da","done":1456637588468}
{"desc":"#0 #v1 (0:30?) (0:14)\nautosave sensibility\n\nmake it so autosave doesn't save again if file was saved -- by *any* client -- in the last n seconds.  This is much more logical than saving every interval/num_clients seconds.","position":-1.313443951366935,"last_edited":1456638496131,"task_id":"fe21793f-8acb-4fe5-a837-3b8d52f22d9e","done":1456638495700}
{"desc":"#v2\nsave on close\n\nrequires save *must* have a callback that works.\n\n\n","position":-1.3134439514833502,"last_edited":1456638741422,"task_id":"b41789d0-15b5-41e7-b63b-317a367abf79"}
{"desc":"#v1 #0 (4:30)\nsynctable craziness -- I spent quite a bit of time rewriting the synctable code as a \"finite state machine\", which really clarifies how things should work.\n\nGot this:\n\n    1 query on 'cursors' returned undefined\n    641 synctable.coffee:437 _update_change(cursors): tried to call _update_change even though local not yet defined (ignoring)\n\ni.e., 641 warnings in a row when I opened a file.  Right when the file *really* opened, the warnings stopped.\n\nThe cursors table fails in the first place due to the server being overloaded... with the second problem, I think.\n\n> tried to call _update_change even though local not yet defined\n\nProbably the solution this case is to ignore the updates and try to reconnect.... right?\n\nOr maybe have one single control point with some thought through rules.  A clear defined \"state machine\" for sync tables would help a lot.  The states are:\n\n1. connecting:   disconnected, not initialized, trying to connect\n2. connected:    initialized\n3. reconnecting: disconnected, initialized (so data may be stale, e.g. offline mode), and trying to connect\n\nHere's the state machine:\n\n\t1 --> 2 <--> 3\n\n\nNEXT STEP: On reconnect, send all offline changes... that we want to send...?","position":-1.3134439514542464,"last_edited":1456727321819,"task_id":"23102a85-d8da-402e-abaa-846513ec03ca","done":1456727321411}
{"desc":"#d4 #0 #v1 (1:30?)\nfor safety -- in a VERY RARE situation in which two different users evaluate code in the same worksheet at the exact same ms, we need to see whether it is possible to change this code in syncstring_evaluator (in call) around \n\n        @_inputs.set\n            id    : [@string._string_id, time, 0]\n            input : misc.copy_without(opts, 'cb')\n            \nto also include some way of knowing for sure. \n\nIdeas: \n\n- Make the set fail (and have a cb) if there was already something in the table with that id.  RethinkDB might be able to do this efficiently.  Would simply retry in such a case. \n \n- Include a uuid with the evaluation, which comes back.  Fail after the fact if something wrong comes back.  Seems annoying.","position":-1.313443951512454,"last_edited":1458310571212,"task_id":"75890974-62b0-4681-8a48-e8fd2a8716ea"}
{"desc":"#0 (1:00?) (0:26)\ntest and fix all obvious sync issues with user having their clock massively skewed.","position":-1.3134439514979022,"last_edited":1456642618822,"task_id":"c2b0285a-461c-498e-95c7-44c6132c4257","done":1456642618415}
{"desc":"#d0 #0 #v1 (0:30?) (0:04) #worry #now\nensure syncstring snapshotting only uses the server time, not the user's time.\n\nNO problem.","position":-1.3134439515051781,"last_edited":1458146104843,"task_id":"d990a5fd-df43-4db7-b117-bc65d66c3a3c","done":1458146104429}
{"desc":"general thought/brainstorm.\n\nI really need to come up with a new approach to computable synchronized documents that somehow 100% ensures there is no \"render error\" situation.  This is annoying right now.    Thoughts go here.  Basically, I think I need the document to be one step removed from anything involving rendering.","position":-1.3134439515015401,"last_edited":1456642738402,"task_id":"8dd26639-5863-4c89-a660-ffb91dc8ccc7"}
{"desc":"implement this different time sync protocol instead of the SNTP algorithm?\n\nhttp://www.mine-control.com/zack/timesync/timesync.html","position":-1.3134439514997212,"last_edited":1456644695983,"task_id":"0261d84b-f646-4955-bf46-7ad07bfd98d6"}
{"desc":"#d3 #v2 (1:30?)\nmake it so chat file only gets created when a user does the first chat, rather than always.","position":-1.3134439562920734,"last_edited":1458263578947,"task_id":"56520240-72d0-410c-b5a9-a6efeee536ba"}
{"desc":"#now #d1 #0 #v1 (0:45?) (0:15)\nstrip trailing whitespace should not do anything on *any* lines with a cursor at the end (of any user).","position":-1.3134439561837432,"last_edited":1458153154734,"task_id":"5b0ce94e-d462-46ed-99c3-c8616fcb9f84","done":1458153154317}
{"desc":"#v1 (0:30?) (0:11) #now\nput file_read_from_project back in local hub -- it's used, e.g., for reading .term file.  Still need it.... ?\n\nOr change to not use it anywhere","position":-1.3134439514506084,"last_edited":1456728284981,"task_id":"8a95eb0d-cb8b-4eef-91be-ea86cffa05e4","done":1456728284573}
{"desc":"#d3 #0 #v1 (1:30?)\nserious worry -- there are many connections between any hub and the project.\n\nMake sure that if one of them fails, then all the sync tables that used that connection\nare reconnected... or set to disconnected in case there are no other hub connections.\n\nThis is definitely not implemented, and wouldn't be seen in testing. Worry about it.","position":-1.3134439562920726,"last_edited":1458263572443,"task_id":"e088ba69-e47d-41a0-a57a-c3bb87de9db3"}
{"desc":"#v1 #sagews #0 (1:00?)\nrethink whether sagews evaluation needs to use the client directly\n\nIt seems that -- given that everthing goes in one direction through the db, that evaluation should be almost as fast without the back and forth string evaluator business, and also much simpler.\n\nRethought -- I like what I wrote as is; it also makes it possible for the initiator of the computation to do various visual optimizations that aren't needed by others.","position":-1.313443951453337,"last_edited":1456797131815,"task_id":"3c585e4d-5202-4581-849f-20b195c95ff2","done":1456797131409}
{"desc":"","position":-1.3134439514537917,"last_edited":1456727230061,"task_id":"4efe3e99-73fb-4ac1-9fd3-2efb1a8c93f2","deleted":true}
{"desc":"#1 #v1 (1:10)\ncursor table is totally crazy; need to debounce and generally think much harder about how this should work.","position":-1.313443951454019,"last_edited":1456801206979,"task_id":"663ec628-e962-4a16-b318-58c216dadbc4","done":1456801206562}
{"desc":"(0:20?)\nwatch this at some point soon-ish: \nhttps://www.youtube.com/watch?v=DyGoHAP8B_s\n\nJupyter : Ipython, State Of Multiuser And Real Time Collaboration | SciPy 2015 | Matthias Bussonnier","position":-1.3134439514988117,"last_edited":1457040698046,"task_id":"8ad903cf-7bec-4c02-8eda-ea3340f2aa8d"}
{"desc":"#v1 (0:30?)\nwhen doing save, if it doesn't finish within a few second... try again, instead of just waiting.\n\nUPDATE: this was a result of some other bug in tasks.","position":-1.3134439562891203,"last_edited":1458047938710,"task_id":"d69104e4-45d9-4ceb-8db5-3015fe71c290","done":1458047938288}
{"desc":"#jupyter #1 #v1 (0:30?)\ncurrent code for opening jupyter notebook doesn't give up on fail, even if one closes the tab.","position":-1.3134439562909392,"last_edited":1457733718438,"task_id":"72e54d74-a49b-4715-bcb1-782a7ae98efe","done":1457733718029}
{"desc":"#0 #jupyter (1:30+)\nimplement basic sync\n\n- [x] test it when DOM and syncstring change at same time, using a debounce parameter and multiple laptops.\n\nhttps://cloud.sagemath.com/45f4aab5-7698-4ac8-9f63-9fd307401ad7/port/54249/#projects/c731f7ba-0d87-47f5-8a66-0e47f8e0a9a7/files/simple.ipynb","position":-1.3134439562918487,"last_edited":1457466583474,"task_id":"68bc24ef-427c-4afc-b3f0-868dbc8c8262","done":1457466583064}
{"desc":"#0 #jupyter (1:30?)\nre-implement other user cursors","position":-1.3134439562923035,"last_edited":1457501830292,"task_id":"f474bcea-11a4-41ce-9b7d-b97ebcccaaee","done":1457501829882}
{"desc":"#0 #jupyter #now (1:00?) (1:05)\nfinish monkey patches:\n\n- [x] get rid of jupyter's own autosave\n- [x] jupyter save, etc. buttons","position":-1.313443956292076,"last_edited":1457479925689,"task_id":"aedeec16-38db-495c-83fa-9deccf756777","done":1457479925280}
{"desc":"#1 #jupyter\nset doc to what is on disk or in syncstring based on which is newer.","position":-1.3134439562919624,"last_edited":1457662025931,"task_id":"ec622644-954c-4f55-8144-f155a4db5c7e","done":1457662025522}
{"desc":"#0 #d0 #v1 (0:45?) (0:27) #now\nwhen saving file, only show error if fail to save for 30s after user activity stops.","position":-1.3134439562920193,"last_edited":1458148003741,"task_id":"3c01196c-3075-4c16-a3d2-6f5e5db9c746","done":1458148003331}
{"desc":"#0 #jupyter (1:15?)\nmake buttons work:\n\n- [x] (0:27) save\n- [x] (1:47) timetravel\n- [x] about\n- [x] reload\n- [x] (0:12) publish\n","position":-1.3134439562921898,"last_edited":1457489044028,"task_id":"44363717-f563-4d16-92f6-e0fe718dcb08","done":1457489043613}
{"desc":"#1 #jupyter (1:15?)\nmake syncstring kernel sync work\n\n\t        # first line is metadata... (TODO: ignore for now; will need to use to set Jupyter kernel, etc.)\n\nRight now, changing the kernel results in the document completely blanking!","position":-1.313443956292133,"last_edited":1457662020829,"task_id":"c934f299-b29f-4080-97a4-99f471dff6fd","done":1457662020420}
{"desc":"#d1 #v1 #2 #jupyter (0:45?) (0:03+)\nensure that publishing jupyter notebooks works still\n\n- This needs to be tested on a real server (not project dev)","position":16.9375,"last_edited":1458240835360,"task_id":"be223d2b-7df1-406b-a0db-4d7a8fe50234"}
{"desc":"#0 #jupyter (0:30?)\nload indicator","position":-1.3134439562922466,"last_edited":1457501806383,"task_id":"b376e212-5975-4400-a158-dd6f7eaed41f","done":1457501805939}
{"desc":"#0 #v1 #jupyter (0:45?) (1:15) #now\ncommand/control+s keyboard shortcut","position":-1.3134439562921791,"last_edited":1457759332871,"task_id":"4bdd23be-4d27-4b5e-a369-018696c1e723","done":1457759332461}
{"desc":"#v2 #jupyter (0:45?)\nprint to pdf -- maybe implement my own?\n\n#idea:  I might implement a separate jupyter to pdf button, with a nice error display. I’m putting that on the list. It’s like our number one reported problem (?).","position":-1.3134439562921756,"last_edited":1457765413101,"task_id":"ba73f6e0-9a4d-4af5-9f2d-7e32df6e7729"}
{"desc":"#0 #now #jupyter (1:33+)\nwrite a function that takes as input a Jupyter syncstring, and returns a nearby one that is valid.\n\nThis will require reading the source and knowing exactly what \"valid Jupyter syncstring\" means.  Clearly relying on calling fromJSON of Jupyter is a disaster.\n\n- if parsing metadata fails, go back in history until metadata is valid.\n\nIdea:\n\n- when we set a cell, we always use \"mutate\" and give as input the string that defines the cell.\n- when setting a cell that way, we also record the string with the cell, and when getting the value of that cell, use that... unless the source of the cell changed (the codemirror editor).\n- ","position":-1.3134439562921187,"last_edited":1457662012405,"task_id":"c36bf40d-8145-4171-8e68-2dccdc328b4f","done":1457662011981}
{"desc":"- #jupyter #idea\nif parsing metadata fails, go back in history until metadata is valid.","position":-1.3134439562919056,"last_edited":1457636998652,"task_id":"828e765a-c6c3-4cd5-a7fb-73e4450aefed","deleted":true}
{"desc":"","position":-1.3134439562918772,"last_edited":1457974544332,"task_id":"c2341ae7-7b9a-4da2-82da-c955f7d73dca","deleted":true}
{"desc":"#jupyter #v2\nthere is a spec for the file format\n\nhttps://github.com/jdfreder/single-cell-rtcollab/issues/3#issuecomment-195502647","position":-1.3134439562919908,"last_edited":1457733551658,"task_id":"58133950-5f97-4e1a-8662-a7b543b12274"}
{"desc":"#0 #v1 #jupyter (1:30?) #now\nre-implement jupyter cursors","position":-1.3134439562921827,"last_edited":1457754628845,"task_id":"01345f35-9611-41b1-986b-d03e912061d9","done":1457754628434}
{"desc":"#v1 #0 #jupyter (0:30?) #now (0:04)\nswitch to last modified version of file on disk -- use correct time.\n\nchecked: it does seem to work now.","position":-1.313443956292005,"last_edited":1457765730174,"task_id":"15576653-f338-46f2-b495-6729d6992f45","done":1457765729765}
{"desc":"#0 #v1 #critical #bug\nif syncstring `_save` happens but we are NOT connected, then I think those changes\nare irrevocably lost, and the user can no longer ever save their document.\n\nThis is, I think, because the extra patches not written back don't get merged in on reconnect.\nInstead, the synctable just gets reset and the not-saved patches are lost.","position":-1.3134439561908948,"last_edited":1457770685763,"task_id":"a8b51733-8e54-4ea5-bfe4-c4ffd7c2477b","done":1457770685350}
{"desc":"#0 #v1 #now (0:45?)\n\ntable.set -- if not connected, instead of an error, do it once connected...","position":-1.313443956292062,"last_edited":1458099894145,"task_id":"f5fb6587-a121-4116-b723-bbd8ad09d6bc","done":1458099893732}
{"desc":"#now #jupyter #v1 #0 (0:05+) (0:40)\ncursors go crazy sometimes. \nBasically flickering back and forth.\n\nIdea: put in a timestamp and only update cursor when new cursor is newer.\n\nDAMN: can't replicate; may have been a caching issue.  :-(\n\nCan replicate by having two browser from SAME person open at once -- then cursor skips back\nand forth quickly between the two versions of the SAME person.\n\nFix: throttle, on both ends.","position":-1.3134439561837894,"last_edited":1457850309362,"task_id":"8fc4409e-f2a1-432e-94c6-17ceefa1421d","done":1457850308951}
{"desc":"#v2 #jupyter\nhave user's face next to cell they are editing.\n","position":-1.3134439561863474,"last_edited":1457807271683,"task_id":"5c7d2388-d495-4ffb-b7a3-6bff7bd2db45"}
{"desc":"#0 #v1\njupter history doesn't work when file is in subdirectory.","position":-1.3134439561890758,"last_edited":1457842236104,"task_id":"b3861ae4-654b-4c16-86fb-349701eeb7da","done":1457842235699}
{"desc":"#v1 #jupyter #0 (0:45?) #now\nneed special logic to deal with two people editing a markdown cell at once.","position":-1.3134439561858926,"last_edited":1457856264347,"task_id":"e2a86c8e-82d7-4400-8fd1-c1413ccc97a4","done":1457856263938}
{"desc":"#0 #jupyter #v1 (1:00?) (0:27) #now\ndirty checking maybe not 100%\n\nOnce or twice I typed something and it vanished - I think this must be due to a failure of dirty checking to work properly in JupyterWrapper.","position":-1.3134439561840736,"last_edited":1457847946579,"task_id":"a4b01a82-7a34-45cd-9ebb-d8b12dce126c","done":1457847946159}
{"desc":"#0 #jupyter #v1 (0:45?) (1:08)\nwhen running computation the [*] for running (for execution count) is maybe not synced out.\n\ncommit: jupyter sync -- properly display execution count during running; also, fix subtle loading issue that could result in file corruption (doubling the file).","position":-1.3134439561838462,"last_edited":1457846504490,"task_id":"046dadc9-0a70-4b43-911e-6e5951292509","done":1457846282634}
{"desc":"#now #d1 #0 #jupyter #v1 (1:00?) (0:31)\njupyter -- implement reload button again -- it's referred to in documentation.","position":16.9453125,"last_edited":1458156805449,"task_id":"a5153bb4-abd0-46a1-a463-1dd8a7f30d2f","done":1458156805039}
{"desc":"#v1 #tasks (0:20)\nfix weird issue where saving indicator keeps spinning for task list.\n\nThis is just something that involves sync api event change.","position":-1.3134439562920122,"last_edited":1457835961692,"task_id":"e5a7665b-ca39-460f-a1c5-81de2784934b","done":1457835896135}
{"desc":"#0 #v1 (0:30?) (0:14)\nI see this on reconnect from idle sometimes:\n\n    _save error: no user set queries of 'stats' allowed\n    \nI think I fixed it by better messages and don't do any setting of tables that user can't set in synctable.    ","position":-1.3134439562920703,"last_edited":1458096610562,"task_id":"70b3e180-31cb-4622-9e22-bae5bb80a666","done":1458096610141}
{"desc":"#1 #v1 major badness:\n\nUPDATE: this seems fixed now due to my many other recent fixes.\n\nI investigated and made some small change -- but not sure.  Will leave this open.\n\nfor some reason getting massive blast of cursor sets from client -- exactly same cursor message with same time:\n\n    2016-03-13T08:05:18.478Z - debug: Client(iAN47nJDHFFVfzqXAAIt).user_query: account_id=b9272559-5467-4a76-810b-6555eaa93f78 makes query='[{\"cursors\":{\"id\":[\"42fdc\n    3b073fcad5dc25b807feca231a1eb2b2442\",0],\"locs\":[{\"x\":11,\"y\":473}],\"time\":\"2016-03-13T08:01:46.086Z\"}}]'\n    2016-03-13T08:05:18.478Z - debug: RethinkDB.user_set_query(account_id='b9272559-5467-4a76-810b-6555eaa93f78', table='cursors'): {\"id\":[\"42fdc3b073fcad5dc25b807fe\n    ca231a1eb2b2442\",0],\"locs\":[{\"x\":11,\"y\":473}],\"time\":\"2016-03-13T08:01:46.086Z\"}\n    2016-03-13T08:05:18.479Z - debug: [1 concurrent]  [0 modified]  rethink: query time using (779840b2-8815-417d-8a08-3c4c38c638f8) took 7ms; average=7ms;  -- 'r.db\n    (\"smc\").table(\"cursors\", {\"readMode\": \"outdated\"}).insert({\"id\": [\"42fdc3b073fcad5dc25b807feca...cs\": [{\"x\": 11, \"y\": 473}], \"time\": r.ISO8601(\"2016-03-13T08:01:\n    46.086Z\")}, {\"conflict\": \"update\"}) (220 chars)'\n    \n    2016-03-13T08:05:18.480Z - debug: [2 concurrent]  rethink: query -- 'r.db(\"smc\").table(\"cursors\", {\"readMode\": \"outdated\"}).insert({\"id\": [\"42fdc3b073fcad5dc25b8\n    07feca...cs\": [{\"x\": 11, \"y\": 473}], \"time\": r.ISO8601(\"2016-03-13T08:01:46.086Z\")}, {\"conflict\": \"update\"}) (220 chars)'\n    2016-03-13T08:05:18.481Z - debug: hub <-- client (client=iAN47nJDHFFVfzqXAAIt): {\"event\":\"query\",\"query\":\"[object]\",\"multi_response\":false,\"options\":\"[object]\",\"\n    id\":\"1b432a64-ae24-48f3-89a2-290e48a...\n    2016-03-13T08:05:18.481Z - debug: Client(iAN47nJDHFFVfzqXAAIt).user_query: account_id=b9272559-5467-4a76-810b-6555eaa93f78 makes query='[{\"cursors\":{\"id\":[\"42fdc\n    3b073fcad5dc25b807feca231a1eb2b2442\",0],\"locs\":[{\"x\":11,\"y\":473}],\"time\":\"2016-03-13T08:01:46.086Z\"}}]'","position":-1.3134439561837539,"last_edited":1458047739500,"task_id":"25a46b5f-5994-47d9-80a5-18466ab0eb3b","done":1458047739082}
{"desc":"#0\nensure before-change syncstring gets made live for all doc types, including jupyter.\n\n- [x] tasks\n\n\n- [x] codemirror editor\n- [x] write a codemirror editor \"bot\" and use for testing: `smc.editors['tmp/2016-03-14-155936.txt'].syncdoc.testbot()`\n- [x] write a jupyter editor \"bot\"\n","position":-1.3134439562873013,"last_edited":1458015924609,"task_id":"7bac9781-42c8-49e3-8b65-b21f09d706da","done":1458015924198}
{"desc":"#0 #now\njupyter: deal with corruption in a way that definitely works.\n\n- whatever solution I come up with... also use for syncdb (task list) AND TEST.\n\nIdeas:\n\n- elect leader\n- who is based on hash of recent\n\n- attempt after random timeout  <--- what about this?\n - More precisely: say, wait a random time up to 15s, and if \n\n- at point when error exists, look in history at who last edited this cell and have that user fix it.  Would break if they vanish.\n\nThe whole reason there is a problem at all is that in order to sync, user has to use \na valid JSON representation.   What if patching starts from last *valid* representation?\nBut then nobody would fix it.  Hmm.\n\nAnother idea: choose a random time t, say \"up to 15s\", and do the following:\n - record patch that would fix it now and pretend that was the last saved version (so we won't do the fix)\n - wait until time t.\n - if at time t the live syncstring is still broken, apply our \"fix it\" patch.\n - doesn't really fit our syncstring abstraction.\n\nAnother idea: chose a random time t (between 1 and 5s) and LOCK the editor that long; but still wait for new updates.  If no updates arrive, sync out (thus fixing).  If an update arrives, reduce to previous case.\n\nInstead of LOCK, we could allow user to make changes, but do not save.\nWhen update arrives, compute patch from when started LOCK until now, \nupdate to version that just arrived, then apply patch.\n\nFirst step to test: \n - suppose we get an update that breaks things.\n - then we get another update but we haven't locally made any changes -- just apply that and don't try to fix.\n - this would be stable and deal with most problems well.  HMM.\n \nXXX --I finally settled on this final idea; it avoids all subtle locking issues.\nIt's purely local.   It works in testing.  There's a small potential for losing\na little bit, but only in the case of a corrupted worklosinheet, when bad\nthings should be happening anyways.","position":-1.3134439562882108,"last_edited":1458018453667,"task_id":"0c6ec314-84fc-4e61-86f6-aa7d419e9570","done":1458018423803}
{"desc":"#0 #v1 #jupyter (0:30?) (0:12) #now\nensure that autosave works....","position":-1.3134439562920157,"last_edited":1458098741357,"task_id":"2331a814-ad74-492a-9db1-c49ae8e6b715","done":1458098740952}
{"desc":"#v1 #bug (0:30?) #notifications","position":-1.313443956292014,"last_edited":1458142061708,"task_id":"aa8cac88-0b25-474e-a3e3-e5dc3ed73d7e","deleted":true}
{"desc":"#v1 #0 #speed #now \n\nThe task list feels slow and uses a lot of cpu on every keystroke.  Investigate!!\n\n- [x] (easy) (0:19) debounce the normal saves/updates; this will speed things up a lot regarding interactive use.\n- [x] (1:55) #now (harder) optimize save_live -- why does it take so long?\n\nThis function `value` in syncstring takes all the time (it took 1:14 of logging):\n\n    smc.editors['smc/src/misc/wstein/sync.tasks'].wrapped.db._doc._syncstring._patch_list.value()\n\nIt turns out that the main reason things were visibly slow were due\nto a typo in the caching code, which a compiler or static type checking\nwould have caught.  1:45 to figure this out.  Hmm.  Of course, I added\nway better documentation while figuring this out, which is good. And\nI understand things better.","position":-1.3134439562920752,"last_edited":1458081948787,"task_id":"4db8e40b-5fbb-4609-b0f9-a3eb374e4a71","done":1458081948377}
{"desc":"#0 #d3 #jupyter #v1 (1:00?)\nadd watching for modification of .ipynb file and reload on mod (github #96)","position":-1.3134439562920717,"last_edited":1458310555338,"task_id":"eca25755-b66d-4890-9efc-83e8c0667d12"}
{"desc":"#v1 #0 #speed #now\n\nfor testing","position":-1.313443956292075,"last_edited":1458081649825,"task_id":"f2c6a467-60d5-4a08-b780-1b00ad358cb9","deleted":true}
{"desc":"#v1 #now\n","position":-1.3134439562920757,"last_edited":1458080734912,"task_id":"d65be3a3-1c11-47b2-ae31-8465e1279e70","deleted":true}
{"desc":"#0 #v1 #now (1:30?) (2:40)\noptimize patch_list value further in syncstring.coffee\n\n- [x] Record up to N distinct values in the cache; maybe make it lru, and record something in the cache if the number of patches needed to compute it exceeds a threshold, which will indirectly mean they get spread out.\n- [x] When inserting new patches, if a patch is older than something in the cache, remove that from the cache.\n- [x] Use best known cached value when computing new value in all cases.\n","position":-1.3134439562920746,"last_edited":1458091834002,"task_id":"d66e0e04-a38e-4c0b-b21b-77613deed70a","done":1458091833594}
{"desc":"#v1 #0 #bug #tasks (0:20?) (0:13) \nopen file from history doesn't work on task list in a subdirectory","position":-1.3134439562920661,"last_edited":1458097450095,"task_id":"72030eea-8933-4372-b525-64bafaeb6d79","done":1458097449680}
{"desc":"#0 #d0 #v1 (1:00?) (0:42) #jupyter #notifications #now\nnotifications area for jupyter shows the wrong filename.\n\nAlso, there is a function that converts: `path/.foo.sage-chat` to `path/foo.sage-chat`\n\nIt gets it wrong for chatrooms, at least ones in a subdirectory. Fix.","position":-1.3134439562920748,"last_edited":1458151764019,"task_id":"4a7cca22-7bdb-42e9-84dd-321ae31189f8","done":1458151763610}
{"desc":"#0 #d2 #v1 (0:30?) (0:35) #now\ngive patches/cursors table an index, if they need one (?)","position":-1.3134439562920737,"last_edited":1458245210893,"task_id":"f0e3b2d1-fadd-46ab-9759-9a1d80782e97","done":1458245210484}
{"desc":"#0 #v1 #d1 (0:30?) (0:10)\nin history brower, attribute the project for changes caused by the filesystem.","position":-1.3134439562920739,"last_edited":1458241723350,"task_id":"13616614-37cf-4051-8926-8db287a8900a","done":1458241722937}
{"desc":"#0 #d2 #v1 (1:00?) (1:00) #now\nconsider adding caching to make checking permissions for syncstrings more efficient.\n\nThough... maybe not, as this would also make things a little less robust.\nThat said, with a stream of patches coming, having to check perms for every one\nis a little worrisome.","position":-1.313443956292074,"last_edited":1458254096676,"task_id":"58ceadf2-f72d-4062-bd09-26cbb78418e9","done":1458254096267}
{"desc":"#0 #d3 #v1 (0:30?) (1:00) #now\nadd security to \"recent_syncstrings_in_project\" in syncstring_schema.coffee","position":-1.3134439562920728,"last_edited":1458249925760,"task_id":"625cf8dd-6cb8-4c45-bfc1-a4e31442b365","done":1458249925353}
{"desc":"#0 #v1 #d1 (0:30?) (0:10) #now\nsecurity for `eval_inputs` and `eval_outputs`","position":-1.3134439562920743,"last_edited":1458255396345,"task_id":"da5d9b66-2831-4cb6-aaed-b90c34733187","done":1458255395936}
{"desc":"#0 #v1 (0:30?)\ndoes autosave happen while not connected?","position":-1.3134439250472099,"last_edited":1458311990721,"task_id":"73182e02-e560-4d95-b459-e3496bedc04e"}