{"desc":"#v1 #1 #syncstring (2:30?)\nautomatic version snapshotting when editing files\n\nWhat we want is that every time n patches are saved since the last snapshot plus k seconds elapses, a new snapshot is made at k seconds ago.  Problem: who makes this new snapshot?  As long as the rules are well defined, and making a snapshot is idempotent, it doesn't matter too much.\n\nThis requires changing the database schema slightly.  This change is **critical** since right now, each time a new user gets added, the full base snapshot gets changefeed'd out to everybody.  Also, we are only saving one snapshot right now, not many of them. \n\n- [ ] loading old history","position":-1.3134439250479772,"last_edited":1451866246699,"task_id":"9d38081d-8304-46ae-9ef5-04678f97be12"}
{"desc":"#now (1:42) disconnect and reconnect needs to send patches made when offline.","position":-2,"last_edited":1448166337932,"task_id":"df5f6b4d-4a8a-43d1-bc08-aaaee7396c70","done":1448166205961}
{"desc":"#v2 #syncstring (3:30?) #8\naddress possibility that the same account_id records two edits to the same file at the same ms in time, but say from two browser sessions.  \n\nNightmare scenario: classroom of students all logged into same account editing same document\n\nSolution: somehow make it so that when client does a write, if there is already a record with that key (and a different value!!!!!!), then the write fails with appropriate error. The client can then increase the timestamp by 1ms and try again.\n\nPlan:\n\n- [ ] (1:00?) add generic schema option to make tables immutable, in that you can write new things, but you can't change anything already there.  If you do, then it just gives an error.  This could be generally useful in other ways.\n- [ ] (0:15?) make the patches table immutable\n- [ ] (0:45?) make synctable aware of immutable tables: add an event that is emitted whenever an issue involving immutability is encountered.\n- [ ] (1:30?) if hit immutability issue for a given patch with timestamp, add 1ms to time and try again until; make sure to update any relevant data structures in syncstring.\n\n\nOther worse ideas:\n\n- if same user is editing the syncstring in more than one browser, create multiple user numbers for them?\n- assign a slight time skew for each one, i.e., add a few ms of time to each one to make conflict impossible.","position":17,"last_edited":1451605251418,"task_id":"7d7cde76-1cdb-4900-a46c-54fe951f3c6a"}
{"desc":"synctable ack: make it so that each changefeed update waits k seconds for an ack; if it fails, it cancels that changefeed.\n\n#wontfix","position":2,"last_edited":1448215322102,"task_id":"ab953de2-e995-4a13-a379-aee3f94e01e6","done":1448215321680}
{"desc":"#now -- only save patches at most once every k ms, not after every single change.","position":-1.5,"last_edited":1448215211632,"task_id":"607373b4-218e-452c-b555-ee19fe0f7cb2","done":1448215211215}
{"desc":"#1 #v1 #syncstring #offline #rebase (2:30?)\nimplement way to rebase/apply offline changes, if they are old.\n\nWhen user comes back online (in reconnect function) instead of just sending all of our offline changes to the server, delete them from the table, and emit a message saying \"here are some unsaved changes\".  The syncstring could then apply those changes--i.e., rebase-- then make a new patch at the top that has the same effect as those changes.  (The user could be asked if they want to apply or not...)\n\n- [ ] (1:00?) add option to synctable so that instead of just applying offline changes on reconnect, they are removed and a message is emitted.\n- [ ] (1:30?) in syncstring, handle that emitted message; have to reply all changes merging our with whatever new changes have happened.  Avoid weird issues of having stuff disappear from synctable messing up our data structure.  Obtain resulting new string... and save as a new commit.","position":-1.3134439250425203,"last_edited":1451766855960,"task_id":"ae17e5dc-d0de-4294-b372-3b479afbb188"}
{"desc":"#v1 #syncstring #history #0 (4:30?) (1:31)\nhistory browser for files using new sync system\n\nTHE Design:\n\n- Same window instead of a separate tab (have another row of buttons to control, but make generic enough that it will work for everything, including tasks, jupyter, normal editing documents, etc.)\n- Load more history functionality for syncstring -- so can only show whatever is already in memory for editing, then have a button to \"load more\".\n\n\n- [ ] (1:30?) create ui that shows a history control row, basically like the current one, which pushes the editor start down by one row.\n- [ ] (0:45?) add a \"load more\" button when the history isn't fully in memory\n- [ ] (0:45?) make document be read only when history control shown.\n- [ ] (1:00?) display document when slider selects a particular point in history\n- [ ] (1:00?) button to revert document to display state\n","position":-1.312744140625,"last_edited":1451635201798,"task_id":"4c6d285c-e59f-4cc8-8af6-f1a5d71ea074","done":1451635201390}
{"desc":"#0 #local_hub (1:30?) (1:06) #now\nmake it so local_hub watches disk file, and if file changes on disk load from disk, and set string to that.","position":-1.314453125,"last_edited":1450506895847,"task_id":"887d5392-5e2a-486d-bbae-7b221f8fdd87","done":1450506895436}
{"desc":"#0 #local_hub (0:30?)\nimplement: save to disk\n\n- [ ] (0:20?) open syncstring in local_hub, if not already opened\n- [ ] (0:05?) compute current version\n- [ ] (0:05?) write it to file","position":-1.31298828125,"last_edited":1451435598837,"task_id":"a52d149d-58d4-4f14-821d-2223af53958b","done":1451435598420}
{"desc":"#local_hub\nfile change on disk resets live version to what is on disk","position":-1.03125,"last_edited":1448216806619,"task_id":"cebc557f-2029-4e6c-8d17-f36fdbbb9d8e","deleted":true}
{"desc":"#0 #local_hub (3:30?)\nability for local_hub to connect to synctables (no worry about security at all)\n- [x] (1:00?) (2:12) message framework from local_hub to hub\n    - keep map in memory of all connections from hubs to this local_hub, and when they last sent us a message\n    - function that calls local hub, i.e., sends a message.  If hub connected, it will try from most recent to least hub until one works (or fail if none work).\n    - also plugs into thing that receives messages to do callback.\n- [x] (1:30?) (1:42+) new message type from local_hub to hub for doing a query, plus handling\n- [x] (1:00?) project get query\n- [x] (1:00?) (1:30+) hook in synctable to this query functionality\n","position":-1.34375,"last_edited":1450380171740,"task_id":"f614e5cc-6dda-4620-8dd2-7b86819b638d","done":1450380171327}
{"desc":"#v1 #local_hub #2 (2:30?)\nsecurity model -- so project can only access very specific tables\n\n- [ ] message to handle query from local_hub: come up with way to allow queries; or for now only allow querying the syncstring and patches tables (?)\n\nThere are going to be many problems with the tables that might be difficult!  Address them all when things are working.","position":-1.015625,"last_edited":1451766621309,"task_id":"a4a5fc03-bcd1-4a28-a88c-0944f64e082c","done":1451766620900}
{"desc":"#syncstring #0 (2:00?)  (2:19) #now\nreplace existing syncstring with new db based one - so syncdb, chat, tasks, jupyter, etc., all automatically switch to use new syncstring...\n\n- [x] basic switch basic editors to work with new approach\n- [x] basic switch of syncdb-based stuff to work\n- [ ] really do new editors\n- [ ] (1:00?) read the exact api of old diffsync based syncstring\n- [x] (1:00?) write something the same based on new syncstring, then plug in\n- [ ] (3:00?) sage worksheets ! tricky!","position":-1.313446044921875,"last_edited":1451452398893,"task_id":"63639ea3-0267-421e-9626-e4199da18a59","done":1451452398482}
{"desc":"#syncstring (1:30?)\nreplace existing diffsync doc with new db based one - so file editing uses this.\n\n- [ ] (1:00?) rewrite some code in syncdoc.coffee, after replicating the syncstring api","position":-1.31341552734375,"last_edited":1451604953969,"task_id":"3b426f45-72da-4cbd-b125-3252ce3049a4","done":1451604953560}
{"desc":"#0 #v1 #local_hub (1:00?) #now\nopening a file on disk should properly create sync session using the file contents, but take into account timestamp.\n\n- [ ] (0:15?) open syncstring editing session.\n- [ ] (0:45?) if timestamp (??) of file is newer than that of most recent edits, set the syncstring to contents of file\n\ncheck to see if the above makes sense, given how git, etc., works.   Alternative could be that if timestamp of file changes, even to something old, then we always revert to that.  Not sure.  Could be an option.","position":-1.3134439270943403,"last_edited":1451763224008,"task_id":"20195c89-6f54-4d85-be3f-ccbea5c95d06","done":1451763223597}
{"desc":"#v2 #local_hub (1:30?)\ndisble/comment out all existing diffsync code in the local hub, etc., (since replaced by new syncstring code.)","position":16,"last_edited":1451605206213,"task_id":"7bd9b51d-b57e-4129-9d04-f30899e26af4"}
{"desc":"#v2 #9 #hub #limit #unclear\nlimit the document **history size** on the backend to avoid insanity.\n\nIdea: Make it so after n patches (say n=10000), something on the backend automatically truncates to a snapshot.  This could be done via some sort of generic capped collection notion in the schema.\nHowever, to keep it efficient, would have to be careful about only doing the truncation if there are a lot of writes happening in a relatively short amount of time (if there aren't... who cares).","position":10,"last_edited":1451605193691,"task_id":"9b2bc540-4880-40b6-9440-c7d8a1607bba"}
{"desc":"#v1 #1 #hub #limit #unclear (2:00?)\nlimit the editable document **size** on the backend to avoid insanity.\n\nDon't allow editing documents at all over a certain size.  Gracefully fail.  We already have this in the client/local_hub, but enforce in backend somehow clearly.\n\n- [ ] (1:00?) don't allow writing a patch beyond a certain size (already messages beyond a certain size are dropped)\n- [ ] (2:00?) make it so frontend and/or local hub reasonably gracefully fail when get an error about trying to write something that isn't allowed.","position":-1.3134439250588912,"last_edited":1451866293140,"task_id":"f9cab25d-9be6-49c4-a7c3-b2059d115c28"}
{"desc":"#v1 #1 #syncstring #security (1:30?)\nenforce schema for mapping thing that defines a syncstring to a uuid\n\nFor files in a project:\n\n   sha1('file', project_id, 'path/to/file')\n\nThen we can use something else besides \"file\" for other things, e.g., 'chat' for ephemeral chats, etc.\n\nWe then have to enforce exactly this in the schema and backend.\n\n#unclear -- we might not do this at all!  The reason is because of compound primary keys, etc.","position":5.5,"last_edited":1451866176158,"task_id":"cd0becca-f9d6-49b1-b324-76a894728c01"}
{"desc":"(2:00?) #client #0\nimplement cursors\n\n- [x] (0:30?) create another table, like syncstring, that is for cursors for a document\n- [x] (0:30?) (0:30) when editing set the cursor **positions** for a given client, and account_id, and when set.\n- [x] (1:00?) (1:53) everybody sees and renders accordingly\n\n- [x] (0:30?) (0:37) only broadcast cursor movement when user-initiated\n\n- Worry: Join synctable?\n- Cursors: who should report if same account but many client sessions?  probably last to move?\n\n```\nt = smc.client.sync_table({cursors:[{id:'string_id', locs:null, time:null}]})\nt.set({id:['string_id', 'user_id2'], locs:[{x:15,y:100}, {x:25,y:30}], time:new Date()}, 'none', function(e){console.log(e)})\n```\n","position":-1.31396484375,"last_edited":1451196433843,"task_id":"8929411f-daa6-46ff-9385-4ebb3d7e030c","done":1451196433425}
{"desc":"#0 (2:00?) (3:01) #client #syncstring #local_hub #now\nsave to file\n\nmake it so the save-to-file state is part of the syncstring state (in the database) and synchronized/persisted across all users.\n","position":-1.3203125,"last_edited":1450498987909,"task_id":"98614c7c-910c-4760-a655-f77bf51cfe0f","done":1450498987501}
{"desc":"#v2 #file (3:00?)\nfile move/rename\n\nActually properly handle file rename/move.  \n\nNot sure how, but natural would be to abstract away the id of the sync editing session (so has nothing to do with project_id/path).  Then file rename is a quick single change in the syncstrings table (namely, changing which file is being edited).  I like this.","position":9.5,"last_edited":1451766643754,"task_id":"20d61a3d-bc49-4474-a17a-f5c05b5529f9"}
{"desc":"#now (0:07) (0:15?) \n(unrelated to sync) fix the download link","position":-1.2421875,"last_edited":1448374552390,"task_id":"12238a67-7317-4ae6-be80-aa43738d88c3","done":1448374551986}
{"desc":"","position":-1.24609375,"last_edited":1448374602225,"task_id":"4e021924-d9c6-4141-81bf-94d71297b37f","deleted":true}
{"desc":"#local_hub (5:00?)\nlocal_hub refactor\n \n- [x] (2:30?) (0:10+) refactor local_hub.coffee into several files so code is easier to work on.\n    - [x] (0:20) diffsync_file\n    - [x] (0:05) desctructured import functions\n    - [x] (0:20?) (3:00) raw server  -- (way harder than expected since I wanted to also test; but valuable to get another one of those \"2 hour\" projects out of the way!)\n    - [x] (0:20?) (0:19) printing files\n    - [x] (0:20?) (0:55) console sessions\n    - [x] (0:20?) (0:39) optimize console session start (?) -- instead did maybe a slight cleanup.\n    - [x] (1:10?) (1:11) \"CodeMirror\" editing sessions and sage sessions\n    - [x] (0:30?) (0:32) factor out reading/writing file to project\n    - [x] (0:45?) (1:20) #now -- refactor jupyter server (?) -- and get it to work\n    \n- [x] more refactor:\n    - [x] (0:20?) (0:03+) blob handling -- do this later after have better way to send messages to hub\n\n- [x] (0:54+) delete lots of unused code, e.g., sage session.\n\n- [x] fix that sending blobs is flakie/broken in dev mode\n    ","position":-1.248046875,"last_edited":1448482559779,"task_id":"fce35c3a-8ebe-4fbd-a3ce-a9ac3d7a0b13","done":1448482559361}
{"desc":"#0 (1:00?) (0:26) #now merge in master","position":-1.2490234375,"last_edited":1450237972215,"task_id":"2caef818-2d72-40ef-9e8f-44ad3563854e","done":1450237971808}
{"desc":"#0 (1:30?) (1:05) #now\ninvestigate compressing the file snapshots.\n\nOptions..?\n\n- lz-string: http://pieroxy.net/blog/pages/lz-string/guide.html So in my testing in browser, about 1MB of coffeescript (not repeating – several different files) compressed to about 70k (so 7% of original size); and this takes about 250ms.; Decompressing takes about 30ms..    A 4MB file would lock the browser for 1s.  Not acceptable.\n\nWe could: (1) use web workers, or (2) use the local_hub, or (3) break up the strings to make lz-string more async.\n\n#rejected\n\nFrom the point of view of the browser in memory, there is no advantage to using compression at all – only a disadvantage, since it takes cpu cycles, and more memory to hold both the compressed and uncompressed version of things for some amount of time.\n\n4 minutes ago\nW\nWebsockets should/will/already do (?) support much better compression natively.\n\n4 minutes ago\nW\nIn any case, whether or not they do it now, they will at some point, and that is the place to deal with size. So doing anything related to compression in the browser isn’t really beneficial to the browser.\n\n1 minute ago\nW\nSo the only real motivation is reducing the database size.\n\n1 minute ago\nW\nHowever, I can just used a compressed filesystem, so that what rethinkdb stores is compressed.\n\nless than a minute ago\nW\nI definitely want to switch to running rethinkdb on zfs + compression, so I get snapshots anyways. ok, do don’t compress.","position":-1.375,"last_edited":1450306508204,"task_id":"add4ce43-42b0-4a1a-9969-3a9974b9d2af","done":1450306507790}
{"desc":"#0 #local_hub (1:00?) (1:47) #now\nget syncstring to work from local_hub","position":-1.328125,"last_edited":1450486278128,"task_id":"7cd3ec92-325e-45e4-a99e-41e6161fa79b","done":1450486277721}
{"desc":"#v2 #9 (0:45?)\nmake set queries on synctable verify that the keys being set are allowed by the original query.\n\nthis is mainly to avoid user/author confusion (me)","position":12,"last_edited":1451605179497,"task_id":"465f4480-598f-43bf-82ca-cd9a7baae726"}
{"desc":"#v1 #0 #now (0:45?) (1:20)\nmake the save ui button properly reflect state of save and hash\n\n(so is perfectly well defined across clients, etc.)\n\n- [x] (1:14) codemirror document editor\n","position":-1.3134441375732422,"last_edited":1451626549906,"task_id":"f74d1a70-2f0b-4d86-85b3-4f1cc1b8ea2a","done":1451626549495}
{"desc":"#0 #local_hub (2:00?) (1:47)\nopen a file for editing\n\ngiven a filename and project_id, function that opens that file for sync editing\n\n- [x] create a new table in the schema that defines the state of the local_hub, e.g., which files are open, by whom, etc.\n- [x] opening a file would also update that state thing\n- [x] this would cause the local hub to open the file for editing in memory\n- [x] set project_id and path when opening a file first time\n\nIt's very unclear what the paramters/semantics/behavior of this should all be:\n- what if file changes on disk and not open in local_hub?","position":-1.3134765625,"last_edited":1451443001013,"task_id":"73b8dda0-e1c0-425e-928f-cc9ed91b8175","done":1451443000588}
{"desc":"#v2 #9 (5:00?)\ntiered storage\n\n> Even text editing/history could be tiered though eventually, where all the data about editing a file gets moved from rethinkdb to GCS after a certain amount of time, then moved back when the user re-requests info. There would be a 3s pause (say) while this happens, but I think it could be made otherwise pretty transparent. So we go from lots of non-compressed patches/snapshots of files in the database to dumping them all to a single compressed JSON archive, uploading to GCS, etc.","position":14,"last_edited":1451605140925,"task_id":"6bbb023d-077f-4359-b6df-e398e5fba747"}
{"desc":"#v2 #9 #cursors (1:15?)\nmake a way for schema to ensure something about structure of cursors table?\n\nimagine invalid data goes in which makes the cursor parsing code get messed up...","position":15,"last_edited":1451605145528,"task_id":"e2568b9c-1481-47a1-b425-c58d274aeb05"}
{"desc":"#v2 #9 #compression (3:00?)\nre-investigate compression ideas:\n\nI realized something about compression when I was \"mulling things over\" during the break.   If patches/snapshots can work either compressed or not compressed, then we could have a backend process -- running on some dedicated machine -- that just goes along and compresses anything not compressed.  Since compression takes 10x longer than decompression, this might still be well worth doing.  Decompression in browser is still very fast.  \n\nAlso: are the patches stored as JSON strings or as a complicated javascript object?  This is relevant to compression questions...","position":14.5,"last_edited":1451605143393,"task_id":"4d8dedd9-d3af-4efb-a2ee-463197e10d3d"}
{"desc":"#v2 #3 #cursor (1:00?)\ncursor fade out still doesn't work well... :-(","position":-1.313232421875,"last_edited":1451766576210,"task_id":"f46c7ca2-d096-4fc2-9ed4-e1af76ba354d"}
{"desc":"#v1 #1 #error (1:30?)\nsyncdoc.coffee doesn't handle errors opening a file at all.\n\nIf you try to open a file from a (1) project you can't use or (2) path you can't access... or (3) file is too big... something needs to indicated this quickly in an error message.","position":-1.3134439250698051,"last_edited":1451866289855,"task_id":"1eecd2b8-3794-4230-8b69-7e638aef14c3"}
{"desc":"#v2 #2 (0:45?) \nget rid of SynchronizedDocument2 and SynchronziedDocument","position":18,"last_edited":1451605428225,"task_id":"07c5df4c-319d-4dbf-93e5-9951bab183fc"}
{"desc":"#0 #v1  #sagews  (3:00?)\nsage worksheets ! tricky!\n\n- [x] (0:15?) (0:06) initial rendering\n- [x] (2:00?) (4:25) code evaluation:  without using messages? -- is it possible? \nYES!  It takes about 100ms without even trying to write/read/write/read from database back and forth from my house to google.  So a reactive approach to evaluation of Sage worksheet code is viable.  How shall we structure it? [[WOW, that was hard.]]\n- [x] (1:00?) (1:15) actual code evaluation in worksheets\n- [x] (0:30?) (0:02) interacts\n- [x] (0:30?) (2:42) introspection\n- [x] (0:30?) (0:16) making blobs permanent\n- [x] (0:45?) (1:11) evaluation continuation when frontend stops working\n- [x] (1:00?) (0:26) ability to interrupt and restart\n- [ ] (0:15?) whenever session starts, have the backend stop anything listed as running.\n- [ ] BONUS: provide precise information to ALL about exactly what is running now and why evaluation of something new isn't happening.","position":-1.313443925086176,"last_edited":1451865794999,"task_id":"fbe46aac-7445-4051-b9a8-f45affed3b62","done":1451865794581}
{"desc":"#v1 #1 #now (1:30?) (0:11)\nrendering/sync of side chat is a mess; seems to get all corrupted on re-connect, etc.\n\n- maybe just redo using react\n\n- doesn't seem any worse than before...; may have been really messed up due to patch sort order issue with dates, which I fixed.","position":-1.3134440183639526,"last_edited":1451696992481,"task_id":"91802ede-525b-4306-8e39-d01a010237ec","done":1451696992073}
{"desc":"#v1 #0 (1:00?) (2:20) #now\ninitial version of document \n\n- [x] (0:30?) (1:05) use version on disk: if file exists on disk and last mod time is > last database patch, set value to what is on disk.\n\n- [x] (0:30?) (1:14) use template -- if when local hub opens the syncstring it finds that the file doesn't exist on disk *and* the syncstring is empty.","position":-1.3131103515625,"last_edited":1451621367205,"task_id":"1464eaa9-19c8-4859-8ba8-c86fec15866c","done":1451621366796}
{"desc":"#v1 #2 (3:00?)\ndo the release and make this new thing live.","position":9.75,"last_edited":1451866179865,"task_id":"7b02f4f8-e5d9-4833-815a-3fa6af53de12"}
{"desc":"#1 (0:45?) (1:18) #bug #now\nensure latest file changes on disk are reflected in memory\n\nbecause of debouncing or whatever, the file on disk can be slightly newer than what is in database via sync.  E.g., open local_hub.log...","position":-1.31304931640625,"last_edited":1451695830762,"task_id":"68aeba43-363a-4d49-8b54-28f2e763e082","done":1451695830352}
{"desc":"#0 #v1 #now (1:00?) (1:45)\nopening a file that already exists on disk for the first time\n\n- [x] should not show file as blank for a second -- instead show \"loading...\"\n\n- [x] ensure that the undo history doesn't start with \"empty document\", but with what got loaded\n\n- [x] opening that file for the first time is also SLOW.","position":-1.3134439587593079,"last_edited":1451713098749,"task_id":"9a52c6cf-4d53-4d26-bfe5-6911a38d96d9","done":1451713098341}
{"desc":"#0 #v1 #now\nfirst tim","position":-1.3130950927734375,"last_edited":1451616454575,"task_id":"386de53e-c466-4294-bdc8-a67432e7a259","deleted":true}
{"desc":"#v1 #0 (0:45?) (0:29) #now\nmaintain cursor position when opening/closing editor\n\n- could just rewrite them in react... :-(","position":-1.3131027221679688,"last_edited":1451628825683,"task_id":"d31962d9-e205-4b7e-ac33-0a9bbd8a6e01","done":1451628825270}
{"desc":"#0 #v1 #latex (1:30?) (0:17) \nlatex editor isn't doing the initial build for some reason\n\n- could also changes to execute commands via db state, rather than messages, exactly like will have to do for sage worksheets.  (save this for the react rewrite)","position":-1.3134439250725336,"last_edited":1451922569300,"task_id":"2e3c1e19-9957-4781-844a-fc26a45ab3cc","done":1451922568880}
{"desc":"#2 (1:30?)\npotential jupyter issues\n\n- [ ] make sure save/load works properly; don't have weird issues with files/timestamps, saving.","position":-1.313443660736084,"last_edited":1451626337567,"task_id":"887c92d3-7dd2-47ea-bb35-74d42b5e781b"}
{"desc":"#v1 #0 (1:30?) #now\nSynchronizedString improvements\n\n- [ ] (0:45?) (0:20+) #now make the save ui button properly reflect state of save and hash for tasks (and anything based on SynchronizedString) -- I'm goint to wait for the react rewrite to do this...\n\ndocs already done completely and nicely.\n\nthe test for this is edit a task, save, make a change, undo it, and see the save grey out.\n\n- [x] (0:45?) (0:12) like for SynchronizedDocument, ensure for a while that SynchronizedString save definitely saves before returning -- retry until success\n","position":-1.3134439253481105,"last_edited":1451766449672,"task_id":"c818fa66-1681-48af-a24e-5c6642424b2e","done":1451766449262}
{"desc":"#0 #v1 (0:30?) (0:12) #now\non first time to ever open file, the backend should set the save hash\n\nTest -- open new file and see the save button stay disabled.","position":-1.3134439382702112,"last_edited":1451755542608,"task_id":"e6419294-cb49-4db6-a629-e7ebafbc9ce0","done":1451755542199}
{"desc":"#0 #v1 (1:00?) #now (1:35)\nproperly handle read only files","position":-1.3134439326822758,"last_edited":1451761126692,"task_id":"1f19faf7-ac51-429c-82f5-9309d2b90dd3","done":1451761126282}
{"desc":"#0 #v1 (0:45?) (0:11)\nsagews history\n\nwow, it just works perfectly already, way *better* than live version!","position":-1.3134439256973565,"last_edited":1451763845092,"task_id":"5acba352-b1ae-4a9b-8ced-0788ec55f8da","done":1451763844676}
{"desc":"#0 #v1 #jupyter  (2:30?) #now\nrewrite jupyter sync to work with new sync\n\n- [x] (0:30) get it to sort of work\n- [ ] (1:15?) rewrite to use syncdb instead of direct syncdoc!\n- [ ] (1:15?) make a separate history viewer exactly like the others","position":-1.3134439250711694,"last_edited":1451924799092,"task_id":"d04c4061-20e6-4e05-852e-ab8c5872d181"}
{"desc":"#2 #v2 (1:30?)\ntasks and courses history slider","position":-1.3126907348632812,"last_edited":1451635177084,"task_id":"c7e7e008-999c-4fcf-bae3-6a8f106f3645"}
{"desc":"#2 #v2 (0:30?) #react\nuse timeago also for the main time description, and actual time in parens","position":-1.3127059936523438,"last_edited":1451672059923,"task_id":"9a6b3b07-2815-4ec5-a49b-88fc132783a3"}
{"desc":"#0 #v1 (0:45?) (0:45)\noptimize history\n\n- [x] (0:45) debounce slider\n- [x] make sure it is fast enough to go from one point to another","position":-1.3127021789550781,"last_edited":1451680760685,"task_id":"97975f59-cd9b-4b45-951e-8976273c96a9","done":1451680760275}
{"desc":"#2 #v2 (2:00?)\nway to link history to file somehow... (maybe import/export...?)","position":-1.3127002716064453,"last_edited":1451679319945,"task_id":"75904b9a-2330-48ef-875b-96be4ba6a99d"}
{"desc":"#2 #v2 (0:45?)\nadd blue button linking back from history to original file","position":-1.3127012252807617,"last_edited":1451679621101,"task_id":"b312e4a1-9f84-4425-ae66-82fe669b3471"}
{"desc":"#1 (1:00?) #bug\nwhen testing/developing locally on my laptop and the server time gets all messed up,\nedits get discarded.\n\n- maybe they appear to be slightly in the future?","position":-1.313018798828125,"last_edited":1451682203615,"task_id":"985c9e35-99af-4a94-9fb6-21d31ad80159"}
{"desc":"#2 #v1 (1:00?)\ncan we do anything sensible when the underlying file being edited is moved...?","position":9.875,"last_edited":1451713483464,"task_id":"1476e868-7e9b-43a3-866b-6d8adbc06c7d","deleted":true}
{"desc":"#1 #asap (0:30?)\nrebuild rethinkdb from source for compute nodes -- the 2.2.2 release is SHIT.\n","position":-1.3134439438581467,"last_edited":1451712186134,"task_id":"e22caa17-38e7-41f4-9ae9-1eb854903956"}
{"desc":"#1 (0:45?)\nany time load the file from disk, also set the \"hash of saved version\" to that.\n\nMaybe change \"saved version\" to \"disk version\" in function naming.\n\nEffect: newly opened file won't have save ready to go.  Also, when git checking out, etc., will do right thing.","position":-1.3134439513087273,"last_edited":1451712725511,"task_id":"9b5d9fa2-41de-4756-9f12-58db04de3c9c"}
{"desc":"","position":-1.3134437911212444,"last_edited":1451763281235,"task_id":"424f4d76-4230-4f20-9871-7b69952f30d4","deleted":true}
{"desc":"#v2 #3 (1:00?) (0:20+)  #react\nmake the save ui button properly reflect state of save and hash \n\n- for tasks (and anything based on SynchronizedString) -- I'm goint to wait for the react rewrite to do this...","position":-1.3134439251734875,"last_edited":1451766441743,"task_id":"14d46970-33df-471d-915f-01c8e3a80b33"}
{"desc":"#2 (0:30?) #v2 #history\nwhen thing changing the file is project -- rather than a user, put that in history properly.","position":-1.313443925075262,"last_edited":1451866014564,"task_id":"ae7e018c-c50f-4a2f-9551-406ccd0864f3"}
{"desc":"#0 #bug #history (0:20?)\nclosing the history does *NOT* actually work -- still get slider update messages, etc.","position":-1.313443925080719,"last_edited":1451786913332,"task_id":"a6b39352-b97f-41b0-9acc-2f3b20d25bea"}
{"desc":"#2 #question (0:20?)\nshould \"strip trailing whitespace\" be a menu option, rather than something that happens on save?  It's possibly very evil...","position":-1.3134439250834475,"last_edited":1451787557384,"task_id":"fdb7891a-3c11-49f4-a891-002d9cbf3b24"}
{"desc":"#1 #v1 #bug #history (0:20?)\nmaybe the history is missing the very last step (?)","position":-1.3134439250848118,"last_edited":1451865954583,"task_id":"7ae24149-6cb0-4a65-b6b0-7bf36b33158a","done":1451865954174}
{"desc":"#1 #v1 #bug\nsynchronized editing of sage worksheets leads to all kinds of corruption.\n\nmaybe soemthing  wrong with setvaluenojump...?\n\nalso had corruption with history, so temporarily disabled setvaluenojump","position":-1.313443925085494,"last_edited":1451806476270,"task_id":"84ecfe4e-55cf-4dec-ac0f-0e217dc454cf","deleted":true}
{"desc":"#v1 (1:00?)\nmake db set/get queries explicit and make synctable use them\n\nit's just too confusing otherwise.","position":-1.313443925085835,"last_edited":1451865926801,"task_id":"b79944b3-2611-4c5a-a2c1-354a075f0495","done":1451865926393}
{"desc":"#0 #v1 (0:15?) (0:04) #history\ndon't show name if not known","position":-1.3134439250704872,"last_edited":1451922828611,"task_id":"0123f2e1-77fb-4c3d-907f-bcf05834c8f3","done":1451922828202}
{"desc":"#1 #v1 (0:20?)\nremove \"revision tracking\" option from account settings; it is no longer optional.","position":-1.3134439250708283,"last_edited":1451923671750,"task_id":"f29c5697-3c07-4095-8dd1-511d5e3514d7"}