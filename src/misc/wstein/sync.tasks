{"desc":"#1 #syncstring (2:00?)\nautomatic version snapshotting when editing files\n\nWhat we want is that every time n patches are saved since the last snapshot plus k seconds elapses, a new snapshot is made at k seconds ago.  Problem: who makes this new snapshot?  As long as the rules are well defined, and making a snapshot is idempotent, it doesn't matter too much.\n\nThis requires changing the database schema slightly.  This change is **critical** since right now, each time a new user gets added, the full base snapshot gets changefeed'd out to everybody.  Also, we are only saving one snapshot right now, not many of them. ","position":-1.3125,"last_edited":1450499506168,"task_id":"9d38081d-8304-46ae-9ef5-04678f97be12"}
{"desc":"#now (1:42) disconnect and reconnect needs to send patches made when offline.","position":-2,"last_edited":1448166337932,"task_id":"df5f6b4d-4a8a-43d1-bc08-aaaee7396c70","done":1448166205961}
{"desc":"#syncstring (3:30?)\naddress possibility that the same account_id records two edits to the same file at the same ms in time, but say from two browser sessions.  \n\nNightmare scenario: classroom of students all logged into same account editing same document\n\nSolution: somehow make it so that when client does a write, if there is already a record with that key (and a different value!!!!!!), then the write fails with appropriate error. The client can then increase the timestamp by 1ms and try again.\n\nPlan:\n\n- [ ] (1:00?) add generic schema option to make tables immutable, in that you can write new things, but you can't change anything already there.  If you do, then it just gives an error.  This could be generally useful in other ways.\n- [ ] (0:15?) make the patches table immutable\n- [ ] (0:45?) make synctable aware of immutable tables: add an event that is emitted whenever an issue involving immutability is encountered.\n- [ ] (1:30?) if hit immutability issue for a given patch with timestamp, add 1ms to time and try again until; make sure to update any relevant data structures in syncstring.\n\n\nOther worse ideas:\n\n- if same user is editing the syncstring in more than one browser, create multiple user numbers for them?\n- assign a slight time skew for each one, i.e., add a few ms of time to each one to make conflict impossible.","position":1,"last_edited":1450301585425,"task_id":"7d7cde76-1cdb-4900-a46c-54fe951f3c6a"}
{"desc":"synctable ack: make it so that each changefeed update waits k seconds for an ack; if it fails, it cancels that changefeed.\n\n#wontfix","position":2,"last_edited":1448215322102,"task_id":"ab953de2-e995-4a13-a379-aee3f94e01e6","done":1448215321680}
{"desc":"#now -- only save patches at most once every k ms, not after every single change.","position":-1.5,"last_edited":1448215211632,"task_id":"607373b4-218e-452c-b555-ee19fe0f7cb2","done":1448215211215}
{"desc":"#syncstring #offline #rebase (2:30?)\nimplement way to rebase/apply offline changes, if they are old.\n\nWhen user comes back online (in reconnect function) instead of just sending all of our offline changes to the server, delete them from the table, and emit a message saying \"here are some unsaved changes\".  The syncstring could then apply those changes--i.e., rebase-- then make a new patch at the top that has the same effect as those changes.  (The user could be asked if they want to apply or not...)\n\n- [ ] (1:00?) add option to synctable so that instead of just applying offline changes on reconnect, they are removed and a message is emitted.\n- [ ] (1:30?) in syncstring, handle that emitted message; have to reply all changes merging our with whatever new changes have happened.  Avoid weird issues of having stuff disappear from synctable messing up our data structure.  Obtain resulting new string... and save as a new commit.","position":3,"last_edited":1448253192181,"task_id":"ae17e5dc-d0de-4294-b372-3b479afbb188"}
{"desc":"#syncstring #history(4:30?)\nhistory browser for files using new sync system\n\nDesign:\n\n- Same window instead of a separate tab (have another row of buttons to control, but make generic enough that it will work for everything, including tasks, jupyter, normal editing documents, etc.)\n- Load more history functionality for syncstring -- so can only show whatever is already in memory for editing, then have a button to \"load more\".\n\n\n- [ ] (1:30?) create ui that shows a history control row, basically like the current one, which pushes the editor start down by one row.\n- [ ] (0:45?) add a \"load more\" button when the history isn't fully in memory\n- [ ] (0:45?) make document be read only when history control shown.\n- [ ] (1:00?) display document when slider selects a particular point in history\n- [ ] (1:00?) button to revert document to display state\n","position":4,"last_edited":1448511700138,"task_id":"4c6d285c-e59f-4cc8-8af6-f1a5d71ea074"}
{"desc":"#0 #local_hub (1:30?) (1:06) #now\nmake it so local_hub watches disk file, and if file changes on disk load from disk, and set string to that.","position":-1.314453125,"last_edited":1450506895847,"task_id":"887d5392-5e2a-486d-bbae-7b221f8fdd87","done":1450506895436}
{"desc":"#local_hub (0:30?)\nimplement: save to disk\n\n- [ ] (0:20?) open syncstring in local_hub, if not already opened\n- [ ] (0:05?) compute current version\n- [ ] (0:05?) write it to file","position":-1.0625,"last_edited":1448216717621,"task_id":"a52d149d-58d4-4f14-821d-2223af53958b"}
{"desc":"#local_hub\nfile change on disk resets live version to what is on disk","position":-1.03125,"last_edited":1448216806619,"task_id":"cebc557f-2029-4e6c-8d17-f36fdbbb9d8e","deleted":true}
{"desc":"#0 #local_hub (3:30?)\nability for local_hub to connect to synctables (no worry about security at all)\n- [x] (1:00?) (2:12) message framework from local_hub to hub\n    - keep map in memory of all connections from hubs to this local_hub, and when they last sent us a message\n    - function that calls local hub, i.e., sends a message.  If hub connected, it will try from most recent to least hub until one works (or fail if none work).\n    - also plugs into thing that receives messages to do callback.\n- [x] (1:30?) (1:42+) new message type from local_hub to hub for doing a query, plus handling\n- [x] (1:00?) project get query\n- [x] (1:00?) (1:30+) hook in synctable to this query functionality\n","position":-1.34375,"last_edited":1450380171740,"task_id":"f614e5cc-6dda-4620-8dd2-7b86819b638d","done":1450380171327}
{"desc":"#local_hub (1:00?)\nsecurity model -- so project can only access very specific tables\n\n- [ ] message to handle query from local_hub: come up with way to allow queries; or for now only allow querying the syncstring and patches tables (?)","position":-1.015625,"last_edited":1448216886375,"task_id":"a4a5fc03-bcd1-4a28-a88c-0944f64e082c"}
{"desc":"#syncstring (2:00?)\nreplace existing syncstring with new db based one - so syncdb, chat, tasks, jupyter, etc., all automatically switch to use new syncstring...\n\n- [ ] (1:00?) read the exact api of old diffsynb based syncstring\n- [ ] (1:00?) write something the same based on new syncstring, then plug in","position":5,"last_edited":1448254686840,"task_id":"63639ea3-0267-421e-9626-e4199da18a59"}
{"desc":"#syncstring (1:30?)\nreplace existing diffsync doc with new db based one - so file editing uses this.\n\n- [ ] (1:30?) rewrite some code in syncdoc.coffee, after replicating the syncstring api","position":5.25,"last_edited":1448254690103,"task_id":"3b426f45-72da-4cbd-b125-3252ce3049a4"}
{"desc":"#local_hub (1:00?)\nopening a file on disk should properly create sync session using the file contents, but take into account timestamp.\n\n- [ ] (0:15?) open syncstring editing session.\n- [ ] (0:45?) if timestamp (??) of file is newer than that of most recent edits, set the syncstring to contents of file\n\ncheck to see if the above makes sense, given how git, etc., works.   Alternative could be that if timestamp of file changes, even to something old, then we always revert to that.  Not sure.  Could be an option.","position":7,"last_edited":1448254056395,"task_id":"20195c89-6f54-4d85-be3f-ccbea5c95d06"}
{"desc":"#local_hub (1:30?)\ndisble/comment out all existing diffsync code in the local hub, etc., (since replaced by new syncstring code.)","position":8,"last_edited":1448253894116,"task_id":"7bd9b51d-b57e-4129-9d04-f30899e26af4"}
{"desc":"#hub #limit #unclear\nlimit the document **history size** on the backend to avoid insanity.\n\nIdea: Make it so after n patches (say n=10000), something on the backend automatically truncates to a snapshot.  This could be done via some sort of generic capped collection notion in the schema.\nHowever, to keep it efficient, would have to be careful about only doing the truncation if there are a lot of writes happening in a relatively short amount of time (if there aren't... who cares).","position":10,"last_edited":1448254720470,"task_id":"9b2bc540-4880-40b6-9440-c7d8a1607bba"}
{"desc":"#hub #limit #unclear (3:00?)\nlimit the editable document **size** on the backend to avoid insanity.\n\nDon't allow editing documents at all over a certain size.  Gracefully fail.  We already have this in the client/local_hub, but enforce in backend somehow clearly.\n\n- [ ] (1:00?) don't allow writing a patch beyond a certain size (already messages beyond a certain size are dropped)\n- [ ] (2:00?) make it so frontend and/or local hub reasonably gracefully fail when get an error about trying to write something that isn't allowed.","position":9,"last_edited":1448254709471,"task_id":"f9cab25d-9be6-49c4-a7c3-b2059d115c28"}
{"desc":"#syncstring #security (1:30?)\nenforce schema for mapping thing that defines a syncstring to a uuid\n\nFor files in a project:\n\n   sha1('file', project_id, 'path/to/file')\n\nThen we can use something else besides \"file\" for other things, e.g., 'chat' for ephemeral chats, etc.\n\nWe then have to enforce exactly this in the schema and backend.","position":5.5,"last_edited":1448254085273,"task_id":"cd0becca-f9d6-49b1-b324-76a894728c01"}
{"desc":"(2:00?) #client #0\nimplement cursors\n\n- [x] (0:30?) create another table, like syncstring, that is for cursors for a document\n- [x] (0:30?) (0:30) when editing set the cursor **positions** for a given client, and account_id, and when set.\n- [x] (1:00?) (1:53) everybody sees and renders accordingly\n\n- [ ] (0:30?) #now only broadcast cursor movement when user-initiated\n\n- Worry: Join synctable?\n- Cursors: who should report if same account but many client sessions?  probably last to move?\n\n```\nt = smc.client.sync_table({cursors:[{id:'string_id', locs:null, time:null}]})\nt.set({id:['string_id', 'user_id2'], locs:[{x:15,y:100}, {x:25,y:30}], time:new Date()}, 'none', function(e){console.log(e)})\n```\n","position":-1.31396484375,"last_edited":1451194179540,"task_id":"8929411f-daa6-46ff-9385-4ebb3d7e030c"}
{"desc":"#0 (2:00?) (3:01) #client #syncstring #local_hub #now\nsave to file\n\nmake it so the save-to-file state is part of the syncstring state (in the database) and synchronized/persisted across all users.\n","position":-1.3203125,"last_edited":1450498987909,"task_id":"98614c7c-910c-4760-a655-f77bf51cfe0f","done":1450498987501}
{"desc":"(3:00?) #file\nfile move/rename\n\nActually properly handle file rename/move.  \n\nNot sure how, but natural would be to abstract away the id of the sync editing session (so has nothing to do with project_id/path).  Then file rename is a quick single change in the syncstrings table (namely, changing which file is being edited).  I like this.","position":11,"last_edited":1450301674480,"task_id":"20d61a3d-bc49-4474-a17a-f5c05b5529f9"}
{"desc":"#now (0:07) (0:15?) \n(unrelated to sync) fix the download link","position":-1.2421875,"last_edited":1448374552390,"task_id":"12238a67-7317-4ae6-be80-aa43738d88c3","done":1448374551986}
{"desc":"","position":-1.24609375,"last_edited":1448374602225,"task_id":"4e021924-d9c6-4141-81bf-94d71297b37f","deleted":true}
{"desc":"#local_hub (5:00?)\nlocal_hub refactor\n \n- [x] (2:30?) (0:10+) refactor local_hub.coffee into several files so code is easier to work on.\n    - [x] (0:20) diffsync_file\n    - [x] (0:05) desctructured import functions\n    - [x] (0:20?) (3:00) raw server  -- (way harder than expected since I wanted to also test; but valuable to get another one of those \"2 hour\" projects out of the way!)\n    - [x] (0:20?) (0:19) printing files\n    - [x] (0:20?) (0:55) console sessions\n    - [x] (0:20?) (0:39) optimize console session start (?) -- instead did maybe a slight cleanup.\n    - [x] (1:10?) (1:11) \"CodeMirror\" editing sessions and sage sessions\n    - [x] (0:30?) (0:32) factor out reading/writing file to project\n    - [x] (0:45?) (1:20) #now -- refactor jupyter server (?) -- and get it to work\n    \n- [x] more refactor:\n    - [x] (0:20?) (0:03+) blob handling -- do this later after have better way to send messages to hub\n\n- [x] (0:54+) delete lots of unused code, e.g., sage session.\n\n- [x] fix that sending blobs is flakie/broken in dev mode\n    ","position":-1.248046875,"last_edited":1448482559779,"task_id":"fce35c3a-8ebe-4fbd-a3ce-a9ac3d7a0b13","done":1448482559361}
{"desc":"#0 (1:00?) (0:26) #now merge in master","position":-1.2490234375,"last_edited":1450237972215,"task_id":"2caef818-2d72-40ef-9e8f-44ad3563854e","done":1450237971808}
{"desc":"#0 (1:30?) (1:05) #now\ninvestigate compressing the file snapshots.\n\nOptions..?\n\n- lz-string: http://pieroxy.net/blog/pages/lz-string/guide.html So in my testing in browser, about 1MB of coffeescript (not repeating – several different files) compressed to about 70k (so 7% of original size); and this takes about 250ms.; Decompressing takes about 30ms..    A 4MB file would lock the browser for 1s.  Not acceptable.\n\nWe could: (1) use web workers, or (2) use the local_hub, or (3) break up the strings to make lz-string more async.\n\n#rejected\n\nFrom the point of view of the browser in memory, there is no advantage to using compression at all – only a disadvantage, since it takes cpu cycles, and more memory to hold both the compressed and uncompressed version of things for some amount of time.\n\n4 minutes ago\nW\nWebsockets should/will/already do (?) support much better compression natively.\n\n4 minutes ago\nW\nIn any case, whether or not they do it now, they will at some point, and that is the place to deal with size. So doing anything related to compression in the browser isn’t really beneficial to the browser.\n\n1 minute ago\nW\nSo the only real motivation is reducing the database size.\n\n1 minute ago\nW\nHowever, I can just used a compressed filesystem, so that what rethinkdb stores is compressed.\n\nless than a minute ago\nW\nI definitely want to switch to running rethinkdb on zfs + compression, so I get snapshots anyways. ok, do don’t compress.","position":-1.375,"last_edited":1450306508204,"task_id":"add4ce43-42b0-4a1a-9969-3a9974b9d2af","done":1450306507790}
{"desc":"#0 #local_hub (1:00?) (1:47) #now\nget syncstring to work from local_hub","position":-1.328125,"last_edited":1450486278128,"task_id":"7cd3ec92-325e-45e4-a99e-41e6161fa79b","done":1450486277721}
{"desc":"#1 (0:30?)\nmake set queries on synctable verify that the keys being set are allowed by the original query.","position":12,"last_edited":1450494135693,"task_id":"465f4480-598f-43bf-82ca-cd9a7baae726"}
{"desc":"#1 (0:30?)\nmake the save ui button properly reflect state of save and hash\n\n(so is perfectly well defined across clients, etc.)","position":13,"last_edited":1450498313387,"task_id":"f74d1a70-2f0b-4d86-85b3-4f1cc1b8ea2a"}
{"desc":"#1 #local_hub (0:45?)\n\ngiven a filename and project_id, function that opens that file for sync editing","position":-1.3134765625,"last_edited":1450499503258,"task_id":"73b8dda0-e1c0-425e-928f-cc9ed91b8175"}
{"desc":"#3 (5:00?)\ntiered storage\n\n> Even text editing/history could be tiered though eventually, where all the data about editing a file gets moved from rethinkdb to GCS after a certain amount of time, then moved back when the user re-requests info. There would be a 3s pause (say) while this happens, but I think it could be made otherwise pretty transparent. So we go from lots of non-compressed patches/snapshots of files in the database to dumping them all to a single compressed JSON archive, uploading to GCS, etc.","position":14,"last_edited":1451165753279,"task_id":"6bbb023d-077f-4359-b6df-e398e5fba747"}
{"desc":"#cursors #later\nmake a way for schema to ensure something about structure of cursors table?\n\nimagine invalid data goes in which makes the cursor parsing code get messed up...","position":-1.313720703125,"last_edited":1451170206713,"task_id":"e2568b9c-1481-47a1-b425-c58d274aeb05"}