{"desc":"#0 #v1 #syncstring #now (2:30?)\nautomatic version snapshotting when editing files\n\nSomehow enforce rules on backend:\n  - one can never insert a patch in the syncstring table that is more than x minutes old\n\nstep 1:\n\n- [x] (2:25) when call snapshot need to: (1) set the snapshot in the patches table, (2) update the syncstring table to change the last_snapshot timestamp; when loading syncstring, use last_snapshot to restrict how much needs to be loaded by default\n\n- [x] (1:38) loading old history\n\n- [x] (0:45?) (0:47) integrate loading all history with the history slider page\n\nstep 2 -- syncstring \"manager\"\n\n- [x] (1:00?) (1:08) automatic snapshots: somehow ensure there is a snapshot that is at least t steps back from the most recent patch, but also at least t steps from the previous snapshot.  Maybe t = 500?\n\nworry:\n\n- [x] (0:10?) (0:01) when getting a given version, ensure that the best snapshot is used, rather than starting from the beginning (this is just a 10-minute double check)\n\n\n\nWhat we want is that every time n patches are saved since the last snapshot plus k seconds elapses, a new snapshot is made at k seconds ago.  Problem: who makes this new snapshot?  As long as the rules are well defined, and making a snapshot is idempotent, it doesn't matter too much.\n\nThis requires changing the database schema slightly.  This change is **critical** since right now, each time a new user gets added, the full base snapshot gets changefeed'd out to everybody.  Also, we are only saving one snapshot right now, not many of them. \n\n","position":-1.3134439250479772,"last_edited":1453482601579,"task_id":"9d38081d-8304-46ae-9ef5-04678f97be12","done":1453482601169}
{"desc":"#now (1:42) disconnect and reconnect needs to send patches made when offline.","position":-2,"last_edited":1448166337932,"task_id":"df5f6b4d-4a8a-43d1-bc08-aaaee7396c70","done":1448166205961}
{"desc":"#v2 #syncstring (3:30?) #8\naddress possibility that the same account_id records two edits to the same file at the same ms in time, but say from two browser sessions.  \n\nNightmare scenario: classroom of students all logged into same account editing same document\n\nSolution: somehow make it so that when client does a write, if there is already a record with that key (and a different value!!!!!!), then the write fails with appropriate error. The client can then increase the timestamp by 1ms and try again.\n\nPlan:\n\n- [ ] (1:00?) add generic schema option to make tables immutable, in that you can write new things, but you can't change anything already there.  If you do, then it just gives an error.  This could be generally useful in other ways.\n- [ ] (0:15?) make the patches table immutable\n- [ ] (0:45?) make synctable aware of immutable tables: add an event that is emitted whenever an issue involving immutability is encountered.\n- [ ] (1:30?) if hit immutability issue for a given patch with timestamp, add 1ms to time and try again until; make sure to update any relevant data structures in syncstring.\n\n\nOther worse ideas:\n\n- if same user is editing the syncstring in more than one browser, create multiple user numbers for them?\n- assign a slight time skew for each one, i.e., add a few ms of time to each one to make conflict impossible.","position":17,"last_edited":1451605251418,"task_id":"7d7cde76-1cdb-4900-a46c-54fe951f3c6a"}
{"desc":"synctable ack: make it so that each changefeed update waits k seconds for an ack; if it fails, it cancels that changefeed.\n\n#wontfix","position":2,"last_edited":1448215322102,"task_id":"ab953de2-e995-4a13-a379-aee3f94e01e6","done":1448215321680}
{"desc":"#now -- only save patches at most once every k ms, not after every single change.","position":-1.5,"last_edited":1448215211632,"task_id":"607373b4-218e-452c-b555-ee19fe0f7cb2","done":1448215211215}
{"desc":"#v1 #syncstring #offline #rebase (2:30?)\nimplement way to rebase/apply offline changes, if they are old.\n\nWhen user comes back online (in reconnect function) instead of just sending all of our offline changes to the server, delete them from the table, and emit a message saying \"here are some unsaved changes\".  The syncstring could then apply those changes--i.e., rebase-- then make a new patch at the top that has the same effect as those changes.  (The user could be asked if they want to apply or not...)\n\n- [ ] (1:00?) add option to synctable so that instead of just applying offline changes on reconnect, they are removed and a message is emitted.\n- [ ] (1:30?) in syncstring, handle that emitted message; have to reply all changes merging our with whatever new changes have happened.  Avoid weird issues of having stuff disappear from synctable messing up our data structure.  Obtain resulting new string... and save as a new commit.","position":-1.3134439250425203,"last_edited":1453354590061,"task_id":"ae17e5dc-d0de-4294-b372-3b479afbb188"}
{"desc":"#v1 #syncstring #history #0 (4:30?) (1:31)\nhistory browser for files using new sync system\n\nTHE Design:\n\n- Same window instead of a separate tab (have another row of buttons to control, but make generic enough that it will work for everything, including tasks, jupyter, normal editing documents, etc.)\n- Load more history functionality for syncstring -- so can only show whatever is already in memory for editing, then have a button to \"load more\".\n\n\n- [ ] (1:30?) create ui that shows a history control row, basically like the current one, which pushes the editor start down by one row.\n- [ ] (0:45?) add a \"load more\" button when the history isn't fully in memory\n- [ ] (0:45?) make document be read only when history control shown.\n- [ ] (1:00?) display document when slider selects a particular point in history\n- [ ] (1:00?) button to revert document to display state\n","position":-1.312744140625,"last_edited":1451635201798,"task_id":"4c6d285c-e59f-4cc8-8af6-f1a5d71ea074","done":1451635201390}
{"desc":"#0 #local_hub (1:30?) (1:06) #now\nmake it so local_hub watches disk file, and if file changes on disk load from disk, and set string to that.","position":-1.314453125,"last_edited":1450506895847,"task_id":"887d5392-5e2a-486d-bbae-7b221f8fdd87","done":1450506895436}
{"desc":"#0 #local_hub (0:30?)\nimplement: save to disk\n\n- [ ] (0:20?) open syncstring in local_hub, if not already opened\n- [ ] (0:05?) compute current version\n- [ ] (0:05?) write it to file","position":-1.31298828125,"last_edited":1451435598837,"task_id":"a52d149d-58d4-4f14-821d-2223af53958b","done":1451435598420}
{"desc":"#local_hub\nfile change on disk resets live version to what is on disk","position":-1.03125,"last_edited":1448216806619,"task_id":"cebc557f-2029-4e6c-8d17-f36fdbbb9d8e","deleted":true}
{"desc":"#0 #local_hub (3:30?)\nability for local_hub to connect to synctables (no worry about security at all)\n- [x] (1:00?) (2:12) message framework from local_hub to hub\n    - keep map in memory of all connections from hubs to this local_hub, and when they last sent us a message\n    - function that calls local hub, i.e., sends a message.  If hub connected, it will try from most recent to least hub until one works (or fail if none work).\n    - also plugs into thing that receives messages to do callback.\n- [x] (1:30?) (1:42+) new message type from local_hub to hub for doing a query, plus handling\n- [x] (1:00?) project get query\n- [x] (1:00?) (1:30+) hook in synctable to this query functionality\n","position":-1.34375,"last_edited":1450380171740,"task_id":"f614e5cc-6dda-4620-8dd2-7b86819b638d","done":1450380171327}
{"desc":"#v1 #local_hub #2 (2:30?)\nsecurity model -- so project can only access very specific tables\n\n- [ ] message to handle query from local_hub: come up with way to allow queries; or for now only allow querying the syncstring and patches tables (?)\n\nThere are going to be many problems with the tables that might be difficult!  Address them all when things are working.","position":-1.015625,"last_edited":1451766621309,"task_id":"a4a5fc03-bcd1-4a28-a88c-0944f64e082c","done":1451766620900}
{"desc":"#syncstring #0 (2:00?)  (2:19) #now\nreplace existing syncstring with new db based one - so syncdb, chat, tasks, jupyter, etc., all automatically switch to use new syncstring...\n\n- [x] basic switch basic editors to work with new approach\n- [x] basic switch of syncdb-based stuff to work\n- [ ] really do new editors\n- [ ] (1:00?) read the exact api of old diffsync based syncstring\n- [x] (1:00?) write something the same based on new syncstring, then plug in\n- [ ] (3:00?) sage worksheets ! tricky!","position":-1.313446044921875,"last_edited":1451452398893,"task_id":"63639ea3-0267-421e-9626-e4199da18a59","done":1451452398482}
{"desc":"#syncstring (1:30?)\nreplace existing diffsync doc with new db based one - so file editing uses this.\n\n- [ ] (1:00?) rewrite some code in syncdoc.coffee, after replicating the syncstring api","position":-1.31341552734375,"last_edited":1451604953969,"task_id":"3b426f45-72da-4cbd-b125-3252ce3049a4","done":1451604953560}
{"desc":"#0 #v1 #local_hub (1:00?) #now\nopening a file on disk should properly create sync session using the file contents, but take into account timestamp.\n\n- [ ] (0:15?) open syncstring editing session.\n- [ ] (0:45?) if timestamp (??) of file is newer than that of most recent edits, set the syncstring to contents of file\n\ncheck to see if the above makes sense, given how git, etc., works.   Alternative could be that if timestamp of file changes, even to something old, then we always revert to that.  Not sure.  Could be an option.","position":-1.3134439270943403,"last_edited":1451763224008,"task_id":"20195c89-6f54-4d85-be3f-ccbea5c95d06","done":1451763223597}
{"desc":"#v1 #local_hub (1:30?)\ndisble/comment out all existing diffsync code in the local hub, etc., (since replaced by new syncstring code.)","position":16,"last_edited":1456357046612,"task_id":"7bd9b51d-b57e-4129-9d04-f30899e26af4"}
{"desc":"#v2 #9 #hub #limit #unclear\nlimit the document **history size** on the backend to avoid insanity.\n\nIdea: Make it so after n patches (say n=10000), something on the backend automatically truncates to a snapshot.  This could be done via some sort of generic capped collection notion in the schema.\nHowever, to keep it efficient, would have to be careful about only doing the truncation if there are a lot of writes happening in a relatively short amount of time (if there aren't... who cares).","position":10,"last_edited":1451605193691,"task_id":"9b2bc540-4880-40b6-9440-c7d8a1607bba"}
{"desc":"#v1 #1 #hub #limit #unclear (4:00?)\nlimit the editable document **size** on the backend to avoid insanity.\n\n- [x] (1:28) syncstring file -- don't allow opening documents at all over a certain size.  Gracefully fail.\n- [ ] (1:00?) if backend file suddenly changes to be large, do something appropriate.  Obviously we need some sort of error state, where UI editing stops, and user is told \"file is very big -- you must edit using the command line or otherwise\".  Right now you silently lose data on save.\n- [ ] (1:00?) don't allow writing a patch beyond a certain size (already messages beyond a certain size are dropped)\n- [ ] (2:00?) make it so frontend and/or local hub reasonably gracefully fail when get an error about trying to write something that isn't allowed.","position":-1.3134439561981708,"last_edited":1455425684806,"task_id":"f9cab25d-9be6-49c4-a7c3-b2059d115c28"}
{"desc":"#v1 #1 #syncstring #security (1:30?)\nenforce schema for mapping thing that defines a syncstring to a uuid\n\nFor files in a project:\n\n   sha1('file', project_id, 'path/to/file')\n\nThen we can use something else besides \"file\" for other things, e.g., 'chat' for ephemeral chats, etc.\n\nWe then have to enforce exactly this in the schema and backend.\n\n#unclear -- we might not do this at all!  The reason is because of compound primary keys, etc.","position":-1.3134439250717662,"last_edited":1453488452650,"task_id":"cd0becca-f9d6-49b1-b324-76a894728c01"}
{"desc":"(2:00?) #client #0\nimplement cursors\n\n- [x] (0:30?) create another table, like syncstring, that is for cursors for a document\n- [x] (0:30?) (0:30) when editing set the cursor **positions** for a given client, and account_id, and when set.\n- [x] (1:00?) (1:53) everybody sees and renders accordingly\n\n- [x] (0:30?) (0:37) only broadcast cursor movement when user-initiated\n\n- Worry: Join synctable?\n- Cursors: who should report if same account but many client sessions?  probably last to move?\n\n```\nt = smc.client.sync_table({cursors:[{id:'string_id', locs:null, time:null}]})\nt.set({id:['string_id', 'user_id2'], locs:[{x:15,y:100}, {x:25,y:30}], time:new Date()}, 'none', function(e){console.log(e)})\n```\n","position":-1.31396484375,"last_edited":1451196433843,"task_id":"8929411f-daa6-46ff-9385-4ebb3d7e030c","done":1451196433425}
{"desc":"#0 (2:00?) (3:01) #client #syncstring #local_hub #now\nsave to file\n\nmake it so the save-to-file state is part of the syncstring state (in the database) and synchronized/persisted across all users.\n","position":-1.3203125,"last_edited":1450498987909,"task_id":"98614c7c-910c-4760-a655-f77bf51cfe0f","done":1450498987501}
{"desc":"#1 #v1 #file (3:00?)\nfile move/rename\n\nActually properly handle file rename/move.  \n\nNot sure how, but natural would be to abstract away the id of the sync editing session (so has nothing to do with project_id/path).  Then file rename is a quick single change in the syncstrings table (namely, changing which file is being edited).  I like this.","position":9.5,"last_edited":1456356356015,"task_id":"20d61a3d-bc49-4474-a17a-f5c05b5529f9"}
{"desc":"#now (0:07) (0:15?) \n(unrelated to sync) fix the download link","position":-1.2421875,"last_edited":1448374552390,"task_id":"12238a67-7317-4ae6-be80-aa43738d88c3","done":1448374551986}
{"desc":"","position":-1.24609375,"last_edited":1448374602225,"task_id":"4e021924-d9c6-4141-81bf-94d71297b37f","deleted":true}
{"desc":"#local_hub (5:00?)\nlocal_hub refactor\n \n- [x] (2:30?) (0:10+) refactor local_hub.coffee into several files so code is easier to work on.\n    - [x] (0:20) diffsync_file\n    - [x] (0:05) desctructured import functions\n    - [x] (0:20?) (3:00) raw server  -- (way harder than expected since I wanted to also test; but valuable to get another one of those \"2 hour\" projects out of the way!)\n    - [x] (0:20?) (0:19) printing files\n    - [x] (0:20?) (0:55) console sessions\n    - [x] (0:20?) (0:39) optimize console session start (?) -- instead did maybe a slight cleanup.\n    - [x] (1:10?) (1:11) \"CodeMirror\" editing sessions and sage sessions\n    - [x] (0:30?) (0:32) factor out reading/writing file to project\n    - [x] (0:45?) (1:20) #now -- refactor jupyter server (?) -- and get it to work\n    \n- [x] more refactor:\n    - [x] (0:20?) (0:03+) blob handling -- do this later after have better way to send messages to hub\n\n- [x] (0:54+) delete lots of unused code, e.g., sage session.\n\n- [x] fix that sending blobs is flakie/broken in dev mode\n    ","position":-1.248046875,"last_edited":1448482559779,"task_id":"fce35c3a-8ebe-4fbd-a3ce-a9ac3d7a0b13","done":1448482559361}
{"desc":"#0 (1:00?) (0:26) #now merge in master","position":-1.2490234375,"last_edited":1450237972215,"task_id":"2caef818-2d72-40ef-9e8f-44ad3563854e","done":1450237971808}
{"desc":"#0 (1:30?) (1:05) #now\ninvestigate compressing the file snapshots.\n\nOptions..?\n\n- lz-string: http://pieroxy.net/blog/pages/lz-string/guide.html So in my testing in browser, about 1MB of coffeescript (not repeating – several different files) compressed to about 70k (so 7% of original size); and this takes about 250ms.; Decompressing takes about 30ms..    A 4MB file would lock the browser for 1s.  Not acceptable.\n\nWe could: (1) use web workers, or (2) use the local_hub, or (3) break up the strings to make lz-string more async.\n\n#rejected\n\nFrom the point of view of the browser in memory, there is no advantage to using compression at all – only a disadvantage, since it takes cpu cycles, and more memory to hold both the compressed and uncompressed version of things for some amount of time.\n\n4 minutes ago\nW\nWebsockets should/will/already do (?) support much better compression natively.\n\n4 minutes ago\nW\nIn any case, whether or not they do it now, they will at some point, and that is the place to deal with size. So doing anything related to compression in the browser isn’t really beneficial to the browser.\n\n1 minute ago\nW\nSo the only real motivation is reducing the database size.\n\n1 minute ago\nW\nHowever, I can just used a compressed filesystem, so that what rethinkdb stores is compressed.\n\nless than a minute ago\nW\nI definitely want to switch to running rethinkdb on zfs + compression, so I get snapshots anyways. ok, do don’t compress.","position":-1.375,"last_edited":1450306508204,"task_id":"add4ce43-42b0-4a1a-9969-3a9974b9d2af","done":1450306507790}
{"desc":"#0 #local_hub (1:00?) (1:47) #now\nget syncstring to work from local_hub","position":-1.328125,"last_edited":1450486278128,"task_id":"7cd3ec92-325e-45e4-a99e-41e6161fa79b","done":1450486277721}
{"desc":"#v2 #9 (0:45?)\nmake set queries on synctable verify that the keys being set are allowed by the original query.\n\nthis is mainly to avoid user/author confusion (me)","position":12,"last_edited":1451605179497,"task_id":"465f4480-598f-43bf-82ca-cd9a7baae726"}
{"desc":"#v1 #0 #now (0:45?) (1:20)\nmake the save ui button properly reflect state of save and hash\n\n(so is perfectly well defined across clients, etc.)\n\n- [x] (1:14) codemirror document editor\n","position":-1.3134441375732422,"last_edited":1451626549906,"task_id":"f74d1a70-2f0b-4d86-85b3-4f1cc1b8ea2a","done":1451626549495}
{"desc":"#0 #local_hub (2:00?) (1:47)\nopen a file for editing\n\ngiven a filename and project_id, function that opens that file for sync editing\n\n- [x] create a new table in the schema that defines the state of the local_hub, e.g., which files are open, by whom, etc.\n- [x] opening a file would also update that state thing\n- [x] this would cause the local hub to open the file for editing in memory\n- [x] set project_id and path when opening a file first time\n\nIt's very unclear what the paramters/semantics/behavior of this should all be:\n- what if file changes on disk and not open in local_hub?","position":-1.3134765625,"last_edited":1451443001013,"task_id":"73b8dda0-e1c0-425e-928f-cc9ed91b8175","done":1451443000588}
{"desc":"#9 #v2 (5:00?)\ntiered storage\n\n> Even text editing/history could be tiered though eventually, where all the data about editing a file gets moved from rethinkdb to GCS after a certain amount of time, then moved back when the user re-requests info. There would be a 3s pause (say) while this happens, but I think it could be made otherwise pretty transparent. So we go from lots of non-compressed patches/snapshots of files in the database to dumping them all to a single compressed JSON archive, uploading to GCS, etc.","position":14,"last_edited":1456356460298,"task_id":"6bbb023d-077f-4359-b6df-e398e5fba747"}
{"desc":"#v2 #9 #cursors (1:15?)\nmake a way for schema to ensure something about structure of cursors table?\n\nimagine invalid data goes in which makes the cursor parsing code get messed up...","position":15,"last_edited":1451605145528,"task_id":"e2568b9c-1481-47a1-b425-c58d274aeb05"}
{"desc":"#v2 #9 #compression (3:00?)\nre-investigate compression ideas:\n\nI realized something about compression when I was \"mulling things over\" during the break.   If patches/snapshots can work either compressed or not compressed, then we could have a backend process -- running on some dedicated machine -- that just goes along and compresses anything not compressed.  Since compression takes 10x longer than decompression, this might still be well worth doing.  Decompression in browser is still very fast.  \n\nAlso: are the patches stored as JSON strings or as a complicated javascript object?  This is relevant to compression questions...","position":14.5,"last_edited":1456357019172,"task_id":"4d8dedd9-d3af-4efb-a2ee-463197e10d3d","done":1456357018761}
{"desc":"#v2 #3 #cursor (1:00?)\ncursor fade out still doesn't work well... :-(","position":-1.313232421875,"last_edited":1451766576210,"task_id":"f46c7ca2-d096-4fc2-9ed4-e1af76ba354d"}
{"desc":"#v1 #0 #error (1:30?) (1:03+) #now\nsyncdoc.coffee doesn't handle errors opening a file sufficiently well.\n\nIf you try to open a file from a (1) project you can't use or (2) path you can't access...something needs to indicated this quickly in an error message.\n\nI think somehow directories and things we shouldn't watch are getting watched.  \nDirectories add files.  It's bad.\n\nActually, i think with the changes I just made, it is good enough.","position":-1.3134439250698051,"last_edited":1455494579408,"task_id":"1eecd2b8-3794-4230-8b69-7e638aef14c3","done":1455494578999}
{"desc":"#v2 #2 (0:45?) \nget rid of SynchronizedDocument2 and SynchronziedDocument","position":18,"last_edited":1451605428225,"task_id":"07c5df4c-319d-4dbf-93e5-9951bab183fc"}
{"desc":"#0 #v1  #sagews  (3:00?)\nsage worksheets ! tricky!\n\n- [x] (0:15?) (0:06) initial rendering\n- [x] (2:00?) (4:25) code evaluation:  without using messages? -- is it possible? \nYES!  It takes about 100ms without even trying to write/read/write/read from database back and forth from my house to google.  So a reactive approach to evaluation of Sage worksheet code is viable.  How shall we structure it? [[WOW, that was hard.]]\n- [x] (1:00?) (1:15) actual code evaluation in worksheets\n- [x] (0:30?) (0:02) interacts\n- [x] (0:30?) (2:42) introspection\n- [x] (0:30?) (0:16) making blobs permanent\n- [x] (0:45?) (1:11) evaluation continuation when frontend stops working\n- [x] (1:00?) (0:26) ability to interrupt and restart\n- [ ] (0:15?) whenever session starts, have the backend stop anything listed as running.\n- [ ] BONUS: provide precise information to ALL about exactly what is running now and why evaluation of something new isn't happening.","position":-1.313443925086176,"last_edited":1451865794999,"task_id":"fbe46aac-7445-4051-b9a8-f45affed3b62","done":1451865794581}
{"desc":"#v1 #1 #now (1:30?) (0:11)\nrendering/sync of side chat is a mess; seems to get all corrupted on re-connect, etc.\n\n- maybe just redo using react\n\n- doesn't seem any worse than before...; may have been really messed up due to patch sort order issue with dates, which I fixed.","position":-1.3134440183639526,"last_edited":1451696992481,"task_id":"91802ede-525b-4306-8e39-d01a010237ec","done":1451696992073}
{"desc":"#v1 #0 (1:00?) (2:20) #now\ninitial version of document \n\n- [x] (0:30?) (1:05) use version on disk: if file exists on disk and last mod time is > last database patch, set value to what is on disk.\n\n- [x] (0:30?) (1:14) use template -- if when local hub opens the syncstring it finds that the file doesn't exist on disk *and* the syncstring is empty.","position":-1.3131103515625,"last_edited":1451621367205,"task_id":"1464eaa9-19c8-4859-8ba8-c86fec15866c","done":1451621366796}
{"desc":"#v1 #2 (3:00?)\ndo the release and make this new thing live.","position":9.75,"last_edited":1451866179865,"task_id":"7b02f4f8-e5d9-4833-815a-3fa6af53de12"}
{"desc":"#1 (0:45?) (1:18) #bug #now\nensure latest file changes on disk are reflected in memory\n\nbecause of debouncing or whatever, the file on disk can be slightly newer than what is in database via sync.  E.g., open local_hub.log...","position":-1.31304931640625,"last_edited":1451695830762,"task_id":"68aeba43-363a-4d49-8b54-28f2e763e082","done":1451695830352}
{"desc":"#0 #v1 #now (1:00?) (1:45)\nopening a file that already exists on disk for the first time\n\n- [x] should not show file as blank for a second -- instead show \"loading...\"\n\n- [x] ensure that the undo history doesn't start with \"empty document\", but with what got loaded\n\n- [x] opening that file for the first time is also SLOW.","position":-1.3134439587593079,"last_edited":1451713098749,"task_id":"9a52c6cf-4d53-4d26-bfe5-6911a38d96d9","done":1451713098341}
{"desc":"#0 #v1 #now\nfirst tim","position":-1.3130950927734375,"last_edited":1451616454575,"task_id":"386de53e-c466-4294-bdc8-a67432e7a259","deleted":true}
{"desc":"#v1 #0 (0:45?) (0:29) #now\nmaintain cursor position when opening/closing editor\n\n- could just rewrite them in react... :-(","position":-1.3131027221679688,"last_edited":1451628825683,"task_id":"d31962d9-e205-4b7e-ac33-0a9bbd8a6e01","done":1451628825270}
{"desc":"#0 #v1 #latex (1:30?) (0:17) \nlatex editor isn't doing the initial build for some reason\n\n- could also changes to execute commands via db state, rather than messages, exactly like will have to do for sage worksheets.  (save this for the react rewrite)","position":-1.3134439250725336,"last_edited":1451922569300,"task_id":"2e3c1e19-9957-4781-844a-fc26a45ab3cc","done":1451922568880}
{"desc":"#2 (1:30?)\npotential jupyter issues\n\n- [ ] make sure save/load works properly; don't have weird issues with files/timestamps, saving.","position":-1.313443660736084,"last_edited":1451626337567,"task_id":"887c92d3-7dd2-47ea-bb35-74d42b5e781b"}
{"desc":"#v1 #0 (1:30?) #now\nSynchronizedString improvements\n\n- [ ] (0:45?) (0:20+) #now make the save ui button properly reflect state of save and hash for tasks (and anything based on SynchronizedString) -- I'm goint to wait for the react rewrite to do this...\n\ndocs already done completely and nicely.\n\nthe test for this is edit a task, save, make a change, undo it, and see the save grey out.\n\n- [x] (0:45?) (0:12) like for SynchronizedDocument, ensure for a while that SynchronizedString save definitely saves before returning -- retry until success\n","position":-1.3134439253481105,"last_edited":1451766449672,"task_id":"c818fa66-1681-48af-a24e-5c6642424b2e","done":1451766449262}
{"desc":"#0 #v1 (0:30?) (0:12) #now\non first time to ever open file, the backend should set the save hash\n\nTest -- open new file and see the save button stay disabled.","position":-1.3134439382702112,"last_edited":1451755542608,"task_id":"e6419294-cb49-4db6-a629-e7ebafbc9ce0","done":1451755542199}
{"desc":"#0 #v1 (1:00?) #now (1:35)\nproperly handle read only files","position":-1.3134439326822758,"last_edited":1451761126692,"task_id":"1f19faf7-ac51-429c-82f5-9309d2b90dd3","done":1451761126282}
{"desc":"#0 #v1 (0:45?) (0:11)\nsagews history\n\nwow, it just works perfectly already, way *better* than live version!","position":-1.3134439256973565,"last_edited":1451763845092,"task_id":"5acba352-b1ae-4a9b-8ced-0788ec55f8da","done":1451763844676}
{"desc":"#1 #v1 #jupyter  (4:00?)\nrewrite jupyter sync to work with new sync, and be generally better.\n\n- [x] (0:30) get it to sort of work\n- [x] (2:00?) (1:45) rewrite jupyter to work using new syncstrings.\n- [x] (1:30?) (1:00+) history\n- [x] (1:00?) (1:10) cursors\n- [ ] (0:45?) address this todo: \"# TODO: If this cell was focused and our cursors were in this cell, we put them back:\"\n\n- [ ] loading jupyter in the iframe sometimes hangs browser now or gets in crazy feedback loop.  BAD.","position":-1.3134439250711694,"last_edited":1453696184976,"task_id":"d04c4061-20e6-4e05-852e-ab8c5872d181"}
{"desc":"#2 #v2 (1:30?)\ntasks and courses history slider","position":-1.3126907348632812,"last_edited":1451635177084,"task_id":"c7e7e008-999c-4fcf-bae3-6a8f106f3645"}
{"desc":"#2 #v2 (0:30?) #react\nuse timeago also for the main time description, and actual time in parens","position":-1.3127059936523438,"last_edited":1451672059923,"task_id":"9a6b3b07-2815-4ec5-a49b-88fc132783a3"}
{"desc":"#0 #v1 (0:45?) (0:45)\noptimize history\n\n- [x] (0:45) debounce slider\n- [x] make sure it is fast enough to go from one point to another","position":-1.3127021789550781,"last_edited":1451680760685,"task_id":"97975f59-cd9b-4b45-951e-8976273c96a9","done":1451680760275}
{"desc":"#2 #v2 (2:00?)\nway to link history to file somehow... (maybe import/export...?)","position":-1.3127002716064453,"last_edited":1451679319945,"task_id":"75904b9a-2330-48ef-875b-96be4ba6a99d"}
{"desc":"#2 #v2 (0:45?)\nadd blue button linking back from history to original file","position":-1.3127012252807617,"last_edited":1451679621101,"task_id":"b312e4a1-9f84-4425-ae66-82fe669b3471"}
{"desc":"#1 (1:00?) #bug\nwhen testing/developing locally on my laptop and the server time gets all messed up,\nedits get discarded.\n\n- maybe they appear to be slightly in the future?","position":-1.313018798828125,"last_edited":1451682203615,"task_id":"985c9e35-99af-4a94-9fb6-21d31ad80159"}
{"desc":"#2 #v1 (1:00?)\ncan we do anything sensible when the underlying file being edited is moved...?","position":9.875,"last_edited":1451713483464,"task_id":"1476e868-7e9b-43a3-866b-6d8adbc06c7d","deleted":true}
{"desc":"#1 #asap (0:30?)\nrebuild rethinkdb from source for compute nodes -- the 2.2.2 release is SHIT.\n","position":-1.3134439438581467,"last_edited":1453656610043,"task_id":"e22caa17-38e7-41f4-9ae9-1eb854903956","done":1453656609633}
{"desc":"#1 (0:45?) #v1\nany time load the file from disk, also set the \"hash of saved version\" to that.\n\nMaybe change \"saved version\" to \"disk version\" in function naming.\n\nEffect: newly opened file won't have save ready to go.  Also, when git checking out, etc., will do right thing.","position":-1.3134439513087273,"last_edited":1456356410438,"task_id":"9b5d9fa2-41de-4756-9f12-58db04de3c9c"}
{"desc":"","position":-1.3134437911212444,"last_edited":1451763281235,"task_id":"424f4d76-4230-4f20-9871-7b69952f30d4","deleted":true}
{"desc":"#v2 #3 (1:00?) (0:20+)  #react\nmake the save ui button properly reflect state of save and hash \n\n- for tasks (and anything based on SynchronizedString) -- I'm goint to wait for the react rewrite to do this...","position":-1.3134439251734875,"last_edited":1451766441743,"task_id":"14d46970-33df-471d-915f-01c8e3a80b33"}
{"desc":"#2 (0:30?) #v2 #history\nwhen thing changing the file is project -- rather than a user, put that in history properly.","position":-1.313443925075262,"last_edited":1451866014564,"task_id":"ae7e018c-c50f-4a2f-9551-406ccd0864f3"}
{"desc":"(0:05) #0 #bug #history #v1 (0:30?)\nclosing the history does *NOT* actually work -- still get slider update messages, etc.\n\nfalse alarm; I can't replicate this.","position":-1.313443925080719,"last_edited":1453414407382,"task_id":"a6b39352-b97f-41b0-9acc-2f3b20d25bea","done":1453414406967}
{"desc":"#2 #question (0:20?)\nshould \"strip trailing whitespace\" be a menu option, rather than something that happens on save?  It's possibly very evil...","position":-1.3134439250834475,"last_edited":1451787557384,"task_id":"fdb7891a-3c11-49f4-a891-002d9cbf3b24"}
{"desc":"#1 #v1 #bug #history (0:20?)\nmaybe the history is missing the very last step (?)","position":-1.3134439250848118,"last_edited":1451865954583,"task_id":"7ae24149-6cb0-4a65-b6b0-7bf36b33158a","done":1451865954174}
{"desc":"#1 #v1 #bug\nsynchronized editing of sage worksheets leads to all kinds of corruption.\n\nmaybe soemthing  wrong with setvaluenojump...?\n\nalso had corruption with history, so temporarily disabled setvaluenojump","position":-1.313443925085494,"last_edited":1451806476270,"task_id":"84ecfe4e-55cf-4dec-ac0f-0e217dc454cf","deleted":true}
{"desc":"#v1 (1:00?)\nmake db set/get queries explicit and make synctable use them\n\nit's just too confusing otherwise.","position":-1.313443925085835,"last_edited":1451865926801,"task_id":"b79944b3-2611-4c5a-a2c1-354a075f0495","done":1451865926393}
{"desc":"#0 #v1 (0:15?) (0:04) #history\ndon't show name if not known","position":-1.3134439250704872,"last_edited":1451922828611,"task_id":"0123f2e1-77fb-4c3d-907f-bcf05834c8f3","done":1451922828202}
{"desc":"#0 #v1 (0:30?) (0:02) #now\nremove \"revision tracking\" option from account settings; it is no longer optional.","position":-1.3134439250718515,"last_edited":1452024748166,"task_id":"f29c5697-3c07-4095-8dd1-511d5e3514d7","done":1452024747756}
{"desc":"#1 #v1 (1:00?)\nmake code evaluation tables and sync editing use soft durability (?)\n\nDaniel explains exactly what to do here: https://mail.google.com/mail/u/1/?zx=v6crnjw8ww15#inbox/152fc888a7429712\n\n> Like Postgres, RethinkDB defaults will not acknowledge writes until they’re fsynced to disk. Users may obtain better performance at the cost of crash safety by relaxing the table’s or request’s durability setting from hard to soft. As with all databases, the filesystem, operating system, device drivers, and hardware must cooperate for fsync to provide crash safety. In this analysis, we use hard durability and do not explore crash safety.\n\nhttps://aphyr.com/posts/329-jepsen-rethinkdb-2-1-5\n\n(Also, maybe I should make the reads even more rigorous for project storage related stuff, e.g., project state, host, etc.)","position":-1.3134439250709988,"last_edited":1456356496333,"task_id":"40652cb5-b54f-40ec-817b-c993f444b1e6"}
{"desc":"#1 #v1 (1:00?)\noutput computation limits; need to limit how many output messages can be written to the database per unit time!","position":-1.313443925071084,"last_edited":1455258260784,"task_id":"702135c8-f220-4d89-bfdd-7c4aabf27c21"}
{"desc":"#1 #v1 #sagews (1:30?) (1:25+)\nre-implement raw_input  (and another related thing).\n\nhttps://github.com/sagemathinc/smc/issues/358\n\nAlso: https://github.com/sagemathinc/smc/issues/78 -- input not implemented\n\n- I realize now that the way to do this is to add sage_session.coffee's call the relevant code, then use the evaluator table, but write a different event. \n\nTOOD: still -- maybe due to delete_last_output not working right now.\n\n- [ ] additional output in the *same* cell (did this ever work)?\n- [ ] normal input\n","position":-1.3134439550340176,"last_edited":1455494614875,"task_id":"7a2628f4-0bce-406a-9849-7359c9d4f7c9"}
{"desc":"#now #0 #v1 (0:20?) (0:07) #sagews\ndoesn't cause a sync when toggling input/output of a cell.","position":-1.3134439250452488,"last_edited":1453415500043,"task_id":"fbfef191-3c76-4d9f-816e-2048e4002527","done":1453415499634}
{"desc":"#0 #now (1:00?) (0:35) #v1\nrefactor syncstring.coffee code some","position":-1.313443925046613,"last_edited":1453420481870,"task_id":"59c523de-1924-43c2-947b-cde85aa3439c","done":1453420481463}
{"desc":"#1 (1:15?) #v1\npatches are not determined only by time, but by time and user.  \n\nHowever, it seems that in the add: method in patch_list, it's assumed they are determined by time? check on this.\n\n- [ ] **best idea**: any time saving a patch, later check that it succeeded, and if not perturb the time and try again.  If this fails repeatedly, need to do some offline thing.  This seems like a good idea anyways.\n\n- Maybe terrible idea: make it so the timestamp of any patch that a user commits is got by rounding down to the nearest 1/10th of a second, then adding the user index.   Then if there are up to 100 simultaneous users who edit the doc, we are always guaranteed that the timestamp determines the user.  Changing the timestamp by 1/10th of a second shouldn't impact the behavior in a noticeable way.\n\n- [ ] in versions and all_versions and PatchList versions, we would get duplicates timestamps if there are two patches with the same timestamp and different users.\n\n","position":-1.3134439250472951,"last_edited":1456356353672,"task_id":"24f0fae2-dfca-4842-b606-9c7abdf003ec"}
{"desc":"#1 (0:45?) #v2\noptimization -- when making a snapshot, we could throw away all in-memory edit history before the snapshot.  \n\nNot sure.","position":-1.313443925046954,"last_edited":1453482707765,"task_id":"8c54b8ec-d4ab-47b6-9dc4-f9da05af8346"}
{"desc":"#v2\nability to load some but not all history\n\nBasically a function like load_full_history, but which goes back just one snapshot in time.\nMaybe expand_history().\n\nAlso, function to tell whether there is any more history available.","position":-1.3134439250476362,"last_edited":1453470299577,"task_id":"defe03b0-f5b7-493a-b62d-a330931b7264"}
{"desc":"(1:00?) #v2\nrobustness idea\n\n- [ ] robustness: if load using last_snapshot, and for some reason no snapshots in the list of patches we got, then load everything starting from the beginning.\n\nI don't know if this is ever needed, but it couldn't hurt to implement...","position":-1.3134439250478067,"last_edited":1453482331241,"task_id":"517db47b-fa4c-49d7-950a-524199f436b2"}
{"desc":"#v2 (2:00?) \n**consider** compressing snapshots (and/or patches) via a manager.\n\nI did this, implemented it, and it adds subtle bugs, issues, problems, for possibly little gain.  It's better to compress the filesystem / network. \n\nREJECTED.","position":-1.313443925047892,"last_edited":1456356947997,"task_id":"26b69eb7-eb8c-4c45-9c1d-790572098e7c","done":1456356947582}
{"desc":"#v1 #0 #now (0:30?)\nfix some bugs/fallout from new snapshot code.","position":-1.3134438580818824,"last_edited":1453488364190,"task_id":"48f7cb23-834e-458a-832d-869fca4b5960","done":1453488363781}
{"desc":"#1 #v1 #jupyter (2:00?)\nsync jupyter is still not working right\n\n- [ ] output now keeps getting removed, possible due to some lineheight css hack race condition (not sure).   Anyway, very broken.\n\n- [x] still get weird infinite loop sometimes when opening jupyter history.","position":-1.3134439250711054,"last_edited":1456356875832,"task_id":"32417b5a-2647-4d5e-986e-da077eb40366"}
{"desc":"#v2\nif possible, history should support \n\n- [ ] task list\n- [ ] course editor\n- [ ] chatroom","position":-1.313443925071148,"last_edited":1453488484387,"task_id":"515b45f3-56bd-473a-a327-3e8b525547d3"}
{"desc":"#v1 #1 #jupyter (2:00?) (0:32+) \njupyter blobs!\n","position":-1.3134439250711587,"last_edited":1455158262185,"task_id":"04488a42-1b67-4baa-a152-601ce3428a9b"}
{"desc":"#1 #v1 #jupyter (0:10?)\nhistory -- open file needs to have a special case for jupyter notebooks.","position":-1.3134439250711534,"last_edited":1453696196681,"task_id":"4b7173b3-c86f-44e1-ab95-ba691a073444","done":0}
{"desc":"#1 #v1 #jupyter (0:30?)\ncommand+s (or control+s) doesn't work to do our save...\n\ncareful of firefox!","position":-1.313443925071156,"last_edited":1454636425694,"task_id":"c1364ce8-33aa-4da9-9a0a-0c0f623fc0ac"}
{"desc":"#v1 #jupyter  (1:30+)\nwhen opening file `Checking whether ipynb file has changed...` don't look at the file on disk, but instead at the last change in memory (from db)\n\nActually, need to do something more interesting.\n\nIdea: Have the backend, when opening a .ipynb file, watch the .ipynb file, but immediately convert it to our sync format.   If the ipynb file changes, it re-reads it and converts to a syncstring.  Save will use this, which avoids any weird race conditions, etc.\n\nBackend conversion to syncstring will take care of removing and saving all the blobs to hub.  (So need a generic client.save_blob() on both front and backend.)\n\nThis is a basic step in going from having the on-disk format and the sync format be different when editing a document.  This might also be useful for latex...\n\n- [ ] factor out the jupyter <--> syncstring code so that it can run in both client and project (and be easier to test).\n- [ ] make project \"aware\" of ipynb files","position":-1.3134439250711574,"last_edited":1454638108162,"task_id":"a4813b45-e6aa-487b-8403-7aace5a8eb0c"}
{"desc":"#0 (1:00?) (1:35) #now\nhistory for task lists\n\n- [x] (0:10?) #now add a history button\n- [x] (0:10?) try opening one and see what happens.\n- [x] (0:25?) add case to create editor\n- [x] (0:25?) add code to set value.","position":-1.3134439250711374,"last_edited":1453662665611,"task_id":"1723fe76-e5d6-4d93-8dce-43c5098146a1","done":1453662665196}
{"desc":"#1 #v2 (0:45?)\nmake the hashtags in task list history work","position":-1.313443925071132,"last_edited":1453662663238,"task_id":"2dc813b7-0d1b-49f4-8e4a-1b01c3bcc371"}
{"desc":"#1 #v1 #sagews (0:40?)\ngreen \"running\" bar flickers like crazy whenever running anything.  \n\nNo idea why.\n\nhsy: that's fixed, the new loading banner caused a css animation name collission","position":-1.3134439250717875,"last_edited":1453731735687,"task_id":"295ec2f3-46ed-4e19-9514-d05c9b8753f6","done":1453731735273}
{"desc":"#v2 (0:30+) I tried opening a file in a path that involves a symlink using the “open” terminal command.\n\n(Namely the file sagews.coffee in the dev project).\n\nThings went to hell – the file wouldn’t open. Opening the same file from the UI works.\n\n> this is actually a general problem having nothing to do with new sync. It’s a subtle problem of using SMC inside SMC and how path’s work. Delay until later. Orthogonal to sync.","position":-1.3134439250717982,"last_edited":1454588983632,"task_id":"01a00452-41d8-4869-8059-54a972fcf131"}
{"desc":"#v2 (0:45?)\neliminte use of smc-project/file_session_manager.coffee \nand also all the codemirror messages from message.coffee","position":-1.3134439250718035,"last_edited":1453699048780,"task_id":"23d86235-0888-49c3-9977-795386f5a19a"}
{"desc":"#2\ninsane processes going crazy in project dev, at least with *new* refactor -- they are console servers\n\n     7580 5811740+  20   0 1490000 641884  15884 R  78.0  2.4   0:57.39 /usr/bin/nodejs /usr/lib/node_modules/forever/bin/monitor /projects/45f4aab5-7698-4ac8-9f63-9fd307401ad7/smc/src/smc-project/console_server.coffee               \n    26349 5811740+  20   0 5832692 4.754g  15908 R  68.7 18.6  11:51.67 /usr/bin/nodejs /usr/lib/node_modules/forever/bin/monitor /projects/45f4aab5-7698-4ac8-9f63-9fd307401ad7/smc/src/smc-project/console_server.coffee               \n","position":-1.3134439250718062,"last_edited":1453702764357,"task_id":"6527b75f-71c0-420a-9b1b-ca0dc248fe3a"}
{"desc":"#0 #v1 #sagews (0:45?) (2:12)\nre-implement delete_last_output\n\nThis is maybe needed for raw_input.\n\nIt's actually missing/not implemented even in live SMC.\n\nEnded up taking a long time and cleaning up code, and going down the rabit hole and learning more... ","position":-1.3134439531713724,"last_edited":1453739389968,"task_id":"1e97b64a-ddf1-4cfa-8227-3173ed56c631","done":1453739389549}
{"desc":"#2 #v1 #sagews (0:45?)\nDeal with this in `syncstring_evaluator.coffee`\n\n\t# TODO: need to perturb time if same as last eval","position":-1.3134439522400498,"last_edited":1453735196103,"task_id":"5da6f2aa-488a-4975-95a8-f3be78b54604"}
{"desc":"#0 #v1 (0:30?)\nsyncstring save is annoying!  it often doesn't save until requesting saving again.\n\na very long time to delete four lines...","position":-1.3134439568966627,"last_edited":1454648882453,"task_id":"5082da21-f539-4c52-ae30-47dc71487b51","done":1454648882041}
{"desc":"#v2\ntasks save should be green only if there are definitely unsaved changes.\n\n(this could be part of react tasks rewrite)","position":-1.3134439559653401,"last_edited":1454638138214,"task_id":"4c0196bc-0da6-4fa4-99d7-05831516215a"}
{"desc":"#v1 #0 (1:00?) (2:30) #now\nmake it so editing a file triggers file_use notifications\n\nOne way: do on the backend\n\n- [ ] remove the scan_local_hub_message_for_activity function (and anything only for it) from hub\n- [ ] change schema so writing to the patches table looks up (and caches) which file this is for, and -- if not too recent -- updates file use table.\n\nOther way: do entirely on the client\n\n- [ ] add an action in the file_use Actions to record activity; make it \"debounce\" non-chat activity by 1 min; less debounce for chat\n- [ ] have syncstring call it when saving patches.\n- [ ] delete all backend code related to this\n\nPros and cons: \n - backend is harder to update and develop\n - backend will work no matter what bad happens with frontend.\n - user cheating -- this isn't a video game.\n - doing it on client means clients can easily implement a \"stealth\" mode if they want.  That's good.\n \nSubtle issue:\n\n - the idle timeout functionality needs to be aware of file editing.  Right now there are *NO* messages to local hub, so project just doesn't get touched.","position":-1.3134439564310014,"last_edited":1454913008198,"task_id":"f43d9080-4285-486a-b388-ef43e1741837","done":1454913007759}
{"desc":"#0 #v1 #file_use #now (0:30) (1:15?)\n\n- [x] there is something wrong with the wstein@sagemath.com project in that it doesn't show all recent notifications initially on load.  Why?  Something wrong with sort or...?\n\nE.g., the file xxxxx.","position":-1.3134439564091736,"last_edited":1455229641634,"task_id":"457ef005-9d81-4a49-8374-347fe609c522","done":1455229641220}
{"desc":"#1 #v1 #file_use #now (0:24) (0:45?)\n\n- [x] fix so that just marking the file_use notifications read, which happens on clicking the bell, no longer results in the file presence showing the face as if the recipient actually looked at the chat/file.","position":-1.3134439562563784,"last_edited":1455472916613,"task_id":"a99993fc-b716-4986-b779-7f12ca6aaf0e","done":1455472916205}
{"desc":"#1 #jupyter #file_use #v1\nnot implemented at all (that weird transform)\n\nif we just move the processing to the backend, it won't matter.","position":-1.313443956314586,"last_edited":1456356384454,"task_id":"d1ef3607-24f9-4561-85ef-c6a6d3d63d8d"}
{"desc":"#2 #file_use #v2\ndelete the backend hub code related to differential sync.","position":-1.3134439563727938,"last_edited":1456356553288,"task_id":"8aefbb99-5cd4-47af-8f4d-e96739c4eeb6"}
{"desc":"#0 #v1 #now (1:00?) (1:30)\nidle timeout of project when user editing\n\nWe need to somehow touch project.  Current functionality in hub doesn't work since local hub no longer does the file editing org directory.\n\nSo... the idle timeout functionality needs to be aware of file editing.  Right now there are *NO* editing messages to local hub, so project just doesn't get touched.\n\nSimilarly, need to automatically start up project whenever user does something with a file that... requires project to be running.  ","position":-1.3134439564018976,"last_edited":1455240521730,"task_id":"4e0cc188-c13f-490d-a7e9-92d26c277836","done":1455240521319}
{"desc":"#0 (1:00?) (0:38) #v1 #now\nmake it so **opening/foregrounding** a file explicitly (not by a reconnect, etc.) triggers file_use notification properly","position":-1.3134439564164495,"last_edited":1455225966372,"task_id":"ba1caf33-b8b1-413a-8fb7-ba77407d4927","done":1455225965962}
{"desc":"#0 #now(2:00?) (1:25)\nchange to storing patches in db as strings; include offline compression \n\n- [ ] (0:45?) #now change all code so patches are strings rather than lists\n- [ ] (0:15?) in schema have way to indicate if compressed or not (with index)\n- [ ] (0:30?) write function that goes through and compresses (and decompresses) using lz-string (http://pieroxy.net/blog/pages/lz-string/guide.html)\n\n","position":-1.3134439250711576,"last_edited":1455169083199,"task_id":"1030175a-56a5-46d8-9a91-1185d105935f","done":1455169082785}
{"desc":"#v1 #0 #sync #speed #now (0:07)\n- [ ] (0:30?) the code that adds patches to list and JSON'ifies ends up going back and forth every time, which seems wasteful.","position":-1.3134439250711583,"last_edited":1455499673348,"task_id":"7e84b008-6102-4af1-89b7-20594eeb2441","done":1455499672939}
{"desc":"#0 #v1 (0:45?) (0:45)\nwhen compressing patches, compress both the patch *and* the snapshot.\n\nAlso, when decompressing, decompress both.","position":-1.3134439250711585,"last_edited":1455220502858,"task_id":"aa0f7413-02ad-4ad6-801f-326dd1ac4654","done":1455220502449}
{"desc":"#0 (0:30?) (0:21) \nmake it only *try* to query db after it has signed in (rather than before!); this resolves a bunch of issues.","position":-1.313443925071158,"last_edited":1455222350977,"task_id":"cb4fe164-19bc-4fed-93b6-f5e7bcb934b3","done":1455222350568}
{"desc":"#3 #v2\n- synctable query errors: retry a few times with exponential backoff","position":-1.3134439250711578,"last_edited":1455222338414,"task_id":"f91c0780-2067-405f-9ed3-1b19c51070d8"}
{"desc":"#0 (0:20?)\ncursors -- don't use underscore.debounce -- instead use underscore.throttle.","position":-1.3134439563873457,"last_edited":1455232158929,"task_id":"8356f20d-e99a-4ba3-a441-20f5dc6b4415","deleted":true}
{"desc":"#0 #v1 #sage\nadd more hooks to start project\n\n- [x] (0:42) making touching a file in a project start that project running (so can check if file on disk changed, or open it in case it hasn't been opened yet, etc.)\n\n- [ ] start sage server running on sage worksheet evaluation.\n\nThis is similar to the hook on synctable save change, which causes the project to start, and get a connection.","position":-1.3134439563946216,"last_edited":1455245942996,"task_id":"171ae0fe-bc79-4e02-a5aa-fe8fcae13f59","done":1455245942580}
{"desc":"#0 #v1 (1:13)\nIt's time -- come up with a way to **guarantee** that the synctables all stay in sync with the database.\n\n- [ ] do not ignore any errors.\n\nactually didn't do this -- but found issues with sagews and fixed those instead.","position":-1.3134439563909837,"last_edited":1455258251966,"task_id":"9d7b8556-42d6-4c32-815c-955c62d34a26","done":1455258251555}
{"desc":"#1 #v1 (0:45?) (0:55) #now\nwhen opening a document, if there is an error, then it sets codemirror to that error\nand makes it read-write. Pretty idiotic.","position":-1.3134439561399631,"last_edited":1455430068998,"task_id":"8cccbcde-440f-43a1-abc1-29f6a5203c72","done":1455430068589}
{"desc":"(0:10?) (0:10) #1 #v1 #now\ncomment out code related to compression -- fact is, it is more trouble than it is worth, probably buggy, doesn't seem worth it, and uses too much client cpu.  I have a bad feeling.","position":-1.313443956169067,"last_edited":1455306231193,"task_id":"e98853db-b731-415e-9e1b-5ce4f6183e8f","done":1455306230783}
{"desc":"#0 (0:05)\nmerge with master","position":-1.3134439561836189,"last_edited":1455425606418,"task_id":"4ba15e3e-41e9-404f-9fbb-ecd57a1002b2","done":1455425606013}
{"desc":"#v1 #0 #now (2:45)\nsyncstring saving breaks sometimes\n\nFiguring out why and fixing this is obviously critical!\n\nSo far I figured out that the changefeed for the syncstrings table is discarded for some reason.  Thus the browser stops getting updates, including that the save worked.\n\nI had an example of this, and checked with `smc.client.query_get_changefeed_ids({cb:function(e,cf) {window.cf =cf}})` then reconnected with `s._syncstring_table._connected = false; s._syncstring_table._reconnect()` and it fixed the problem.    There is an explicit cancelling of the changefeed in the log.\n\nI think what might happen is the connection closes and re-opens very quickly; the changefeed id is still listed so it doesn't get reset.  However, it then gets closed moments later.  Yes, that's *EXACTLY* it -- in fact, clicking reconnect breaks everything completely.  Idea for a fix -- more quicly remove all the changefeed id's.\n\nANSWER: fixed via getting rid of destroy timer and reconnecting properly.","position":-1.3134439562272746,"last_edited":1455469254946,"task_id":"699cda28-8543-4a38-848d-e303418fedcd","done":1455469254536}
{"desc":"#0 #v1 (0:30?) (0:45)\nfix this: `FileUseActions.mark_file error:  don't know server yet`\nin the log a lot","position":-1.3134439250588912,"last_edited":1455497694225,"task_id":"bfc49359-f31d-48d7-83e4-443d661372db","done":1455497693813}
{"desc":"#0 #v1 (1:30?) #crazy #wontfix\nensure that the client and hub (both project and browser) know with certainty exactly what syncstrings are being managed by the hub.  \n\nNo matter how hard I try to make this event based, it's just not working well. \n\nHow about this:\n\nAny time the list of changefeed id's being managed changes send a message to the client saying \"here's how they changed\", and be satisfied with a response that is a hash of the result proving that they applied the update.  If not, send a different message saying \"Here's the whole thing\", reset to this.\n\nProblems -- the above could go very wrong in case of lots of changefeeds connecting, disconnecting, etc.\n\nOther ideas.\n","position":-1.3134439250534342,"last_edited":1455497707756,"task_id":"dd286797-150d-424d-92d2-ef42ca6fd4a1","done":1455497707349}
{"desc":"#0 (0:45?) #v1 #now (2:30)\nWhy does jupyter spin out of control???\n\nDue to not waiting for the kernel to be fully connected.  Doing that fixes things nicely.","position":-1.3134439250471246,"last_edited":1456356342276,"task_id":"d9b7439f-7733-4386-a6d4-e82347c81a99","deleted":false,"done":1456356341819}
{"desc":"#jupyter #v2\nif one user changes the kernel, have to make everybody else update their kernel setting\n\neither:\n\n- [ ] refresh iframe (easy but ugly)\n- [ ] somehow run some javascript on the page (harder)","position":-1.3134439562854823,"last_edited":1455506844913,"task_id":"bcfa25ec-f436-4a37-a38c-175025babc38"}
{"desc":"#1 #jupyter #v1 (2:00?)\nsimulate cell that won't JSON and make sure this problem gets treated sensibly.\n\nRight now it definitely can go into a horrible infinite loop, breaking all clients, eating the worksheet, etc.!   This one experienced that https://cloud.sagemath.com/45f4aab5-7698-4ac8-9f63-9fd307401ad7/port/54249/projects/6976fd09-bf0e-4c21-81c7-066214b123fd/files/2016-02-14-185842.ipynb\n\nSo this is critical to get right.","position":-1.3134439250470393,"last_edited":1455515713624,"task_id":"84b303e1-69d2-489f-bfba-5cccdae5a001"}
{"desc":"#1 #v1 #jupyter  (1:00?)\ncursors suddenly start using huge amounts of cpu","position":-1.313443925071164,"last_edited":1456357500937,"task_id":"516cb219-1900-44ae-abc0-4f1ebe798d1e"}