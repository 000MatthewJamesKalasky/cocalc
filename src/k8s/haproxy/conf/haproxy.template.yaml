# This is a DaemonSet, so one pod gets scheduled on every node.
# TODO: it tired to schedule a pod on the master node, which fails... and sits there
# in an error state.  I don't know a **good** way to prevent this yet.

apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  annotations:
    deployment.kubernetes.io/revision: "1"
  creationTimestamp: null
  generation: 1
  labels:
    run: haproxy
  name: haproxy
  selfLink: /apis/extensions/v1beta1/namespaces/deployments/haproxy
spec:
  selector:
    matchLabels:
      run: haproxy
  template:
    metadata:
      creationTimestamp: null
      labels:
        run: haproxy
    spec:
      containers:
      - image: {image}
        imagePullPolicy: {pull_policy}
        name: haproxy
        livenessProbe:
          httpGet:
            path: /
            port: 1936
            scheme: HTTP
          initialDelaySeconds: 30
          timeoutSeconds: 5
        terminationMessagePath: /dev/termination-log
        ports:
        # All http services
        - containerPort: 80
          protocol: TCP
        # https
        - containerPort: 443
          protocol: TCP
        # haproxy stats
        - containerPort: 1936
          protocol: TCP
        volumeMounts:
        - name: secret
          mountPath: /secret
          readOnly: true
      volumes:
        - name: secret
          secret:
            secretName: ssl-cert
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      terminationGracePeriodSeconds: 30
